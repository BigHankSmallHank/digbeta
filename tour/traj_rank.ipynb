{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Recommendation using POI Ranking and Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Rank POIs using rankSVM\n",
    "1. Recommend a set of POIs given (start, end, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import os, re, sys, time, pickle, tempfile\n",
    "import math, random, itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as sop\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(987654321) # control random choice when splitting training/testing set\n",
    "np.random.seed(987654321)\n",
    "ranksvm_dir = '$HOME/work/ranksvm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/data-ijcai15'\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Osak.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Osak.csv')\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Glas.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Glas.csv')\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Edin.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Edin.csv')\n",
    "fvisit = os.path.join(data_dir, 'userVisits-Toro.csv')\n",
    "fcoord = os.path.join(data_dir, 'photoCoords-Toro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suffix = fvisit.split('-')[-1].split('.')[0]\n",
    "fseqpart = os.path.join(data_dir, 'seqPart-' + suffix + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visits = pd.read_csv(fvisit, sep=';')\n",
    "coords = pd.read_csv(fcoord, sep=';')\n",
    "assert(visits.shape[0] == coords.shape[0])\n",
    "traj = pd.merge(visits, coords, on='photoID') # merge data frames according to column 'photoID'\n",
    "#traj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#photo</th>\n",
       "      <th>#photo/user</th>\n",
       "      <th>#poi</th>\n",
       "      <th>#seq</th>\n",
       "      <th>#seq/user</th>\n",
       "      <th>#user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Toro</th>\n",
       "      <td>39419</td>\n",
       "      <td>28.257348</td>\n",
       "      <td>29</td>\n",
       "      <td>6057</td>\n",
       "      <td>4.341935</td>\n",
       "      <td>1395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      #photo  #photo/user  #poi  #seq  #seq/user  #user\n",
       "Toro   39419    28.257348    29  6057   4.341935   1395"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_photo = traj['photoID'].unique().shape[0]\n",
    "num_user = traj['userID'].unique().shape[0]\n",
    "num_poi = traj['poiID'].unique().shape[0]\n",
    "num_seq = traj['seqID'].unique().shape[0]\n",
    "pd.DataFrame({'#photo': num_photo, '#user': num_user, '#poi': num_poi, '#seq': num_seq, \\\n",
    "              '#photo/user': num_photo/num_user, '#seq/user': num_seq/num_user}, index=[str(suffix)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Compute POI Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute POI (Longitude, Latitude) as the average coordinates of the assigned photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_coords = traj[['poiID', 'photoLon', 'photoLat']].groupby('poiID').mean()\n",
    "poi_coords.reset_index(inplace=True)\n",
    "poi_coords.rename(columns={'photoLon':'poiLon', 'photoLat':'poiLat'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract POI category and visiting frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_catfreq = traj[['poiID', 'poiTheme', 'poiFreq']].groupby('poiID').first()\n",
    "poi_catfreq.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poiTheme</th>\n",
       "      <th>poiFreq</th>\n",
       "      <th>poiLon</th>\n",
       "      <th>poiLat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poiID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sport</td>\n",
       "      <td>3506</td>\n",
       "      <td>-79.379243</td>\n",
       "      <td>43.643183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sport</td>\n",
       "      <td>609</td>\n",
       "      <td>-79.418634</td>\n",
       "      <td>43.632772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sport</td>\n",
       "      <td>688</td>\n",
       "      <td>-79.380045</td>\n",
       "      <td>43.662175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sport</td>\n",
       "      <td>3056</td>\n",
       "      <td>-79.389290</td>\n",
       "      <td>43.641297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cultural</td>\n",
       "      <td>986</td>\n",
       "      <td>-79.392396</td>\n",
       "      <td>43.653662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       poiTheme  poiFreq     poiLon     poiLat\n",
       "poiID                                         \n",
       "1         Sport     3506 -79.379243  43.643183\n",
       "2         Sport      609 -79.418634  43.632772\n",
       "3         Sport      688 -79.380045  43.662175\n",
       "4         Sport     3056 -79.389290  43.641297\n",
       "6      Cultural      986 -79.392396  43.653662"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_all = pd.merge(poi_catfreq, poi_coords, on='poiID')\n",
    "poi_all.set_index('poiID', inplace=True)\n",
    "poi_all.head()\n",
    "#poi_all.to_csv(fpoi, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Construct Travelling Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>seqID</th>\n",
       "      <th>poiID</th>\n",
       "      <th>arrivalTime</th>\n",
       "      <th>departureTime</th>\n",
       "      <th>#photo</th>\n",
       "      <th>poiDuration(sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10007579@N00</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1346844688</td>\n",
       "      <td>1346844688</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1142731848</td>\n",
       "      <td>1142732445</td>\n",
       "      <td>4</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1142916492</td>\n",
       "      <td>1142916492</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>1319327174</td>\n",
       "      <td>1319332848</td>\n",
       "      <td>9</td>\n",
       "      <td>5674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10014440@N06</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1196128621</td>\n",
       "      <td>1196128878</td>\n",
       "      <td>3</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         userID  seqID  poiID  arrivalTime  departureTime  #photo  \\\n",
       "0  10007579@N00      1     30   1346844688     1346844688       1   \n",
       "1  10012675@N05      2      6   1142731848     1142732445       4   \n",
       "2  10012675@N05      3      6   1142916492     1142916492       1   \n",
       "3  10012675@N05      4     13   1319327174     1319332848       9   \n",
       "4  10014440@N06      5     24   1196128621     1196128878       3   \n",
       "\n",
       "   poiDuration(sec)  \n",
       "0                 0  \n",
       "1               597  \n",
       "2                 0  \n",
       "3              5674  \n",
       "4               257  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_all = traj[['userID', 'seqID', 'poiID', 'dateTaken']].copy().groupby(['userID', 'seqID', 'poiID'])\\\n",
    "          .agg([np.min, np.max, np.size])\n",
    "seq_all.columns = seq_all.columns.droplevel()\n",
    "seq_all.reset_index(inplace=True)\n",
    "seq_all.rename(columns={'amin':'arrivalTime', 'amax':'departureTime', 'size':'#photo'}, inplace=True)\n",
    "seq_all['poiDuration(sec)'] = seq_all['departureTime'] - seq_all['arrivalTime']\n",
    "seq_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>seqLen</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seqID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10007579@N00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10014440@N06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             userID  seqLen\n",
       "seqID                      \n",
       "1      10007579@N00       1\n",
       "2      10012675@N05       1\n",
       "3      10012675@N05       1\n",
       "4      10012675@N05       1\n",
       "5      10014440@N06       1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_stats = seq_all[['userID', 'seqID', 'poiID']].copy().groupby(['userID', 'seqID']).agg(np.size)\n",
    "seq_stats.reset_index(inplace=True)\n",
    "seq_stats.rename(columns={'poiID':'seqLen'}, inplace=True)\n",
    "seq_stats.set_index('seqID', inplace=True)\n",
    "seq_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_seq(seqid, seq_all):\n",
    "    seqi = seq_all[seq_all['seqID'] == seqid].copy()\n",
    "    seqi.sort_values(by=['arrivalTime'], ascending=True, inplace=True)\n",
    "    return seqi['poiID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poi_info(seqid_set, seq_all, poi_all):\n",
    "    poi_info = seq_all[seq_all['seqID'].isin(seqid_set)][['poiID', 'poiDuration(sec)']].copy()\n",
    "    poi_info = poi_info.groupby('poiID').agg([np.mean, np.size])\n",
    "    poi_info.columns = poi_info.columns.droplevel()\n",
    "    poi_info.reset_index(inplace=True)\n",
    "    poi_info.rename(columns={'mean':'avgDuration(sec)', 'size':'popularity'}, inplace=True)\n",
    "    poi_info.set_index('poiID', inplace=True)\n",
    "    poi_info['poiTheme'] = poi_all.loc[poi_info.index, 'poiTheme']\n",
    "    poi_info['poiLon'] = poi_all.loc[poi_info.index, 'poiLon']\n",
    "    poi_info['poiLat'] = poi_all.loc[poi_info.index, 'poiLat']\n",
    "    return poi_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Train vs. Query vs. Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split sequences into training set and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_percent = 0.3\n",
    "query_percent = 0.5\n",
    "seqid_set_train0 = []\n",
    "seqid_set_query0 = []\n",
    "seqid_set_test0 = []\n",
    "query_id_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate trajectories (i.e. same trajectory for different users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_ = seq_stats[seq_stats['seqLen'] > 2].index.tolist()\n",
    "seq_dict = dict()\n",
    "for seqid in seqid_set_:\n",
    "    seq = extract_seq(seqid, seq_all)\n",
    "    key = str(seq)\n",
    "    if key in seq_dict: seq_dict[key].append(seqid)\n",
    "    else: seq_dict[key] = [seqid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seqid_set_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_exp = [seq_dict[x][0] for x in sorted(seq_dict.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a *query* (in IR terminology) using ~~tuple (start POI, end POI, #POI)~~ user ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seqs_exp = [extract_seq(seqid, seq_all) for seqid in seqid_set_exp]\n",
    "#keys = [(seq[0], seq[-1], len(seq)) for seq in seqs_exp]\n",
    "keys = [seq_stats.loc[seqid, 'userID'] for seqid in seqid_set_exp]\n",
    "cnt = 0\n",
    "for key in keys:\n",
    "    if key not in query_id_dict:\n",
    "        query_id_dict[key] = cnt\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random split trajectories for training, querying and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(seqid_set_exp)\n",
    "#ntrain = int(train_percent * len(seqid_set_exp))\n",
    "ntrain = 1\n",
    "nquery = int(query_percent * len(seqid_set_exp))\n",
    "seqid_set_train0 = sorted(list(seqid_set_exp[:ntrain]))\n",
    "seqid_set_query0 = sorted(list(seqid_set_exp[ntrain:ntrain+nquery]))\n",
    "seqid_set_test0 = sorted(list(seqid_set_exp[ntrain+nquery:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#seq in total: 292\n",
      "#seq for training: 1 [1938]\n",
      "#seq for querying: 146\n",
      "#seq for testing: 145\n"
     ]
    }
   ],
   "source": [
    "print('#seq in total:', len(seqid_set_exp))\n",
    "print('#seq for training:', ntrain, seqid_set_train0)\n",
    "print('#seq for querying:', nquery)\n",
    "print('#seq for testing:', len(seqid_set_exp)-ntrain-nquery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data are generated as follows:\n",
    "1. each input tuple $(\\text{startPOI}, \\text{endPOI}, \\text{#POI})$ form a `query` (in IR terminology).\n",
    "1. the label of a specific POI is the number of presence of that POI in a specific `query`.\n",
    "1. ~~the label of all absence POIs in a specific `query` got a label 0.~~ unobserved for absence POIs in a specific `query`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data are generated the same way as training data, ~~except that the labels of testing data (unknown) could be arbitrary values as suggested in [libsvm FAQ](http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#f431).~~\n",
    "Yes, but the reported accuracy (by `svm-predict` command) is calculated based on these values, so just use the same procedure as that to generate training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data_df(seqid_set, seq_all, seq_stats, poi_info, query_id_dict):\n",
    "    columns = ['label', 'queryId', 'popularity', 'avgDuration(sec)']\n",
    "    poi_list = poi_info.index\n",
    "    train_df = pd.DataFrame(data=np.zeros((len(poi_list), len(columns)), dtype=np.float), \\\n",
    "                            index=poi_list, columns=columns)\n",
    "    train_df['popularity'] = poi_info.loc[train_df.index, 'popularity']\n",
    "    train_df['avgDuration(sec)'] = poi_info.loc[train_df.index, 'avgDuration(sec)']\n",
    "    for seqid in sorted(set(seqid_set)):\n",
    "        seq = extract_seq(seqid, seq_all)\n",
    "        #key = (seq[0], seq[-1], len(seq))\n",
    "        key = seq_stats.loc[seqid, 'userID']\n",
    "        qid = query_id_dict[key]\n",
    "        for poi in seq:\n",
    "            train_df.loc[poi, 'queryId'] = qid\n",
    "            train_df.loc[poi, 'label'] += 1\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Ranking POIs using rankSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RankSVM (zip file) can be downloaded [here](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ranksvm/libsvm-ranksvm-3.20.zip), please read `README.ranksvm` in the zip file for installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a python wrapper of the `svm-train` and `svm-predict` commands of rankSVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# python wrapper of rankSVM\n",
    "class RankSVM:\n",
    "    def __init__(self, bin_dir, debug=False):\n",
    "        dir_ = !echo $bin_dir  # deal with environmental variables in path\n",
    "        assert(os.path.exists(dir_[0]))\n",
    "        self.bin_dir = dir_[0]\n",
    "        assert(isinstance(debug, bool))\n",
    "        self.debug = debug\n",
    "        \n",
    "        # create named tmp files for model and feature scaling parameters\n",
    "        self.fmodel = None\n",
    "        self.fscale = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fmodel = fd.name\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fscale = fd.name\n",
    "        \n",
    "        if self.debug:\n",
    "            print('model file:', self.fmodel)\n",
    "            print('feature scaling parameter file:', self.fscale)\n",
    "    \n",
    "    \n",
    "    def __del__(self):\n",
    "        # remove tmp files\n",
    "        if self.fmodel is not None and os.path.exists(self.fmodel):\n",
    "            os.unlink(self.fmodel)\n",
    "        if self.fscale is not None and os.path.exists(self.fscale):\n",
    "            os.unlink(self.fscale)\n",
    "    \n",
    "    \n",
    "    def train(self, train_df, cost=1):\n",
    "        # cost is parameter C in SVM\n",
    "        # write train data to file\n",
    "        ftrain = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain = fd.name\n",
    "            for idx in train_df.index:\n",
    "                line = str(train_df.loc[idx, 'label'])\n",
    "                line += ' qid:' + str(int(train_df.loc[idx, 'queryId']))\n",
    "                line += ' 1:' + str(train_df.loc[idx, 'popularity'])\n",
    "                line += ' 2:' + str(train_df.loc[idx, 'avgDuration(sec)'])\n",
    "                fd.write(line + '\\n')\n",
    "        \n",
    "        # feature scaling\n",
    "        ftrain_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -s $self.fscale $ftrain > $ftrain_scaled\n",
    "        \n",
    "        if self.debug:\n",
    "            print('train data file:', ftrain)\n",
    "            print('feature scaled train data file:', ftrain_scaled)\n",
    "        \n",
    "        # train rank svm and generate model file, if the model file exists, rewrite it\n",
    "        #n_cv = 10  # parameter k for k-fold cross-validation, NO model file will be generated in CV mode\n",
    "        #result = !$self.bin_dir/svm-train -c $cost -v $n_cv $ftrain $self.fmodel\n",
    "        result = !$self.bin_dir/svm-train -c $cost $ftrain_scaled $self.fmodel\n",
    "        if self.debug:\n",
    "            print('Training finished.')\n",
    "            for i in range(len(result)): print(result[i])\n",
    "\n",
    "        # remove train data file\n",
    "        os.unlink(ftrain)\n",
    "        os.unlink(ftrain_scaled)\n",
    "        \n",
    "    \n",
    "    def predict(self, test_df):\n",
    "        if self.fmodel is None or not os.path.exists(self.fmodel):\n",
    "            print('Model should be trained before predicting')\n",
    "            return\n",
    "        \n",
    "        # write test data to file\n",
    "        ftest = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftest = fd.name\n",
    "            for idx in test_df.index:\n",
    "                line = str(test_df.loc[idx, 'label'])\n",
    "                line += ' qid:' + str(int(test_df.loc[idx, 'queryId']))\n",
    "                line += ' 1:' + str(test_df.loc[idx, 'popularity'])\n",
    "                line += ' 2:' + str(test_df.loc[idx, 'avgDuration(sec)'])\n",
    "                fd.write(line + '\\n')\n",
    "                \n",
    "        # feature scaling\n",
    "        ftest_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            ftest_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -r $self.fscale $ftest > $ftest_scaled\n",
    "            \n",
    "        # generate prediction file\n",
    "        fpredict = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            fpredict = fd.name\n",
    "            \n",
    "        if self.debug:\n",
    "            print('test data file:', ftest)\n",
    "            print('feature scaled test data file:', ftest_scaled)\n",
    "            print('predict result file:', fpredict)\n",
    "        \n",
    "            \n",
    "        # predict using trained model and write prediction to file\n",
    "        result = !$self.bin_dir/svm-predict $ftest_scaled $self.fmodel $fpredict\n",
    "        if self.debug:\n",
    "            print('Predict result: %-30s  %s' % (result[0], result[1]))\n",
    "        \n",
    "        # generate prediction DataFrame from prediction file\n",
    "        poi_rank_df = pd.read_csv(fpredict, header=None)\n",
    "        poi_rank_df.rename(columns={0:'rank'}, inplace=True)\n",
    "        poi_rank_df['poiID'] = test_df.index\n",
    "        poi_rank_df.set_index('poiID', inplace=True)\n",
    "        \n",
    "        # remove test file and prediction file\n",
    "        os.unlink(ftest)\n",
    "        os.unlink(ftest_scaled)\n",
    "        os.unlink(fpredict)\n",
    "        \n",
    "        return poi_rank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Recommend Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enum_seq(poi_list, startPOI, endPOI, nPOI):\n",
    "    \"\"\"\n",
    "    Enumerate all possible subset of poi_list without startPOI and endPOI,\n",
    "    The size of subset is nPOI-2\n",
    "    \"\"\"\n",
    "    assert(nPOI > 2)\n",
    "    assert(nPOI < len(poi_list))\n",
    "    tuples = itertools.combinations([p for p in poi_list if p not in {startPOI, endPOI}], nPOI-2)\n",
    "    return [[startPOI] + list(x) + [endPOI] for x in tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_F1score(seq_act, seq_rec):\n",
    "    assert(len(seq_act) > 0)\n",
    "    assert(len(seq_rec) > 0)\n",
    "\n",
    "    act_set = set(seq_act)\n",
    "    rec_set = set(seq_rec)\n",
    "    intersect = act_set & rec_set\n",
    "    \n",
    "    recall = len(intersect) / len(act_set)\n",
    "    precision = len(intersect) / len(rec_set)\n",
    "    F1score = 2. * precision * recall / (precision + recall)\n",
    "  \n",
    "    #return precision, recall, F1score\n",
    "    return F1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(seqid_set_test, seq_all, poi_all, poi_rank_df, query_id_dict, debug=False):\n",
    "    assert(isinstance(debug, bool))\n",
    "    \n",
    "    F1scores = []\n",
    "    pois = poi_rank_df.index\n",
    "    ranks = poi_rank_df['rank'].get_values()\n",
    "    ranked_poi = pois[ranks.argsort()][::-1]  # highest rank --> lowest rank \n",
    "    \n",
    "    # randomly put POIs that haven't seen before at the bottom of ranking\n",
    "    unseen_poi = sorted(list(set(poi_all.index) - set(pois)))\n",
    "    np.random.shuffle(unseen_poi)\n",
    "    ranked_poi = list(ranked_poi) + unseen_poi\n",
    "    \n",
    "    #seqs = [extract_seq(seqid, seq_all) for seqid in seqid_set_test]\n",
    "    #F1scores = Parallel(n_jobs=-2)\\\n",
    "    #           (delayed(calc_F1score)\\\n",
    "    #            (x, [x[0]] + [p for p in ranked_poi if p not in {x[0], x[-1]}][:len(x)-2] + [x[-1]]) for x in seqs)\n",
    "    \n",
    "    for seqid in seqid_set_test:\n",
    "        seq = extract_seq(seqid, seq_all)\n",
    "        poi_rec = [p for p in ranked_poi if p not in {seq[0], seq[-1]}][:len(seq)-2]\n",
    "        seq_rec = [seq[0]] + poi_rec + [seq[-1]]\n",
    "        F1 = calc_F1score(seq, seq_rec)\n",
    "        F1scores.append(F1)\n",
    "        if debug: print('%.2f: %-20s -> %s' % (F1, str(seq), str(seq_rec)))\n",
    "    return F1scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Leave-one-out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance with IJCAI paper use the same leave-one-out evaluation strategy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leave1out(seqid_set_all, seq_all, seq_stats, poi_all, query_id_dict):\n",
    "    F1scores_train = []\n",
    "    #precisions_test = []\n",
    "    #recalls_test = []\n",
    "    F1scores_test  = []\n",
    "    \n",
    "    for i in range(len(seqid_set_all)):\n",
    "        seqid_set_train = seqid_set_all[:i] + seqid_set_all[i+1:]\n",
    "        seqid_set_test = [seqid_set_all[i]]\n",
    "        poi_info_train = calc_poi_info(seqid_set_train, seq_all, poi_all)\n",
    "        train_df = gen_data_df(seqid_set_train, seq_all, seq_stats, poi_info_train, query_id_dict)\n",
    "        \n",
    "        # training rankSVM\n",
    "        #C = 10\n",
    "        #C = 30\n",
    "        #C = 100\n",
    "        #C = 300\n",
    "        C = 1000\n",
    "        ranksvm = RankSVM(ranksvm_dir)\n",
    "        ranksvm.train(train_df, C)\n",
    "        poi_rank_df = ranksvm.predict(train_df)\n",
    "        \n",
    "        # compute training accuracy\n",
    "        F1scores = evaluate(seqid_set_train, seq_all, poi_all, poi_rank_df, query_id_dict)\n",
    "        F1scores_train.append(F1scores)\n",
    "        \n",
    "        # compute testing accuracy\n",
    "        F1scores = evaluate(seqid_set_test, seq_all, poi_all, poi_rank_df, query_id_dict)\n",
    "        F1scores_test.append(F1scores)\n",
    "        \n",
    "    return F1scores_train, F1scores_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IJCAI paper, the best score reported on Toronto data is \n",
    " - Recall: 0.779&plusmn;0.10\n",
    " - Precision: 0.706&plusmn;0.013\n",
    " - F1-score: 0.732&plusmn;0.012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F1_l1o_train, F1_l1o_test = leave1out(seqid_set_exp, seq_all, seq_stats, poi_all, query_id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision and recall should be the same value (for a recommendation) as the length of actual trajectory and that of recommended trajectory are the same in our setting, which further results in the same F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print('Test Recall:', np.mean(r_t), np.std(r_t))\n",
    "#print('Test Precision:', np.mean(p_t), np.std(p_t))\n",
    "print('Train F1:', np.mean(F1_l1o_train), np.std(F1_l1o_train))\n",
    "print('Test F1:', np.mean(F1_l1o_test), np.std(F1_l1o_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test F1:\n",
    "<table>\n",
    "<tr><td><b>C</b></td><td><b>mean of F1</b></td><td><b>std of F1</b></td></tr>\n",
    "<tr><td>10</td><td>0.691</td><td>0.131</td></tr>\n",
    "<tr><td>30</td><td>0.695</td><td>0.133</td></tr>\n",
    "<tr><td>100</td><td>0.694</td><td>0.133</td></tr>\n",
    "<tr><td>300</td><td>0.694</td><td>0.131</td></tr>\n",
    "<tr><td>1000</td><td>0.692</td><td>0.128</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16, 6])\n",
    "plt.xlabel('#Query')\n",
    "plt.ylabel('Train F1')\n",
    "plt.boxplot(F1_l1o_train)\n",
    "plt.plot(np.arange(1, len(F1_l1o_train)+1), [np.mean(x) for x in F1_l1o_train], color='g', marker='o')\n",
    "plt.xticks(list(range(0, len(F1_l1o_train), 10)), list(range(0, len(F1_l1o_train), 10))) # xticks starts from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16, 6])\n",
    "plt.hist([y for x in F1_l1o_test for y in x], bins=50)\n",
    "plt.xlabel('Test F1')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Random Baseline - Passive Learing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a sequence to query uniformly at random, i.e. passive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_baseline(seqid_set_train, seqid_set_query, seqid_set_test, seq_all, seq_stats, poi_all, query_id_dict):\n",
    "    seq_idx = np.arange(len(seqid_set_query))\n",
    "    np.random.shuffle(seq_idx)\n",
    "    F1scores_train = []\n",
    "    F1scores_test  = []\n",
    "    \n",
    "    cnt = 0\n",
    "    while len(seqid_set_query) > 0:\n",
    "        # compute features for training\n",
    "        poi_info_train = calc_poi_info(seqid_set_train, seq_all, poi_all)\n",
    "        train_df = gen_data_df(seqid_set_train, seq_all, seq_stats, poi_info_train, query_id_dict)\n",
    "        \n",
    "        # training rankSVM\n",
    "        #C = 100\n",
    "        #C = 300\n",
    "        C = 1000\n",
    "        ranksvm = RankSVM(ranksvm_dir)\n",
    "        ranksvm.train(train_df, C)\n",
    "        poi_rank_df = ranksvm.predict(train_df)\n",
    "        \n",
    "        # compute training accuracy\n",
    "        F1scores = evaluate(seqid_set_train, seq_all, poi_all, poi_rank_df, query_id_dict, debug=True)\n",
    "        F1scores_train.append(F1scores)\n",
    "        print('Iteration %d, mean F1: %.2f\\n' % (cnt, np.mean(F1scores)))\n",
    "        \n",
    "        # compute testing accuracy\n",
    "        F1scores = evaluate(seqid_set_test, seq_all, poi_all, poi_rank_df, query_id_dict)\n",
    "        F1scores_test.append(F1scores)\n",
    "        \n",
    "        # query strategy\n",
    "        seq_idx = -1 # the last element after random shuffle\n",
    "        seqid_set_train.append(seqid_set_query[seq_idx])\n",
    "        del seqid_set_query[seq_idx]\n",
    "        \n",
    "        cnt += 1\n",
    "        \n",
    "    return F1scores_train, F1scores_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Least Confident Query Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant Definitions\n",
    "- Constraint: $\\textbf{x} = (P_s, P_e, \\lvert Traj \\rvert)$\n",
    "- Query: what is the best trajectory to recommend with respect to the above constraint?\n",
    "- Result of Query: $\\textbf{y} = Traj$\n",
    "- Probability of Possible Trajectory: rescale the score for ranking of POIs to $[-1, 1]$, then shift 1 unit to the right. define the score of a trajectory by summing over its POIs, normalise scores over all possible trajectory to get a probability distribution.  \n",
    "*Problem: NOT Good because the \"number\" for each POI only optimised for ranking purpose, i.e. the absolute value of the \"number\" for each POI is meaningless as long as \"number\" for visited POI is greater than \"number\" for unvisited POI.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Strategy:\n",
    "\\begin{equation}\n",
    "\\phi^{LC}(\\textbf{x}) = 1 - P(\\textbf{y}^* | \\textbf{x}; \\Theta)\n",
    "\\end{equation}\n",
    "where $\\textbf{y}^*$ is the most likely label of example $\\textbf{x}$ with respect to a probabilistic model of which the parameters are denoted by $\\Theta$.  \n",
    "This query strategy select the example $\\textbf{x}$ of maximum $\\phi^{LC}$ from all unlabelled examples in a pool to query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strategy is biased to long trajectories (i.e. trajectory with more POIs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_confidence(startPOI, endPOI, nPOI, poi_list, poi_rank_df):\n",
    "    # calculate the probability of most likely possible sequence\n",
    "    assert(nPOI > 2)\n",
    "    \n",
    "    # enumerate all possible trajectories\n",
    "    enum_seqs = enum_seq(poi_list, startPOI, endPOI, nPOI) \n",
    "    \n",
    "    # scoring each trajectory\n",
    "    scores_traj = np.array([poi_rank_df.loc[x, 'rank'].sum() for x in enum_seqs])\n",
    "    \n",
    "    # normalise to get a probability distribution\n",
    "    assert(np.sum(scores_traj) > 0)\n",
    "    probs = scores_traj / np.sum(scores_traj)\n",
    "    \n",
    "    return max(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def least_confident(seqid_set_train, seqid_set_query, seqid_set_test, seq_all, seq_stats, poi_all, query_id_dict):\n",
    "    F1scores_train = []\n",
    "    F1scores_test  = []\n",
    "    query_seqs = [extract_seq(seqid, seq_all) for seqid in seqid_set_query]\n",
    "    \n",
    "    while len(seqid_set_query) > 0:\n",
    "        # compute features for training\n",
    "        poi_info_train = calc_poi_info(seqid_set_train, seq_all, poi_all)\n",
    "        train_df = gen_data_df(seqid_set_train, seq_all, seq_stats, poi_info_train, query_id_dict)\n",
    "        \n",
    "        # training rankSVM\n",
    "        #C = 100\n",
    "        #C = 300\n",
    "        C = 1000\n",
    "        ranksvm = RankSVM(ranksvm_dir)\n",
    "        ranksvm.train(train_df, C)\n",
    "        poi_rank_df = ranksvm.predict(train_df)\n",
    "        \n",
    "        # compute training accuracy\n",
    "        F1scores = evaluate(seqid_set_train, seq_all, poi_all, poi_rank_df, query_id_dict)\n",
    "        F1scores_train.append(F1scores)\n",
    "        \n",
    "        # compute testing accuracy\n",
    "        F1scores = evaluate(seqid_set_test, seq_all, poi_all, poi_rank_df, query_id_dict)\n",
    "        F1scores_test.append(F1scores)\n",
    "        \n",
    "        # deal with unseen POIs\n",
    "        # randomly put POIs that haven't seen before at the bottom of ranking\n",
    "        unseen_poi = sorted(list(set(poi_all.index) - set(poi_rank_df.index)))\n",
    "        np.random.shuffle(unseen_poi)  # tie breaking\n",
    "        \n",
    "        # special case: the same rank for all seen POIs\n",
    "        if np.all(np.abs(poi_rank_df['rank'] - poi_rank_df['rank'].max()) < 1e-6):\n",
    "            seen_poi = poi_rank_df.index.tolist()\n",
    "            np.random.shuffle(seen_poi) # tie break\n",
    "            pois = seen_poi + unseen_poi\n",
    "            poi_rank_df = pd.DataFrame.from_dict({'poiID': pois, 'rank': np.zeros(len(pois), dtype=np.float)})\n",
    "            poi_rank_df.loc[seen_poi, 'rank'] = 1 # scores for ranking: 1 for seen POIs, 0 for unseen POIs\n",
    "        else:\n",
    "            bottom_score = poi_rank_df['rank'].min()\n",
    "            bottom_score = bottom_score - 2*abs(bottom_score)\n",
    "            unseen_poi_df = pd.DataFrame.from_dict(\\\n",
    "                                {'poiID': unseen_poi, 'rank': bottom_score * np.ones(len(unseen_poi), dtype=np.float)})\n",
    "            unseen_poi_df.set_index('poiID', inplace=True)\n",
    "            poi_rank_df = poi_rank_df.append(unseen_poi_df)\n",
    "\n",
    "        # (naively) convert POI scores for ranking into range [0, 2] so that \n",
    "        # a distribution of possible trajectories could be computed.\n",
    "        scores = poi_rank_df['rank'].get_values()\n",
    "        assert(max(np.abs(scores) > 1e-6))\n",
    "        scores = scores / max(np.abs(scores))  # range [-1, 1]\n",
    "        scores = scores + 1                    # range [ 0, 2]\n",
    "        poi_rank_df['rank'] = scores\n",
    "        \n",
    "        # query strategy \n",
    "        confidence = Parallel(n_jobs=-2)\\\n",
    "                     (delayed(calc_confidence)(seq[0], seq[-1], len(seq), poi_all.index, poi_rank_df) \\\n",
    "                      for seq in query_seqs)\n",
    "        seq_idx = np.argmin(confidence) # choose the sequence with least confident to query\n",
    "        seqid_set_train.append(seqid_set_query[seq_idx])\n",
    "        print('choose sequence:', query_seqs[seq_idx]); sys.stdout.flush()\n",
    "        del seqid_set_query[seq_idx]\n",
    "        del query_seqs[seq_idx]\n",
    "        \n",
    "    return F1scores_train, F1scores_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Maximum Sequence Entropy Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\phi^{SE}(\\textbf{x}) = - \\sum_{\\hat{\\textbf{y}}} P(\\hat{\\textbf{y}} | \\textbf{x}; \\Theta) \n",
    "\\log(P(\\hat{\\textbf{y}} | \\textbf{x}; \\Theta))\n",
    "\\end{equation}\n",
    "where $\\hat{\\textbf{y}}$ ranges over all possible labels for example $\\textbf{x}$.  \n",
    "**Note** that the number of possible labels grows exponentially with $|Traj|$ in $\\textbf{x}$, to make computation feasible,\n",
    "[Kim06](http://www.aclweb.org/anthology/N06-2018) used the $N$-best possible labels to approximate, concretely, \n",
    "define **N-best Sequence Entropy** as\n",
    "\\begin{equation}\n",
    "\\phi^{NSE}(\\textbf{x}) = - \\sum_{\\hat{\\textbf{y}} \\in \\mathcal{N}} P(\\hat{\\textbf{y}} | \\textbf{x}; \\Theta) \n",
    "\\log(P(\\hat{\\textbf{y}} | \\textbf{x}; \\Theta))\n",
    "\\end{equation}\n",
    "where $\\mathcal{N} = \\{\\textbf{y}_1^*, \\dots, \\textbf{y}_N^*\\}$ is the set of the $N$ most likely labels of example $\\textbf{x}$.  \n",
    "This query strategy select the example $\\textbf{x}$ of maximum $\\phi^{SE}$ or $\\phi^{NSE}$ from all unlabelled examples in a pool to query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_sequence_entropy(startPOI, endPOI, nPOI, poi_list, poi_rank_df):\n",
    "    # calculate the sequence entropy of all possible sequences/trajectories\n",
    "    assert(nPOI > 2)\n",
    "    \n",
    "    # enumerate all possible trajectories\n",
    "    enum_seqs = enum_seq(poi_list, startPOI, endPOI, nPOI) \n",
    "    \n",
    "    # scoring each trajectory\n",
    "    scores_traj = np.array([poi_rank_df.loc[x, 'rank'].sum() for x in enum_seqs])\n",
    "    \n",
    "    # normalise to get a probability distribution\n",
    "    assert(np.sum(scores_traj) > 0)\n",
    "    probs = scores_traj / np.sum(scores_traj)\n",
    "    \n",
    "    return -np.dot(probs, np.log2(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maximum_sequence_entropy(seqid_set_train, seqid_set_query, seqid_set_test, seq_all, \\\n",
    "                             seq_stats, poi_all, query_id_dict):\n",
    "    F1scores_train = []\n",
    "    F1scores_test  = []\n",
    "    query_seqs = [extract_seq(seqid, seq_all) for seqid in seqid_set_query]\n",
    "    \n",
    "    while len(seqid_set_query) > 0:\n",
    "        # compute features for training\n",
    "        poi_info_train = calc_poi_info(seqid_set_train, seq_all, poi_all)\n",
    "        train_df = gen_data_df(seqid_set_train, seq_all, seq_stats, poi_info_train, query_id_dict)\n",
    "        \n",
    "        # training rankSVM\n",
    "        #C = 100\n",
    "        #C = 300\n",
    "        C = 1000\n",
    "        ranksvm = RankSVM(ranksvm_dir)\n",
    "        ranksvm.train(train_df, C)\n",
    "        poi_rank_df = ranksvm.predict(train_df)\n",
    "        \n",
    "        # compute training accuracy\n",
    "        F1scores = evaluate(seqid_set_train, seq_all, poi_all, poi_rank_df, query_id_dict)\n",
    "        F1scores_train.append(F1scores)\n",
    "        \n",
    "        # compute testing accuracy\n",
    "        F1scores = evaluate(seqid_set_test, seq_all, poi_all, poi_rank_df, query_id_dict)\n",
    "        F1scores_test.append(F1scores)\n",
    "        \n",
    "        # deal with unseen POIs\n",
    "        # randomly put POIs that haven't seen before at the bottom of ranking\n",
    "        unseen_poi = sorted(list(set(poi_all.index) - set(poi_rank_df.index)))\n",
    "        np.random.shuffle(unseen_poi)  # tie breaking\n",
    "        \n",
    "        # special case: the same rank for all seen POIs\n",
    "        if np.all(np.abs(poi_rank_df['rank'] - poi_rank_df['rank'].max()) < 1e-6):\n",
    "            seen_poi = poi_rank_df.index.tolist()\n",
    "            np.random.shuffle(seen_poi) # tie break\n",
    "            pois = seen_poi + unseen_poi\n",
    "            poi_rank_df = pd.DataFrame.from_dict({'poiID': pois, 'rank': np.zeros(len(pois), dtype=np.float)})\n",
    "            poi_rank_df.loc[seen_poi, 'rank'] = 1 # scores for ranking: 1 for seen POIs, 0 for unseen POIs\n",
    "        else:\n",
    "            bottom_score = poi_rank_df['rank'].min()\n",
    "            bottom_score = bottom_score - 2*abs(bottom_score)\n",
    "            unseen_poi_df = pd.DataFrame.from_dict(\\\n",
    "                                {'poiID': unseen_poi, 'rank': bottom_score * np.ones(len(unseen_poi), dtype=np.float)})\n",
    "            unseen_poi_df.set_index('poiID', inplace=True)\n",
    "            poi_rank_df = poi_rank_df.append(unseen_poi_df)\n",
    "\n",
    "        # (naively) convert POI scores for ranking into range [0, 2] so that \n",
    "        # a distribution of possible trajectories could be computed.\n",
    "        scores = poi_rank_df['rank'].get_values()\n",
    "        assert(max(np.abs(scores) > 1e-6))\n",
    "        scores = scores / max(np.abs(scores))  # range [-1, 1]\n",
    "        scores = scores + 1                    # range [ 0, 2]\n",
    "        poi_rank_df['rank'] = scores\n",
    "        \n",
    "        # query strategy \n",
    "        entropy = Parallel(n_jobs=-2)\\\n",
    "                          (delayed(calc_sequence_entropy)(seq[0], seq[-1], len(seq), poi_all.index, poi_rank_df) \\\n",
    "                           for seq in query_seqs)\n",
    "        seq_idx = np.argmax(entropy) # choose the sequence with maximum entropy\n",
    "        seqid_set_train.append(seqid_set_query[seq_idx])\n",
    "        print('choose sequence:', query_seqs[seq_idx]); sys.stdout.flush()\n",
    "        del seqid_set_query[seq_idx]\n",
    "        del query_seqs[seq_idx]\n",
    "        \n",
    "    return F1scores_train, F1scores_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_evaluation_results(F1_train_list, F1_test_list):\n",
    "    assert(len(F1_train_list) == len(F1_test_list))\n",
    "    F1_train_mean = [np.mean(x) for x in F1_train_list]\n",
    "    F1_train_median = [np.median(x) for x in F1_train_list]\n",
    "    F1_test_mean = [np.mean(x) for x in F1_test_list]\n",
    "    F1_test_median = [np.median(x) for x in F1_test_list]\n",
    "    \n",
    "    plt.figure(figsize=[15, 18])\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.xlabel('#Query')\n",
    "    plt.ylabel('Train F1')\n",
    "    plt.ylim([0.3, 1.05])\n",
    "    plt.boxplot(F1_train_list)\n",
    "    plt.plot(np.arange(1, len(F1_train_list)+1), F1_train_mean, color='g', marker='o')\n",
    "    xticks = [10*x for x in range(math.ceil(len(F1_train_list)/10))]\n",
    "    plt.xticks(xticks, xticks) # xticks starts from 1\n",
    "    \n",
    "    \n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.xlabel('#Query')\n",
    "    plt.ylabel('Test F1')\n",
    "    plt.ylim([0.3, 1.05])\n",
    "    plt.boxplot(F1_test_list)\n",
    "    plt.plot(np.arange(1, len(F1_test_list)+1), F1_test_mean, color='g', marker='^')\n",
    "    plt.xticks(xticks, xticks)\n",
    "    \n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.xlabel('#Query')\n",
    "    plt.ylabel('F1')\n",
    "    plt.ylim([0.5, 0.9])\n",
    "    plt.plot(np.arange(len(F1_train_list)), F1_train_mean, ls='-.', label='Train F1 - Mean')\n",
    "    plt.plot(np.arange(len(F1_train_list)), F1_train_median, ls='--', label='Train F1 - Median')\n",
    "    plt.plot(np.arange(len(F1_test_list)), F1_test_mean, ls='-', label='Test F1 - Mean')\n",
    "    plt.plot(np.arange(len(F1_test_list)), F1_test_median, ls=':', label='Test F1 - Median')\n",
    "    plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Evaluate Random Baseline Query Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_train = seqid_set_train0.copy()\n",
    "seqid_set_query = seqid_set_query0.copy()\n",
    "seqid_set_test  = seqid_set_test0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F1_rand_train, F1_rand_test = random_baseline(seqid_set_train, seqid_set_query, seqid_set_test, \\\n",
    "                                              seq_all, seq_stats, poi_all, query_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_evaluation_results(F1_rand_train, F1_rand_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Evaluate Least Confident Query Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_train = seqid_set_train0.copy()\n",
    "seqid_set_query = seqid_set_query0.copy()\n",
    "seqid_set_test  = seqid_set_test0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F1_lc_train, F1_lc_test = least_confident(seqid_set_train, seqid_set_query, seqid_set_test, \\\n",
    "                                          seq_all, seq_stats, poi_all, query_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_evaluation_results(F1_lc_train, F1_lc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Evaluate Maximum Entropy Query Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_train = seqid_set_train0.copy()\n",
    "seqid_set_query = seqid_set_query0.copy()\n",
    "seqid_set_test  = seqid_set_test0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1_mse_train, F1_mse_test = maximum_sequence_entropy(seqid_set_train, seqid_set_query, seqid_set_test, \\\n",
    "                                                     seq_all, seq_stats, poi_all, query_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_evaluation_results(F1_mse_train, F1_mse_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
