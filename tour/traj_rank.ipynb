{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Recommendation using POI Ranking and Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Rank POIs using rankSVM\n",
    "1. Recommend a set of POIs given (start, end, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import os, re, sys, time, pickle, tempfile\n",
    "import math, random, itertools, scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as sop\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(987654321) # control random choice when splitting training/testing set\n",
    "np.random.seed(987654321)\n",
    "ranksvm_dir = '$HOME/work/ranksvm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/data-ijcai15'\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Osak.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Osak.csv')\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Glas.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Glas.csv')\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Edin.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Edin.csv')\n",
    "fvisit = os.path.join(data_dir, 'userVisits-Toro.csv')\n",
    "fcoord = os.path.join(data_dir, 'photoCoords-Toro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suffix = fvisit.split('-')[-1].split('.')[0]\n",
    "fseqpart = os.path.join(data_dir, 'seqPart-rank-' + suffix + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visits = pd.read_csv(fvisit, sep=';')\n",
    "coords = pd.read_csv(fcoord, sep=';')\n",
    "assert(visits.shape[0] == coords.shape[0])\n",
    "traj = pd.merge(visits, coords, on='photoID') # merge data frames according to column 'photoID'\n",
    "#traj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_photo = traj['photoID'].unique().shape[0]\n",
    "num_user = traj['userID'].unique().shape[0]\n",
    "num_poi = traj['poiID'].unique().shape[0]\n",
    "num_seq = traj['seqID'].unique().shape[0]\n",
    "pd.DataFrame({'#photo': num_photo, '#user': num_user, '#poi': num_poi, '#seq': num_seq, \\\n",
    "              '#photo/user': num_photo/num_user, '#seq/user': num_seq/num_user}, index=[str(suffix)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Compute POI Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute POI (Longitude, Latitude) as the average coordinates of the assigned photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_coords = traj[['poiID', 'photoLon', 'photoLat']].groupby('poiID').mean()\n",
    "poi_coords.reset_index(inplace=True)\n",
    "poi_coords.rename(columns={'photoLon':'poiLon', 'photoLat':'poiLat'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract POI category and visiting frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_catfreq = traj[['poiID', 'poiTheme', 'poiFreq']].groupby('poiID').first()\n",
    "poi_catfreq.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_all = pd.merge(poi_catfreq, poi_coords, on='poiID')\n",
    "poi_all.set_index('poiID', inplace=True)\n",
    "poi_all.head()\n",
    "#poi_all.to_csv(fpoi, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Construct Travelling Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_all = traj[['userID', 'seqID', 'poiID', 'dateTaken']].copy().groupby(['userID', 'seqID', 'poiID'])\\\n",
    "          .agg([np.min, np.max, np.size])\n",
    "seq_all.columns = seq_all.columns.droplevel()\n",
    "seq_all.reset_index(inplace=True)\n",
    "seq_all.rename(columns={'amin':'arrivalTime', 'amax':'departureTime', 'size':'#photo'}, inplace=True)\n",
    "seq_all['poiDuration(sec)'] = seq_all['departureTime'] - seq_all['arrivalTime']\n",
    "seq_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_stats = seq_all[['userID', 'seqID', 'poiID']].copy().groupby(['userID', 'seqID']).agg(np.size)\n",
    "seq_stats.reset_index(inplace=True)\n",
    "seq_stats.rename(columns={'poiID':'seqLen'}, inplace=True)\n",
    "seq_stats.set_index('seqID', inplace=True)\n",
    "seq_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_seq(seqid, seq_all):\n",
    "    seqi = seq_all[seq_all['seqID'] == seqid].copy()\n",
    "    seqi.sort_values(by=['arrivalTime'], ascending=True, inplace=True)\n",
    "    return seqi['poiID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poi_info(seqid_set, seq_all, poi_all):\n",
    "    poi_info = seq_all[seq_all['seqID'].isin(seqid_set)][['poiID', 'poiDuration(sec)']].copy()\n",
    "    poi_info = poi_info.groupby('poiID').agg([np.mean, np.size])\n",
    "    poi_info.columns = poi_info.columns.droplevel()\n",
    "    poi_info.reset_index(inplace=True)\n",
    "    poi_info.rename(columns={'mean':'avgDuration(sec)', 'size':'popularity'}, inplace=True)\n",
    "    poi_info.set_index('poiID', inplace=True)\n",
    "    poi_info['poiTheme'] = poi_all.loc[poi_info.index, 'poiTheme']\n",
    "    poi_info['poiLon'] = poi_all.loc[poi_info.index, 'poiLon']\n",
    "    poi_info['poiLat'] = poi_all.loc[poi_info.index, 'poiLat']\n",
    "    return poi_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Train vs. Query vs. Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split sequences into training set and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_percent = 0.3\n",
    "query_percent = 0.5\n",
    "seqid_set_train0 = []\n",
    "seqid_set_query0 = []\n",
    "seqid_set_test0 = []\n",
    "query_id_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate trajectories (i.e. same trajectory for different users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_stats = seq_stats[seq_stats['seqLen'] > 2]\n",
    "seq_stats = seq_stats[seq_stats['seqLen'] < 10]\n",
    "seqid_set_ = seq_stats.index.tolist()\n",
    "seq_dict = dict()\n",
    "for seqid in seqid_set_:\n",
    "    seq = extract_seq(seqid, seq_all)\n",
    "    key = str(seq)\n",
    "    if key in seq_dict: seq_dict[key].append(seqid)\n",
    "    else: seq_dict[key] = [seqid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(seqid_set_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_exp = [seq_dict[x][0] for x in sorted(seq_dict.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a *query* (in IR terminology) using tuple (start POI, end POI, #POI) ~~user ID.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqs_exp = [extract_seq(seqid, seq_all) for seqid in seqid_set_exp]\n",
    "keys = [(seq[0], seq[-1], len(seq)) for seq in seqs_exp]\n",
    "#keys = [seq_stats.loc[seqid, 'userID'] for seqid in seqid_set_exp]\n",
    "cnt = 0\n",
    "for key in keys:\n",
    "    if key not in query_id_dict:\n",
    "        query_id_dict[key] = cnt\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random split trajectories for training, querying and testing.  \n",
    "Make sure all POIs in test set are covered in trajectories for training and query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ntrain = int(train_percent * len(seqid_set_exp))\n",
    "ntrain = 1\n",
    "nquery = int(query_percent * len(seqid_set_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists(fseqpart):\n",
    "    (seqid_set_train0, seqid_set_query0, seqid_set_test0) = pickle.load(open(fseqpart, 'rb'))\n",
    "else:\n",
    "    while True:\n",
    "        np.random.shuffle(seqid_set_exp)\n",
    "        \n",
    "        seqid_set_train0 = sorted(list(seqid_set_exp[:ntrain]))\n",
    "        seqid_set_query0 = sorted(list(seqid_set_exp[ntrain:ntrain+nquery]))\n",
    "        seqid_set_test0 = sorted(list(seqid_set_exp[ntrain+nquery:]))\n",
    "        poi_train = seq_all[seq_all['seqID'].isin(seqid_set_train0)]['poiID'].unique().tolist()\n",
    "        poi_query = seq_all[seq_all['seqID'].isin(seqid_set_query0)]['poiID'].unique().tolist()\n",
    "        poi_test  = seq_all[seq_all['seqID'].isin(seqid_set_test0)]['poiID'].unique().tolist()\n",
    "        #print(poi_train)\n",
    "        #print(poi_query)\n",
    "        #print(poi_test)\n",
    "        train_query_set = set(poi_train) | set(poi_query)\n",
    "        if len(set(poi_test)) == len(train_query_set & set(poi_test)):\n",
    "            pickle.dump((seqid_set_train0, seqid_set_query0, seqid_set_test0), open(fseqpart, 'wb'))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('#seq in total:', len(seqid_set_exp))\n",
    "print('#seq for training:', ntrain, seqid_set_train0)\n",
    "print('#seq for querying:', nquery)\n",
    "print('#seq for testing:', len(seqid_set_exp)-ntrain-nquery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI Features used for ranking:\n",
    "1. `popularity`: POI popularity\n",
    "2. `avgDuration`: average POI visit duration\n",
    "3. `sameCatStart`: 1=category same as start, -1 otherwise\n",
    "4. `sameCatEnd`: 1=category same as end, -1 otherwise\n",
    "5. `distStart`: distance (haversine formula) from start\n",
    "6. `distEnd`: distance from end\n",
    "7. `seqLen`: trajectory length (copy from query)\n",
    "8. `diffPopStart`: difference in popularity from start\n",
    "9. `diffPopEnd`: difference in popularity from end\n",
    "10. `diffDurationStart`: difference in average duration of POI from the actual duration spent at start POI\n",
    "11. `diffDurationEnd`: difference in average duration of POI from the actual duration spent at end POI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features aggregated from a number of trajectories:\n",
    "1. Compute POI popularity and average duration using all trajectories from training and querying set,\n",
    "1. Use the same features that computed above for the test set, except the distance based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_info_tq = calc_poi_info(seqid_set_train0 + seqid_set_query0, seq_all, poi_all)\n",
    "print(poi_info_tq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_dist(longitude1, latitude1, longitude2, latitude2):\n",
    "    \"\"\"Calculate the distance (unit: km) between two places on earth\"\"\"\n",
    "    # convert degrees to radians\n",
    "    lon1 = math.radians(longitude1)\n",
    "    lat1 = math.radians(latitude1)\n",
    "    lon2 = math.radians(longitude2)\n",
    "    lat2 = math.radians(latitude2)\n",
    "    radius = 6371.009 # mean earth radius is 6371.009km, en.wikipedia.org/wiki/Earth_radius#Mean_radius\n",
    "    # The haversine formula, en.wikipedia.org/wiki/Great-circle_distance\n",
    "    dlon = math.fabs(lon1 - lon2)\n",
    "    dlat = math.fabs(lat1 - lat2)\n",
    "    return 2 * radius * math.asin(math.sqrt(\\\n",
    "               (math.sin(0.5*dlat))**2 + math.cos(lat1) * math.cos(lat2) * (math.sin(0.5*dlon))**2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "treat POI popularity and POI average visit duration as the properties of POI, so use all trajectories to compute these properties, do NOT just use trajectories in training set to compute these properties (otherwise, for training from just 1 trajectories, these properties of many POI are just 0, which leads to the POI ranking meaningless)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data are generated as follows:\n",
    "1. each input tuple $(\\text{startPOI}, \\text{endPOI}, \\text{#POI})$ form a `query` (in IR terminology).\n",
    "1. the label of a specific POI is the number of presence of that POI in a specific `query`, excluding the presence as $\\text{startPOI}$ or $\\text{endPOI}$.\n",
    "1. the label of all absence POIs from training set got a label 0 for each `query` in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data_df(list_of_poi, poi_info, query_id_dict, isSequence=True):\n",
    "    columns = ['poiID', 'label', 'queryID', 'popularity', 'avgDuration(sec)', 'sameCatStart', 'sameCatEnd', \\\n",
    "               'distStart', 'distEnd', 'seqLen', 'diffPopStart', 'diffPopEnd', 'diffDurationStart', 'diffDurationEnd']\n",
    "    assert(len(list_of_poi) > 2)\n",
    "    for poi in list_of_poi: \n",
    "        assert(poi in poi_info.index)\n",
    "        \n",
    "    seq = list_of_poi\n",
    "    lon0 = poi_info.loc[seq[0],  'poiLon']; lat0 = poi_info.loc[seq[0],  'poiLat']\n",
    "    lonN = poi_info.loc[seq[-1], 'poiLon']; latN = poi_info.loc[seq[-1], 'poiLat']\n",
    "    pop0 = poi_info.loc[seq[0],  'popularity']; duration0 = poi_info.loc[seq[0],  'avgDuration(sec)']\n",
    "    popN = poi_info.loc[seq[-1], 'popularity']; durationN = poi_info.loc[seq[-1], 'avgDuration(sec)']\n",
    "    cat0 = poi_info.loc[seq[0], 'poiTheme']; catN = poi_info.loc[seq[-1], 'poiTheme']\n",
    "    \n",
    "    # do NOT generate features for the startPOI and endPOI for fake sequence\n",
    "    indices = np.arange(len(seq)) if isSequence else np.arange(len(seq)-2)\n",
    "    df_ = pd.DataFrame(data=np.zeros((len(indices), len(columns)), dtype= np.float), \\\n",
    "                       columns=columns, index=indices)\n",
    "    for i in range(df_.index.shape[0]):\n",
    "        idx = df_.index[i]\n",
    "        poi = seq[i] if isSequence else seq[i+1] # skip startPOI for fake sequence\n",
    "        pop = poi_info.loc[poi, 'popularity']; duration = poi_info.loc[poi, 'avgDuration(sec)']\n",
    "        lon = poi_info.loc[poi, 'poiLon']; lat = poi_info.loc[poi, 'poiLat']\n",
    "        cat = poi_info.loc[poi, 'poiTheme']\n",
    "        df_.loc[idx, 'poiID'] = poi\n",
    "        df_.loc[idx, 'queryID'] = query_id_dict[(seq[0], seq[-1], len(seq))] if isSequence else np.nan\n",
    "        df_.loc[idx, 'popularity'] = pop\n",
    "        df_.loc[idx, 'avgDuration(sec)'] = duration\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == cat0 else -1\n",
    "        df_.loc[idx, 'sameCatEnd']   = 1 if cat == catN else -1\n",
    "        df_.loc[idx, 'distStart'] = 0 if poi == seq[0]  else calc_dist(lon, lat, lon0, lat0)\n",
    "        df_.loc[idx, 'distEnd']   = 0 if poi == seq[-1] else calc_dist(lon, lat, lonN, latN)\n",
    "        df_.loc[idx, 'seqLen'] = len(seq)\n",
    "        df_.loc[idx, 'diffPopStart'] = 0 if poi == seq[0]  else pop - pop0\n",
    "        df_.loc[idx, 'diffPopEnd']   = 0 if poi == seq[-1] else pop - popN\n",
    "        df_.loc[idx, 'diffDurationStart'] = 0 if poi == seq[0]  else duration - duration0\n",
    "        df_.loc[idx, 'diffDurationEnd']   = 0 if poi == seq[-1] else duration - durationN\n",
    "    return df_    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a string for a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data_str(df_):\n",
    "    columns = ['label', 'queryID', 'popularity', 'avgDuration(sec)', 'sameCatStart', 'sameCatEnd', 'distStart', \\\n",
    "               'distEnd', 'seqLen', 'diffPopStart', 'diffPopEnd', 'diffDurationStart', 'diffDurationEnd']\n",
    "    for col in columns:\n",
    "        assert(col in df_.columns)\n",
    "        \n",
    "    datastr = ''\n",
    "    for idx in df_.index:\n",
    "        line = str(df_.loc[idx, 'label'])\n",
    "        line += ' qid:' + str(int(df_.loc[idx, 'queryID']))\n",
    "        for j in range(2, len(columns)):\n",
    "            line += ' ' + str(j-1) + ':' + str(df_.loc[idx, columns[j]])\n",
    "        line += '\\n'\n",
    "        datastr += line\n",
    "    return datastr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate training data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_train_df(seqid_set_train, seq_all, poi_info, query_id_dict):\n",
    "    train_df = None\n",
    "    seqid_list = sorted(set(seqid_set_train))\n",
    "    seqs = [extract_seq(seqid, seq_all) for seqid in seqid_list]\n",
    "    \n",
    "    # generate label for POI in different query: count the presence of a POI in a specific query\n",
    "    # do NOT count startPOI and endPOI\n",
    "    label_dict = dict()\n",
    "    for seq in seqs:\n",
    "        qid = query_id_dict[seq[0], seq[-1], len(seq)]\n",
    "        for poi in seq[1:-1]: # exclude startPOI and endPOI\n",
    "            key = (qid, poi)\n",
    "            if key in label_dict:\n",
    "                label_dict[key] += 1\n",
    "            else:\n",
    "                label_dict[key] = 1\n",
    "    \n",
    "    # set labels for all POIs that are present in training set\n",
    "    for seq in seqs:\n",
    "        df_ = gen_data_df(seq, poi_info, query_id_dict)\n",
    "        for i in df_.index:\n",
    "            poi = int(df_.loc[i, 'poiID'])\n",
    "            qid = int(df_.loc[i, 'queryID'])\n",
    "            key = (qid, poi)\n",
    "            df_.loc[i, 'label'] = label_dict[key] if key in label_dict else 0\n",
    "    \n",
    "        if train_df is None: \n",
    "            train_df = df_\n",
    "        else:\n",
    "            train_df = train_df.append(df_, ignore_index=True) # re-generate index\n",
    "    \n",
    "    # generate negative examples for all POIs that are not in training set, set 'label' = 0\n",
    "    poi_train = seq_all[seq_all['seqID'].isin(seqid_set_train)]['poiID'].unique().tolist()\n",
    "    poi_unseen = list(set(poi_info.index) - set(poi_train))\n",
    "    qid_set = train_df['queryID'].unique().tolist()\n",
    "    for qid in qid_set:\n",
    "        start = None; end = None\n",
    "        for k, v in query_id_dict.items():\n",
    "            if v == qid:\n",
    "                start = k[0]\n",
    "                end = k[1]\n",
    "                break\n",
    "        assert(start is not None)\n",
    "        assert(end is not None)\n",
    "        fake_seq = [start] + poi_unseen + [end] # produce a fake sequence starts and ends at those of a specific qid\n",
    "        df_ = gen_data_df(fake_seq, poi_info, query_id_dict, isSequence=False)\n",
    "        df_['queryID'] = qid\n",
    "        train_df = train_df.append(df_, ignore_index=True) # re-generate index\n",
    "        \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data are generated the same way as training data, except that the labels of testing data (unknown) could be arbitrary values as suggested in [libsvm FAQ](http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#f431).\n",
    "The reported accuracy (by `svm-predict` command) is meaningless as it is calculated based on these labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_test_df(seqid, seq_all, poi_info, query_id_dict):\n",
    "    seq = extract_seq(seqid, seq_all)\n",
    "    start = seq[0]\n",
    "    end = seq[-1]\n",
    "    poi_list = poi_info.index.tolist()\n",
    "    \n",
    "    # for testing features, generate a fake sequence that contains all POIs \n",
    "    # and use the start/end of the test sequence as those of the fake sequence\n",
    "    test_df = gen_data_df([start] + poi_list + [end], poi_info, query_id_dict, isSequence=False)\n",
    "    test_df['queryID'] = query_id_dict[(start, end, len(seq))]\n",
    "    test_df['label'] = np.random.rand(test_df.shape[0]) # label for test data is arbitrary according to libsvm FAQ\n",
    "    \n",
    "    #print(start, poi_list, end)\n",
    "    #print(test_df)\n",
    "    assert(test_df.shape[0] == len(poi_list))\n",
    "    \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Ranking POIs using POI Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranking POIs using POI popularity computed using all trajectories in training and querying set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_using_popularity(poi_info):\n",
    "    poi_rank_df = poi_info['popularity'].copy()\n",
    "    poi_rank_df.rename(columns={'popularity':'rank'}, inplace=True)\n",
    "    return poi_rank_df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Ranking POIs using rankSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RankSVM implementation in libsvm can be downloaded [here (zip file)](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ranksvm/libsvm-ranksvm-3.20.zip), please read `README.ranksvm` in the zip file for installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a python wrapper of the `svm-train` or `train` and `svm-predict` or `predict` commands of rankSVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# python wrapper of rankSVM\n",
    "class RankSVM:\n",
    "    def __init__(self, bin_dir, useLinear=False, debug=False):\n",
    "        dir_ = !echo $bin_dir  # deal with environmental variables in path\n",
    "        assert(os.path.exists(dir_[0]))\n",
    "        self.bin_dir = dir_[0]\n",
    "        \n",
    "        self.bin_train = 'svm-train'\n",
    "        self.bin_predict = 'svm-predict'\n",
    "        if useLinear:\n",
    "            self.bin_train = 'train'\n",
    "            self.bin_predict = 'predict'\n",
    "        \n",
    "        assert(isinstance(debug, bool))\n",
    "        self.debug = debug\n",
    "        \n",
    "        # create named tmp files for model and feature scaling parameters\n",
    "        self.fmodel = None\n",
    "        self.fscale = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fmodel = fd.name\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fscale = fd.name\n",
    "        \n",
    "        if self.debug:\n",
    "            print('model file:', self.fmodel)\n",
    "            print('feature scaling parameter file:', self.fscale)\n",
    "    \n",
    "    \n",
    "    def __del__(self):\n",
    "        # remove tmp files\n",
    "        if self.fmodel is not None and os.path.exists(self.fmodel):\n",
    "            os.unlink(self.fmodel)\n",
    "        if self.fscale is not None and os.path.exists(self.fscale):\n",
    "            os.unlink(self.fscale)\n",
    "    \n",
    "    \n",
    "    def train(self, train_df, cost=1):\n",
    "        # cost is parameter C in SVM\n",
    "        # write train data to file\n",
    "        ftrain = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain = fd.name\n",
    "            datastr = gen_data_str(train_df)\n",
    "            fd.write(datastr)\n",
    "        \n",
    "        # feature scaling\n",
    "        ftrain_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -s $self.fscale $ftrain > $ftrain_scaled\n",
    "        \n",
    "        if self.debug:\n",
    "            print('train data file:', ftrain)\n",
    "            print('feature scaled train data file:', ftrain_scaled)\n",
    "        \n",
    "        # train rank svm and generate model file, if the model file exists, rewrite it\n",
    "        #n_cv = 10  # parameter k for k-fold cross-validation, NO model file will be generated in CV mode\n",
    "        #result = !$self.bin_dir/svm-train -c $cost -v $n_cv $ftrain $self.fmodel\n",
    "        result = !$self.bin_dir/$self.bin_train -c $cost $ftrain_scaled $self.fmodel\n",
    "        if self.debug:\n",
    "            print('Training finished.')\n",
    "            for i in range(len(result)): print(result[i])\n",
    "\n",
    "        # remove train data file\n",
    "        os.unlink(ftrain)\n",
    "        os.unlink(ftrain_scaled)\n",
    "        \n",
    "    \n",
    "    def predict(self, test_df, poi_list):\n",
    "        \"\"\"\n",
    "        Row `i` in DataFrame `test_df` corresponds to the features of POI `poi_list[i]`\n",
    "        \"\"\"\n",
    "        assert(test_df.shape[0] == len(poi_list))\n",
    "        \n",
    "        if self.fmodel is None or not os.path.exists(self.fmodel):\n",
    "            print('Model should be trained before predicting')\n",
    "            return\n",
    "        \n",
    "        # write test data to file\n",
    "        ftest = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftest = fd.name\n",
    "            datastr = gen_data_str(test_df)\n",
    "            fd.write(datastr)\n",
    "                \n",
    "        # feature scaling\n",
    "        ftest_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            ftest_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -r $self.fscale $ftest > $ftest_scaled\n",
    "            \n",
    "        # generate prediction file\n",
    "        fpredict = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            fpredict = fd.name\n",
    "            \n",
    "        if self.debug:\n",
    "            print('test data file:', ftest)\n",
    "            print('feature scaled test data file:', ftest_scaled)\n",
    "            print('predict result file:', fpredict)\n",
    "        \n",
    "            \n",
    "        # predict using trained model and write prediction to file\n",
    "        result = !$self.bin_dir/$self.bin_predict $ftest_scaled $self.fmodel $fpredict\n",
    "        if self.debug:\n",
    "            print('Predict result: %-30s  %s' % (result[0], result[1]))\n",
    "        \n",
    "        # generate prediction DataFrame from prediction file\n",
    "        poi_rank_df = pd.read_csv(fpredict, header=None)\n",
    "        poi_rank_df.rename(columns={0:'rank'}, inplace=True)\n",
    "        poi_rank_df['poiID'] = poi_list\n",
    "        poi_rank_df.set_index('poiID', inplace=True)\n",
    "        \n",
    "        # remove test file and prediction file\n",
    "        os.unlink(ftest)\n",
    "        os.unlink(ftest_scaled)\n",
    "        os.unlink(fpredict)\n",
    "        \n",
    "        return poi_rank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Recommend Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enum_seq(poi_list, startPOI, endPOI, nPOI):\n",
    "    \"\"\"\n",
    "    Enumerate all possible subset of poi_list without startPOI and endPOI,\n",
    "    The size of subset is nPOI-2\n",
    "    \"\"\"\n",
    "    assert(nPOI > 2)\n",
    "    assert(nPOI < len(poi_list))\n",
    "    tuples = itertools.combinations([p for p in poi_list if p not in {startPOI, endPOI}], nPOI-2)\n",
    "    return [[startPOI] + list(x) + [endPOI] for x in tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_F1score(seq_act, seq_rec, includeStartEnd=True):\n",
    "    assert(len(seq_act) > 2)\n",
    "    assert(len(seq_rec) > 2)\n",
    "\n",
    "    act_set = set(seq_act)\n",
    "    rec_set = set(seq_rec)\n",
    "    intersect = act_set & rec_set\n",
    "    \n",
    "    intersize = len(intersect) - 2\n",
    "    if includeStartEnd:\n",
    "        intersize = len(intersect)\n",
    "    \n",
    "    recall = intersize / len(act_set)\n",
    "    precision = intersize / len(rec_set)\n",
    "    F1score = 2. * precision * recall / (precision + recall)\n",
    "  \n",
    "    #return precision, recall, F1score\n",
    "    return F1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(seqid_set_test, seq_all, ranksvm, poi_info, query_id_dict, debug=False):\n",
    "    assert(isinstance(debug, bool))\n",
    "    F1scores = []\n",
    "    \n",
    "    for seqid in seqid_set_test:\n",
    "        test_df = gen_test_df(seqid, seq_all, poi_info, query_id_dict)\n",
    "        poi_rank_df = ranksvm.predict(test_df, poi_info.index)\n",
    "        pois = poi_rank_df.index.tolist()\n",
    "        np.random.shuffle(pois) # tie breaking\n",
    "        pois = np.array(pois)\n",
    "        ranks = poi_rank_df.loc[pois, 'rank'].get_values()\n",
    "        ranked_poi = pois[ranks.argsort()][::-1]  # highest rank --> lowest rank \n",
    "        ranked_poi = list(ranked_poi)\n",
    "        \n",
    "        # randomly put POIs that haven't seen before at the bottom of ranking\n",
    "        #unseen_poi = sorted(list(set(poi_all.index) - set(pois)))\n",
    "        #np.random.shuffle(unseen_poi)\n",
    "        #ranked_poi = list(ranked_poi) + unseen_poi\n",
    "        \n",
    "        # recommend the top ranked POIs\n",
    "        seq = extract_seq(seqid, seq_all)\n",
    "        poi_rec = [p for p in ranked_poi if p not in {seq[0], seq[-1]}][:len(seq)-2]\n",
    "        seq_rec = [seq[0]] + poi_rec + [seq[-1]]\n",
    "        F1 = calc_F1score(seq, seq_rec)\n",
    "        F1scores.append(F1)\n",
    "        if debug:\n",
    "            #print(seq, 'qid:', query_id_dict[(seq[0], seq[-1], len(seq))])\n",
    "            #print('unseen POI:', unseen_poi)\n",
    "            #print(test_df)\n",
    "            print('POI ranking:', ranked_poi)\n",
    "            #print('%.2f: %-20s -> %s' % (F1, str(seq), str(seq_rec)))\n",
    "    return F1scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = gen_train_df(seqid_set_train0 + seqid_set_query0[:30], seq_all, poi_info_tq, query_id_dict)\n",
    "ranksvm = RankSVM(ranksvm_dir)\n",
    "ranksvm.train(train_df, 1000)\n",
    "F1scores = evaluate(seqid_set_train0, seq_all, ranksvm, poi_info_tq, query_id_dict, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = gen_train_df(seqid_set_train0 + seqid_set_query0[:50], seq_all, poi_info_tq, query_id_dict)\n",
    "ranksvm = RankSVM(ranksvm_dir)\n",
    "ranksvm.train(train_df, 1000)\n",
    "F1scores = evaluate(seqid_set_train0, seq_all, ranksvm, poi_info_tq, query_id_dict, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = gen_train_df(seqid_set_test0, seq_all, poi_info_tq, query_id_dict)\n",
    "ranksvm = RankSVM(ranksvm_dir)\n",
    "ranksvm.train(train_df, 1000)\n",
    "F1scores = evaluate(seqid_set_train0, seq_all, ranksvm, poi_info_tq, query_id_dict, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F1scores = evaluate([seqid_set_query0[20]], seq_all, ranksvm, poi_info_tq, query_id_dict, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Random Baseline - Passive Learing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a sequence to query uniformly at random, i.e. passive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_baseline(seqid_set_train, seqid_set_query, seqid_set_test, seq_all, poi_info, query_id_dict):\n",
    "    np.random.shuffle(seqid_set_query)\n",
    "    query_seqs = [extract_seq(seqid, seq_all) for seqid in seqid_set_query]\n",
    "    F1scores_train = []\n",
    "    F1scores_test  = []\n",
    "    \n",
    "    cnt = 0\n",
    "    while len(seqid_set_query) > 0:\n",
    "        # compute features for training\n",
    "        #poi_info_train = calc_poi_info(seqid_set_train, seq_all, poi_all)\n",
    "        train_df = gen_train_df(seqid_set_train, seq_all, poi_info, query_id_dict)\n",
    "        \n",
    "        # training rankSVM\n",
    "        #C = 100\n",
    "        #C = 300\n",
    "        C = 1000\n",
    "        #C = 1e6\n",
    "        ranksvm = RankSVM(ranksvm_dir)#, debug=True)\n",
    "        ranksvm.train(train_df, C)\n",
    " \n",
    "        # compute training accuracy\n",
    "        F1scores = evaluate(seqid_set_train, seq_all, ranksvm, poi_info, query_id_dict, debug=True)\n",
    "        F1scores_train.append(F1scores)\n",
    "        print('Iteration %d, train mean F1: %.2f\\n' % (cnt, np.mean(F1scores)))\n",
    "        \n",
    "        # compute testing accuracy\n",
    "        F1scores = evaluate(seqid_set_test, seq_all, ranksvm, poi_info, query_id_dict)#, debug=True)\n",
    "        F1scores_test.append(F1scores)\n",
    "        print('Iteration %d, test  mean F1: %.2f\\n' % (cnt, np.mean(F1scores)))\n",
    "        \n",
    "        # query strategy\n",
    "        seq_idx = -1 # the last element after random shuffle\n",
    "        seqid_set_train.append(seqid_set_query[seq_idx])\n",
    "        print('choose sequence:', query_seqs[seq_idx])\n",
    "        del seqid_set_query[seq_idx]\n",
    "        del query_seqs[seq_idx]\n",
    "        \n",
    "        cnt += 1\n",
    "        \n",
    "    return F1scores_train, F1scores_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_evaluation_results(F1_train_list, F1_test_list):\n",
    "    assert(len(F1_train_list) == len(F1_test_list))\n",
    "    F1_train_mean = [np.mean(x) for x in F1_train_list]\n",
    "    F1_train_median = [np.median(x) for x in F1_train_list]\n",
    "    F1_test_mean = [np.mean(x) for x in F1_test_list]\n",
    "    F1_test_median = [np.median(x) for x in F1_test_list]\n",
    "    \n",
    "    plt.figure(figsize=[15, 18])\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.xlabel('#Query')\n",
    "    plt.ylabel('Train F1')\n",
    "    plt.ylim([0.3, 1.05])\n",
    "    plt.boxplot(F1_train_list)\n",
    "    plt.plot(np.arange(1, len(F1_train_list)+1), F1_train_mean, color='g', marker='o')\n",
    "    xticks = [10*x for x in range(math.ceil(len(F1_train_list)/10))]\n",
    "    plt.xticks(xticks, xticks) # xticks starts from 1\n",
    "    \n",
    "    \n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.xlabel('#Query')\n",
    "    plt.ylabel('Test F1')\n",
    "    plt.ylim([0.3, 1.05])\n",
    "    plt.boxplot(F1_test_list)\n",
    "    plt.plot(np.arange(1, len(F1_test_list)+1), F1_test_mean, color='g', marker='^')\n",
    "    plt.xticks(xticks, xticks)\n",
    "    \n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.xlabel('#Query')\n",
    "    plt.ylabel('F1')\n",
    "    plt.ylim([0.5, 0.9])\n",
    "    plt.plot(np.arange(len(F1_train_list)), F1_train_mean, ls='-.', label='Train F1 - Mean')\n",
    "    plt.plot(np.arange(len(F1_train_list)), F1_train_median, ls='--', label='Train F1 - Median')\n",
    "    plt.plot(np.arange(len(F1_test_list)), F1_test_mean, ls='-', label='Test F1 - Mean')\n",
    "    plt.plot(np.arange(len(F1_test_list)), F1_test_median, ls=':', label='Test F1 - Median')\n",
    "    plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Evaluate Random Baseline Query Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_train = seqid_set_train0.copy()\n",
    "seqid_set_query = seqid_set_query0.copy()\n",
    "seqid_set_test  = seqid_set_test0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "F1_rand_train, F1_rand_test = random_baseline(seqid_set_train, seqid_set_query, seqid_set_test, \\\n",
    "                                              seq_all, poi_info_tq, query_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_evaluation_results(F1_rand_train, F1_rand_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
