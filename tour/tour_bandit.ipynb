{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trajectory Recommendation with Active Learning\n",
    "==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulate trajectory recommendation as a contextual multi-armed bandit problem, the trade-off is recommendation with available POI/user features vs. query user preference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Arm: transition from current POI$_i$ to another POI$_j$, (allow sub-tours?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Context: transition features including POI category transition probabilities, POI popularity transition probabilities, distance of POI pair transition probabilities and user specific features (visit duration/frequency), available in [this notebook ](./tour_MC.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Reward: a F1-scored based metric, e.g. let the trajectory up to time $t$ (after choosing a certain arm) is `[p1, p2, p3]`, ground truth trajectories `[p1, p2, p3]` and `[p1, p2, p4]`, the reward is computed as \n",
    "\\begin{equation}\n",
    "max\\{F1([p1, p2, p3], [p1, p2, p3]), F1([p1, p2, p3], [p1, p2, p4])\\} = max\\{1.0, 0.667\\} = 1.0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Regret: regret after $T$ trials is defined as \n",
    "\\begin{equation}\n",
    "R(T) = \\textbf{E}\\left[\\sum_t^T(\\text{optimal reward})_t\\right] - \\textbf{E}\\left[\\sum_t^T(\\text{actual reward})_t\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Sampling strategies: \n",
    "1. $\\epsilon$-greedy: choose arm (i.e. transition) of highest log likelihood with probability 1-$\\epsilon$, choose a random arm with probability $\\epsilon$.\n",
    "\n",
    "1. Thompson sampling: design a prior of arm's reward (uniform? gaussian? ...), compute likelihood like [this approach](./tour_MC.ipynb#sec4).\n",
    "\n",
    "1. UCB-type strategies such as LinUCB introduced in [this paper](http://www.research.rutgers.edu/~lihong/pub/Li10Contextual.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Query: query users which POIs they prefer or what the characteristics (category/popularity/distance from current place) of POIs they prefer (show selected photos of these POIs?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
