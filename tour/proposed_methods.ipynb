{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trajectory Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Contents](#toc)\n",
    "1. [Preprocess Dataset](#sec1)\n",
    "  1. [Load Data](#sec1.1)\n",
    "  1. [Utility function](#sec1.2)\n",
    "1. [POI Ranking](#sec2)\n",
    "  1. [POI Features for Ranking](#sec2.1)\n",
    "  1. [Training DataFrame](#sec2.2)\n",
    "  1. [Test DataFrame](#sec2.3)\n",
    "  1. [Ranking POIs using rankSVM](#sec2.4)\n",
    "  1. [Recommendation based on POI Ranking](#sec2.5)\n",
    "1. [Factorise Transition Probabilities in POI Feature Space](#sec3)\n",
    "  1. [POI Features for Factorisation](#sec3.1)\n",
    "  1. [Transition Matrix between POI Cateogries](#sec3.2)\n",
    "  1. [Transition Matrix between POI Popularity Classes](#sec3.3)\n",
    "  1. [Transition Matrix between the Number of POI Visit Classes](#sec3.4)\n",
    "  1. [Transition Matrix between POI Average Visit Duration Classes](#sec3.5)\n",
    "  1. [Transition Matrix between POI Neighborhood Classes](#sec3.6)\n",
    "  1. [Transition Matrix between POIs](#sec3.7)\n",
    "  1. [Recommendation based on POI Transition Matrix](#sec3.8)\n",
    "1. [Combine POI Ranking with Factorised Markov Chain](#sec4)\n",
    "1. [Random Guessing](#sec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import os, sys, time, pickle, tempfile\n",
    "import math, random, itertools, scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as sop\n",
    "from scipy.linalg import kron\n",
    "from scipy.misc import logsumexp\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "import pulp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(987654321) # control random choice when splitting training/testing set\n",
    "np.random.seed(987654321)\n",
    "ranksvm_dir = '$HOME/work/ranksvm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/data-recsys16'\n",
    "dat_suffix = ['Osak', 'Glas', 'Edin', 'Toro', 'Melb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_ix = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_types = ['all', 'nofew']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type_ix = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noloop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uspecific = False\n",
    "KX = 100  # KX folds in user specific setting [100, 50, 20, 10, 8, 4, 2, 1]\n",
    "kxstr = str(KX) + 'X-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ALPHA = 0.5  # [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "alphastr = str(ALPHA).replace('.', '_') + '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BIN_CLUSTER = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method switches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_rank = False\n",
    "run_tran = False\n",
    "run_comb = False\n",
    "run_rand = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpoi = os.path.join(data_dir, 'poi-' + dat_suffix[dat_ix] + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/data-recsys16/poi-Toro.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if noloop == True:\n",
    "    ftraj = os.path.join(data_dir, 'traj-noloop-' + dat_types[type_ix] + '-' + dat_suffix[dat_ix] + '.csv')\n",
    "else:\n",
    "    ftraj = os.path.join(data_dir, 'traj-' + dat_types[type_ix] + '-' + dat_suffix[dat_ix] + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/data-recsys16/traj-noloop-all-Toro.csv'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftraj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files to store results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_fname(data_dir, dat_suffix, dat_ix, dat_types, type_ix, noloop, alpha=ALPHA, uspecific=False):\n",
    "    assert(0 <= dat_ix < len(dat_suffix))\n",
    "    assert(0 <= type_ix < len(dat_types))\n",
    "    assert(isinstance(noloop, bool))\n",
    "    assert(0 < alpha < 1)\n",
    "    alpha_str = str(ALPHA).replace('.', '_') + '-'\n",
    "    \n",
    "    if noloop == True:\n",
    "        loop_str = 'noloop-'\n",
    "    else:\n",
    "        loop_str = ''\n",
    "    \n",
    "    type_str = dat_types[type_ix] + '-'\n",
    "    \n",
    "    if uspecific == True:\n",
    "        user_str = 'specific-'\n",
    "    else:\n",
    "        user_str = 'agnostic-'\n",
    "    \n",
    "    fname1 = loop_str + type_str + user_str + dat_suffix[dat_ix] + '.pkl'\n",
    "    fname2 = loop_str + type_str + user_str + alpha_str + dat_suffix[dat_ix] + '.pkl'\n",
    "    \n",
    "    frank = os.path.join(data_dir, 'rank-' + fname1)\n",
    "    ftran = os.path.join(data_dir, 'tran-' + fname1)\n",
    "    fcomb = os.path.join(data_dir, 'comb-' + fname2)\n",
    "    return frank, ftran, fcomb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frecdict_rank, frecdict_tran, frecdict_comb = gen_fname(data_dir, dat_suffix, dat_ix, dat_types, type_ix, noloop, \\\n",
    "                                                        ALPHA, uspecific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data-recsys16/rank-noloop-all-agnostic-Toro.pkl\n",
      "data/data-recsys16/tran-noloop-all-agnostic-Toro.pkl\n",
      "data/data-recsys16/comb-noloop-all-agnostic-0_5-Toro.pkl\n"
     ]
    }
   ],
   "source": [
    "print(frecdict_rank)\n",
    "print(frecdict_tran)\n",
    "print(frecdict_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>trajID</th>\n",
       "      <th>poiID</th>\n",
       "      <th>startTime</th>\n",
       "      <th>endTime</th>\n",
       "      <th>#photo</th>\n",
       "      <th>trajLen</th>\n",
       "      <th>poiDuration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10007579@N00</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1346844688</td>\n",
       "      <td>1346844688</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1142731848</td>\n",
       "      <td>1142732445</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1142916492</td>\n",
       "      <td>1142916492</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>1319327174</td>\n",
       "      <td>1319332848</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10014440@N06</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1196128621</td>\n",
       "      <td>1196128878</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         userID  trajID  poiID   startTime     endTime  #photo  trajLen  \\\n",
       "0  10007579@N00       1     30  1346844688  1346844688       1        1   \n",
       "1  10012675@N05       2      6  1142731848  1142732445       4        1   \n",
       "2  10012675@N05       3      6  1142916492  1142916492       1        1   \n",
       "3  10012675@N05       4     13  1319327174  1319332848       9        1   \n",
       "4  10014440@N06       5     24  1196128621  1196128878       3        1   \n",
       "\n",
       "   poiDuration  \n",
       "0            0  \n",
       "1          597  \n",
       "2            0  \n",
       "3         5674  \n",
       "4          257  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_all = pd.read_csv(ftraj)\n",
    "traj_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poiCat</th>\n",
       "      <th>poiLon</th>\n",
       "      <th>poiLat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poiID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sport</td>\n",
       "      <td>-79.379243</td>\n",
       "      <td>43.643183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sport</td>\n",
       "      <td>-79.418634</td>\n",
       "      <td>43.632772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sport</td>\n",
       "      <td>-79.380045</td>\n",
       "      <td>43.662175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sport</td>\n",
       "      <td>-79.389290</td>\n",
       "      <td>43.641297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cultural</td>\n",
       "      <td>-79.392396</td>\n",
       "      <td>43.653662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         poiCat     poiLon     poiLat\n",
       "poiID                                \n",
       "1         Sport -79.379243  43.643183\n",
       "2         Sport -79.418634  43.632772\n",
       "3         Sport -79.380045  43.662175\n",
       "4         Sport -79.389290  43.641297\n",
       "6      Cultural -79.392396  43.653662"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_all = pd.read_csv(fpoi)\n",
    "poi_all.set_index('poiID', inplace=True)\n",
    "poi_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#poi</th>\n",
       "      <th>#traj</th>\n",
       "      <th>#traj/user</th>\n",
       "      <th>#user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Toro</th>\n",
       "      <td>29</td>\n",
       "      <td>6057</td>\n",
       "      <td>4.341935</td>\n",
       "      <td>1395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      #poi  #traj  #traj/user  #user\n",
       "Toro    29   6057    4.341935   1395"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_user = traj_all['userID'].unique().shape[0]\n",
    "num_poi = traj_all['poiID'].unique().shape[0]\n",
    "num_traj = traj_all['trajID'].unique().shape[0]\n",
    "#assert(num_poi == poi_all.shape[0])\n",
    "pd.DataFrame({'#user': num_user, '#poi': num_poi, '#traj': num_traj, '#traj/user': num_traj/num_user}, \\\n",
    "             index=[str(dat_suffix[dat_ix])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFYCAYAAACcb79EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbd2c873c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = traj_all['trajLen'].hist(bins=20)\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping trajectory to user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trajID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10007579@N00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10012675@N05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10012675@N05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10012675@N05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10014440@N06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              userID\n",
       "trajID              \n",
       "1       10007579@N00\n",
       "2       10012675@N05\n",
       "3       10012675@N05\n",
       "4       10012675@N05\n",
       "5       10014440@N06"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_user = traj_all[['trajID', 'userID']].copy().groupby('trajID').first()\n",
    "traj_user.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Utility function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print computing progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_progress(cnt, total):\n",
    "    \"\"\"Display a progress bar\"\"\"\n",
    "    assert(cnt > 0 and total > 0 and cnt <= total)\n",
    "    length = 80\n",
    "    ratio = cnt / total\n",
    "    n = int(length * ratio)\n",
    "    sys.stdout.write('\\r[%-80s] %d%%' % ('-'*n, int(ratio*100)))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract trajectory, i.e., a list of POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_traj(tid, traj_all):\n",
    "    traj = traj_all[traj_all['trajID'] == tid].copy()\n",
    "    traj.sort_values(by=['startTime'], ascending=True, inplace=True)\n",
    "    return traj['poiID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tid in traj_all['trajID'].unique():\n",
    "    t = extract_traj(tid, traj_all)\n",
    "    if len(t) != len(set(t)): print(tid, ':', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute POI properties, e.g., popularity, total number of visit, average visit duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>trajID</th>\n",
       "      <th>poiID</th>\n",
       "      <th>startTime</th>\n",
       "      <th>endTime</th>\n",
       "      <th>#photo</th>\n",
       "      <th>trajLen</th>\n",
       "      <th>poiDuration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>10502709@N05</td>\n",
       "      <td>67</td>\n",
       "      <td>22</td>\n",
       "      <td>1211647414</td>\n",
       "      <td>1211649578</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>10502709@N05</td>\n",
       "      <td>67</td>\n",
       "      <td>23</td>\n",
       "      <td>1211641612</td>\n",
       "      <td>1211646745</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>10502709@N05</td>\n",
       "      <td>67</td>\n",
       "      <td>28</td>\n",
       "      <td>1211634164</td>\n",
       "      <td>1211651552</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>17388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          userID  trajID  poiID   startTime     endTime  #photo  trajLen  \\\n",
       "71  10502709@N05      67     22  1211647414  1211649578       3        3   \n",
       "72  10502709@N05      67     23  1211641612  1211646745       5        3   \n",
       "73  10502709@N05      67     28  1211634164  1211651552       4        3   \n",
       "\n",
       "    poiDuration  \n",
       "71         2164  \n",
       "72         5133  \n",
       "73        17388  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_all[traj_all['trajID'] == 67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poi_info(trajid_list, traj_all, poi_all):\n",
    "    assert(len(trajid_list) > 0)\n",
    "    # to allow duplicated trajid\n",
    "    poi_info = traj_all[traj_all['trajID'] == trajid_list[0]][['poiID', 'poiDuration']].copy() \n",
    "    for i in range(1, len(trajid_list)):\n",
    "        traj = traj_all[traj_all['trajID'] == trajid_list[i]][['poiID', 'poiDuration']]\n",
    "        poi_info = poi_info.append(traj, ignore_index=True)\n",
    "    \n",
    "    poi_info = poi_info.groupby('poiID').agg([np.mean, np.size])\n",
    "    poi_info.columns = poi_info.columns.droplevel()\n",
    "    poi_info.reset_index(inplace=True)\n",
    "    poi_info.rename(columns={'mean':'avgDuration', 'size':'nVisit'}, inplace=True)\n",
    "    poi_info.set_index('poiID', inplace=True) \n",
    "    poi_info['poiCat'] = poi_all.loc[poi_info.index, 'poiCat']\n",
    "    poi_info['poiLon'] = poi_all.loc[poi_info.index, 'poiLon']\n",
    "    poi_info['poiLat'] = poi_all.loc[poi_info.index, 'poiLat']\n",
    "    \n",
    "    # POI popularity: the number of distinct users that visited the POI\n",
    "    pop_df = traj_all[traj_all['trajID'].isin(trajid_list)][['poiID', 'userID']].copy()\n",
    "    pop_df = pop_df.groupby('poiID').agg(pd.Series.nunique)\n",
    "    pop_df.rename(columns={'userID':'nunique'}, inplace=True)\n",
    "    poi_info['popularity'] = pop_df.loc[poi_info.index, 'nunique']\n",
    "    \n",
    "    return poi_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the F1 score for recommended trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_F1(seq_act, seq_rec):\n",
    "    '''Compute recall, precision and F1 when trajectories contain sub-tours'''\n",
    "    assert(len(seq_act) > 0)\n",
    "    assert(len(seq_rec) > 0)\n",
    "    #actset = set(seq_act)\n",
    "    #recset = set(seq_rec)\n",
    "    #intersect = actset & recset\n",
    "    \n",
    "    match_tags = np.zeros(len(seq_act), dtype=np.bool)\n",
    "    for poi in seq_rec:\n",
    "        for j in range(len(seq_act)):\n",
    "            if match_tags[j] == False and poi == seq_act[j]:\n",
    "                match_tags[j] = True\n",
    "                break\n",
    "    intersize = np.nonzero(match_tags)[0].shape[0]\n",
    "    recall = intersize / len(seq_act)\n",
    "    precision = intersize / len(seq_rec)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute distance between two POIs using [Haversine formula](http://en.wikipedia.org/wiki/Great-circle_distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_dist_vec(longitudes1, latitudes1, longitudes2, latitudes2):\n",
    "    \"\"\"Calculate the distance (unit: km) between two places on earth, vectorised\"\"\"\n",
    "    # convert degrees to radians\n",
    "    lng1 = np.radians(longitudes1)\n",
    "    lat1 = np.radians(latitudes1)\n",
    "    lng2 = np.radians(longitudes2)\n",
    "    lat2 = np.radians(latitudes2)\n",
    "    radius = 6371.0088 # mean earth radius, en.wikipedia.org/wiki/Earth_radius#Mean_radius\n",
    "\n",
    "    # The haversine formula, en.wikipedia.org/wiki/Great-circle_distance\n",
    "    dlng = np.fabs(lng1 - lng2)\n",
    "    dlat = np.fabs(lat1 - lat2)\n",
    "    dist =  2 * radius * np.arcsin( np.sqrt( \n",
    "                (np.sin(0.5*dlat))**2 + np.cos(lat1) * np.cos(lat2) * (np.sin(0.5*dlng))**2 ))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance between POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_distmat = pd.DataFrame(data=np.zeros((poi_all.shape[0], poi_all.shape[0]), dtype=np.float), \\\n",
    "                           index=poi_all.index, columns=poi_all.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ix in poi_all.index:\n",
    "    poi_distmat.loc[ix] = calc_dist_vec(poi_all.loc[ix, 'poiLon'], \\\n",
    "                                        poi_all.loc[ix, 'poiLat'], \\\n",
    "                                        poi_all['poiLon'], \\\n",
    "                                        poi_all['poiLat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a *query* (in IR terminology) using tuple (start POI, end POI, #POI) ~~user ID.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trajid_set_all = sorted(traj_all['trajID'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_id_dict = dict()  # (start, end, length) --> qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trajs = [extract_traj(tid, traj_all) for tid in trajid_set_all]\n",
    "keys = [(t[0], t[-1], len(t)) for t in trajs if len(t) > 2]\n",
    "cnt = 0\n",
    "for key in keys:\n",
    "    if key not in query_id_dict:   # (start, end, length) --> qid\n",
    "        query_id_dict[key] = cnt\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avgDuration</th>\n",
       "      <th>nVisit</th>\n",
       "      <th>poiCat</th>\n",
       "      <th>poiLon</th>\n",
       "      <th>poiLat</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poiID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3018.941558</td>\n",
       "      <td>308</td>\n",
       "      <td>Sport</td>\n",
       "      <td>-79.379243</td>\n",
       "      <td>43.643183</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2332.102564</td>\n",
       "      <td>78</td>\n",
       "      <td>Sport</td>\n",
       "      <td>-79.418634</td>\n",
       "      <td>43.632772</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1418.300000</td>\n",
       "      <td>200</td>\n",
       "      <td>Sport</td>\n",
       "      <td>-79.380045</td>\n",
       "      <td>43.662175</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5291.538012</td>\n",
       "      <td>342</td>\n",
       "      <td>Sport</td>\n",
       "      <td>-79.389290</td>\n",
       "      <td>43.641297</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1379.530435</td>\n",
       "      <td>345</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>-79.392396</td>\n",
       "      <td>43.653662</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       avgDuration  nVisit    poiCat     poiLon     poiLat  popularity\n",
       "poiID                                                                 \n",
       "1      3018.941558     308     Sport -79.379243  43.643183         129\n",
       "2      2332.102564      78     Sport -79.418634  43.632772          55\n",
       "3      1418.300000     200     Sport -79.380045  43.662175          83\n",
       "4      5291.538012     342     Sport -79.389290  43.641297         155\n",
       "6      1379.530435     345  Cultural -79.392396  43.653662         209"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_info_ = calc_poi_info(trajid_set_all, traj_all, poi_all)\n",
    "poi_info_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#traj in total: 6057\n",
      "#traj (length > 2): 335\n",
      "#query tuple: 232\n"
     ]
    }
   ],
   "source": [
    "print('#traj in total:', len(trajid_set_all))\n",
    "print('#traj (length > 2):', traj_all[traj_all['trajLen'] > 2]['trajID'].unique().shape[0])\n",
    "print('#query tuple:', len(query_id_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. POI Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 POI Features for Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI Features used for ranking:\n",
    "1. `popularity`: POI popularity, i.e., the number of distinct users that visited the POI\n",
    "1. `nVisit`: the total number of visit by all users\n",
    "1. `avgDuration`: average POI visit duration\n",
    "1. `sameCatStart`: 1 if POI category is the same as that of `startPOI`, -1 otherwise\n",
    "1. `sameCatEnd`: 1 if POI category is the same as that of `endPOI`, -1 otherwise\n",
    "1. `distStart`: distance (haversine formula) from `startPOI`\n",
    "1. `distEnd`: distance from `endPOI`\n",
    "1. `seqLen`: trajectory length (copy from query)\n",
    "1. `diffPopStart`: difference in POI popularity from `startPOI`\n",
    "1. `diffPopEnd`: difference in POI popularity from `endPOI`\n",
    "1. `diffNVisitStart`: difference in the total number of visit from `startPOI`\n",
    "1. `diffNVisitEnd`: difference in the total number of visit from `endPOI`\n",
    "1. `diffDurationStart`: difference in average POI visit duration from the actual duration spent at `startPOI`\n",
    "1. `diffDurationEnd`: difference in average POI visit duration from the actual duration spent at `endPOI`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_columns = ['poiID', 'label', 'queryID', 'popularity', 'nVisit', 'avgDuration', \\\n",
    "              'sameCatStart', 'sameCatEnd', 'distStart', 'distEnd', 'trajLen', 'diffPopStart', \\\n",
    "              'diffPopEnd', 'diffNVisitStart', 'diffNVisitEnd', 'diffDurationStart', 'diffDurationEnd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Features aggregated from a number of trajectories:~~\n",
    "~~1. Compute POI `popularity` and average visit `duration` using all trajectories from training and querying set,~~\n",
    "~~1. Use the same features that computed above for the test set, except the distance based features.~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data are generated as follows:\n",
    "1. each input tuple $(\\text{startPOI}, \\text{endPOI}, \\text{#POI})$ form a `query` (in IR terminology).\n",
    "1. the label of a specific POI is the number of presence of that POI in a specific `query`, excluding the presence as $\\text{startPOI}$ or $\\text{endPOI}$.\n",
    "1. for each `query`, the label of all absence POIs from trajectories of that `query` in training set got a label 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of training data matrix is `#(qid, poi)` by `#feature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_train_subdf(poi_id, query_id_set, columns, poi_info, poi_distmat, query_id_rdict):\n",
    "    df_ = pd.DataFrame(data=np.zeros((len(query_id_set), len(columns)), dtype=np.float), columns=columns)\n",
    "    \n",
    "    pop = poi_info.loc[poi_id, 'popularity']; nvisit = poi_info.loc[poi_id, 'nVisit']\n",
    "    cat = poi_info.loc[poi_id, 'poiCat']; duration = poi_info.loc[poi_id, 'avgDuration']\n",
    "    \n",
    "    for j in range(len(query_id_set)):\n",
    "        qid = query_id_set[j]\n",
    "        assert(qid in query_id_rdict) # qid --> (start, end, length)\n",
    "        (p0, pN, trajLen) = query_id_rdict[qid]\n",
    "        idx = df_.index[j]\n",
    "        df_.loc[idx, 'poiID'] = poi_id\n",
    "        df_.loc[idx, 'queryID'] = qid\n",
    "        df_.loc[idx, 'popularity'] = pop\n",
    "        df_.loc[idx, 'nVisit'] = nvisit\n",
    "        df_.loc[idx, 'avgDuration'] = duration\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_info.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'sameCatEnd']   = 1 if cat == poi_info.loc[pN, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi_id, p0]\n",
    "        df_.loc[idx, 'distEnd']   = poi_distmat.loc[poi_id, pN]\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffPopEnd']   = pop - poi_info.loc[pN, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffNVisitEnd']   = nvisit - poi_info.loc[pN, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'diffDurationEnd']   = duration - poi_info.loc[pN, 'avgDuration']\n",
    "        \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_train_df(trajid_list, columns, traj_all, poi_info, poi_distmat, query_id_dict):\n",
    "    columns = columns.copy()\n",
    "    train_trajs = [extract_traj(tid, traj_all) for tid in trajid_list]\n",
    "    \n",
    "    qid_set = sorted(set([query_id_dict[(t[0], t[-1], len(t))] for t in train_trajs if len(t) > 2]))\n",
    "    poi_set = sorted(set(poi_info.index.tolist()))\n",
    "    \n",
    "    #qid_poi_pair = list(itertools.product(qid_set, poi_set)) # Cartesian product of qid_set and poi_set\n",
    "    #df_ = pd.DataFrame(data=np.zeros((len(qid_poi_pair), len(columns)), dtype= np.float), columns=columns)\n",
    "    \n",
    "    query_id_rdict = dict()\n",
    "    for k, v in query_id_dict.items(): \n",
    "        query_id_rdict[v] = k  # qid --> (start, end, length)\n",
    "    \n",
    "    train_df_list = Parallel(n_jobs=-2)\\\n",
    "                            (delayed(gen_train_subdf)(poi, qid_set, columns, poi_info, poi_distmat, query_id_rdict) \\\n",
    "                             for poi in poi_set)\n",
    "                        \n",
    "    assert(len(train_df_list) > 0)\n",
    "    df_ = train_df_list[0]\n",
    "    for j in range(1, len(train_df_list)):\n",
    "        df_ = df_.append(train_df_list[j], ignore_index=True)            \n",
    "        \n",
    "    # set label\n",
    "    df_.set_index(['queryID', 'poiID'], inplace=True)\n",
    "    for t in train_trajs:\n",
    "        if len(t) > 2:\n",
    "            qid = query_id_dict[(t[0], t[-1], len(t))]\n",
    "            for poi in t[1:-1]:  # do NOT count if the POI is startPOI/endPOI\n",
    "                df_.loc[(qid, poi), 'label'] += 1\n",
    "    \n",
    "    df_.reset_index(inplace=True)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: \n",
    "- different POIs have different features for the same query trajectory\n",
    "- the same POI get different features for different query-id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Test DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data are generated the same way as training data, except that the labels of testing data (unknown) could be arbitrary values as suggested in [libsvm FAQ](http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#f431).\n",
    "The reported accuracy (by `svm-predict` command) is meaningless as it is calculated based on these labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of training data matrix is `#poi` by `#feature` with one specific `query`, i.e. tuple $(\\text{startPOI}, \\text{endPOI}, \\text{#POI})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_test_df(startPOI, endPOI, nPOI, columns, poi_info, poi_distmat, query_id_dict):\n",
    "    columns = columns.copy()\n",
    "    key = (p0, pN, trajLen) = (startPOI, endPOI, nPOI)\n",
    "    assert(key in query_id_dict)\n",
    "    assert(p0 in poi_info.index)\n",
    "    assert(pN in poi_info.index)\n",
    "    \n",
    "    poi_set = sorted(set(poi_info.index.tolist()))\n",
    "    df_ = pd.DataFrame(data=np.zeros((len(poi_set), len(columns)), dtype= np.float), columns=columns)\n",
    "    \n",
    "    qid = query_id_dict[key]\n",
    "    df_['queryID'] = qid\n",
    "    df_['label'] = np.random.rand(df_.shape[0]) # label for test data is arbitrary according to libsvm FAQ\n",
    "\n",
    "    for i in range(df_.index.shape[0]):\n",
    "        poi = poi_set[i]\n",
    "        lon = poi_info.loc[poi, 'poiLon']; lat = poi_info.loc[poi, 'poiLat']\n",
    "        pop = poi_info.loc[poi, 'popularity']; nvisit = poi_info.loc[poi, 'nVisit']\n",
    "        cat = poi_info.loc[poi, 'poiCat']; duration = poi_info.loc[poi, 'avgDuration']\n",
    "        idx = df_.index[i]\n",
    "        df_.loc[idx, 'poiID'] = poi \n",
    "        df_.loc[idx, 'popularity'] = pop\n",
    "        df_.loc[idx, 'nVisit'] = nvisit\n",
    "        df_.loc[idx, 'avgDuration'] = duration\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_all.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'sameCatEnd']   = 1 if cat == poi_all.loc[pN, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi, p0]\n",
    "        df_.loc[idx, 'distEnd']   = poi_distmat.loc[poi, pN]\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffPopEnd']   = pop - poi_info.loc[pN, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffNVisitEnd']   = nvisit - poi_info.loc[pN, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'diffDurationEnd']   = duration - poi_info.loc[pN, 'avgDuration']\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: \n",
    "- different POIs have different features for the same query trajectory\n",
    "- the same POI get different features for different query-id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a string for a training/test data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data_str(df_, df_columns=df_columns):\n",
    "    columns = df_columns[1:].copy()  # get rid of 'poiID'\n",
    "    for col in columns:\n",
    "        assert(col in df_.columns)\n",
    "        \n",
    "    lines = []\n",
    "    for idx in df_.index:\n",
    "        slist = [str(df_.loc[idx, 'label'])]\n",
    "        slist.append(' qid:')\n",
    "        slist.append(str(int(df_.loc[idx, 'queryID'])))\n",
    "        for j in range(2, len(columns)):\n",
    "            slist.append(' ')\n",
    "            slist.append(str(j-1))\n",
    "            slist.append(':')\n",
    "            slist.append(str(df_.loc[idx, columns[j]]))\n",
    "        slist.append('\\n')\n",
    "        lines.append(''.join(slist))\n",
    "    return ''.join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2.4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Ranking POIs using rankSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RankSVM implementation in [libsvm.zip](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ranksvm/libsvm-ranksvm-3.20.zip) or [liblinear.zip](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ranksvm/liblinear-ranksvm-1.95.zip), please read `README.ranksvm` in the zip file for installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [softmax function](https://en.wikipedia.org/wiki/Softmax_function) to convert ranking scores to a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    expx = np.exp(x)\n",
    "    return expx / np.sum(expx, axis=0) # column-wise sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a python wrapper of the `svm-train` or `train` and `svm-predict` or `predict` commands of rankSVM with ranking probabilities $P(p_i \\lvert (p_s, p_e, len))$ computed using [softmax function](https://en.wikipedia.org/wiki/Softmax_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python wrapper of rankSVM\n",
    "class RankSVM:\n",
    "    def __init__(self, bin_dir, useLinear=True, debug=False):\n",
    "        dir_ = !echo $bin_dir  # deal with environmental variables in path\n",
    "        assert(os.path.exists(dir_[0]))\n",
    "        self.bin_dir = dir_[0]\n",
    "        \n",
    "        self.bin_train = 'svm-train'\n",
    "        self.bin_predict = 'svm-predict'\n",
    "        if useLinear:\n",
    "            self.bin_train = 'train'\n",
    "            self.bin_predict = 'predict'\n",
    "        \n",
    "        assert(isinstance(debug, bool))\n",
    "        self.debug = debug\n",
    "        \n",
    "        # create named tmp files for model and feature scaling parameters\n",
    "        self.fmodel = None\n",
    "        self.fscale = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fmodel = fd.name\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fscale = fd.name\n",
    "        \n",
    "        if self.debug:\n",
    "            print('model file:', self.fmodel)\n",
    "            print('feature scaling parameter file:', self.fscale)\n",
    "    \n",
    "    \n",
    "    def __del__(self):\n",
    "        # remove tmp files\n",
    "        if self.fmodel is not None and os.path.exists(self.fmodel):\n",
    "            os.unlink(self.fmodel)\n",
    "        if self.fscale is not None and os.path.exists(self.fscale):\n",
    "            os.unlink(self.fscale)\n",
    "    \n",
    "    \n",
    "    def train(self, train_df, cost=1):\n",
    "        # cost is parameter C in SVM\n",
    "        # write train data to file\n",
    "        ftrain = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain = fd.name\n",
    "            datastr = gen_data_str(train_df)\n",
    "            fd.write(datastr)\n",
    "        \n",
    "        # feature scaling\n",
    "        ftrain_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -s $self.fscale $ftrain > $ftrain_scaled\n",
    "        \n",
    "        if self.debug:\n",
    "            print('cost:', cost)\n",
    "            print('train data file:', ftrain)\n",
    "            print('feature scaled train data file:', ftrain_scaled)\n",
    "        \n",
    "        # train rank svm and generate model file, if the model file exists, rewrite it\n",
    "        #n_cv = 10  # parameter k for k-fold cross-validation, NO model file will be generated in CV mode\n",
    "        #result = !$self.bin_dir/svm-train -c $cost -v $n_cv $ftrain $self.fmodel\n",
    "        result = !$self.bin_dir/$self.bin_train -c $cost $ftrain_scaled $self.fmodel\n",
    "        if self.debug:\n",
    "            print('Training finished.')\n",
    "            for i in range(len(result)): print(result[i])\n",
    "\n",
    "        # remove train data file\n",
    "        os.unlink(ftrain)\n",
    "        os.unlink(ftrain_scaled)        \n",
    "    \n",
    "    \n",
    "    def predict(self, test_df):\n",
    "        # predict ranking scores for the given feature matrix\n",
    "        if self.fmodel is None or not os.path.exists(self.fmodel):\n",
    "            print('Model should be trained before predicting')\n",
    "            return\n",
    "        \n",
    "        # write test data to file\n",
    "        ftest = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftest = fd.name\n",
    "            datastr = gen_data_str(test_df)\n",
    "            fd.write(datastr)\n",
    "                \n",
    "        # feature scaling\n",
    "        ftest_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            ftest_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -r $self.fscale $ftest > $ftest_scaled\n",
    "            \n",
    "        # generate prediction file\n",
    "        fpredict = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            fpredict = fd.name\n",
    "            \n",
    "        if self.debug:\n",
    "            print('test data file:', ftest)\n",
    "            print('feature scaled test data file:', ftest_scaled)\n",
    "            print('predict result file:', fpredict)\n",
    "            \n",
    "        # predict using trained model and write prediction to file\n",
    "        result = !$self.bin_dir/$self.bin_predict $ftest_scaled $self.fmodel $fpredict\n",
    "        if self.debug:\n",
    "            print('Predict result: %-30s  %s' % (result[0], result[1]))\n",
    "        \n",
    "        # generate prediction DataFrame from prediction file\n",
    "        poi_rank_df = pd.read_csv(fpredict, header=None)\n",
    "        poi_rank_df.rename(columns={0:'rank'}, inplace=True)\n",
    "        poi_rank_df['poiID'] = test_df['poiID'].astype(np.int)\n",
    "        #poi_rank_df.set_index('poiID', inplace=True) # duplicated 'poiID' when evaluating training data\n",
    "        poi_rank_df['probability'] = softmax(poi_rank_df['rank'])  # softmax\n",
    "        \n",
    "        # remove test file and prediction file\n",
    "        os.unlink(ftest)\n",
    "        os.unlink(ftest_scaled)\n",
    "        os.unlink(fpredict)\n",
    "        \n",
    "        return poi_rank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_info_ = calc_poi_info(trajid_set_all, traj_all, poi_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_sanity_check = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_sanity_check == True:\n",
    "    train_df_ = gen_train_df(trajid_set_all, df_columns, traj_all, poi_info_, poi_distmat, query_id_dict)\n",
    "    ranksvm_ = RankSVM(ranksvm_dir, useLinear=True)\n",
    "    ranksvm_.train(train_df_, cost=1000)\n",
    "    rank_train_ = ranksvm_.predict(train_df_)\n",
    "    print(train_df_.shape, rank_train_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms on training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_sanity_check == True:\n",
    "    labels = sorted(train_df_['label'].unique())\n",
    "    labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_sanity_check == True:\n",
    "    rmin_ = rank_train_['rank'].min()\n",
    "    rmax_ = rank_train_['rank'].max()\n",
    "    print(rmin_, rmax_)\n",
    "    rmin1_ = np.round(rmin_); rmin1_ = rmin1_ if rmin1_ < rmin_ else rmin1_ - 1\n",
    "    rmax1_ = np.round(rmax_); rmax1_ = rmax1_ if rmax1_ > rmax_ else rmax1_ + 1\n",
    "    print(rmin1_, rmax1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_sanity_check == True:\n",
    "    pmin_ = rank_train_['probability'].min()\n",
    "    pmax_ = rank_train_['probability'].max()\n",
    "    print(pmin_, pmax_)\n",
    "    pmin1_ = np.round(pmin_); pmin1_ = pmin1_ if pmin1_ < pmin_ else max(0, pmin1_ - 0.001)\n",
    "    pmax1_ = np.round(pmax_); pmax1_ = pmax1_ if pmax1_ > pmax_ else min(1, pmax1_ + 0.001)\n",
    "    print(pmin1_, pmax1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_sanity_check == True:\n",
    "    plt.figure(figsize=[15, 4])\n",
    "    ax1 = plt.subplot(1, 3, 1); plt.title('Histogram of Lables (Train)')\n",
    "    train_df_['label'].hist(ax=ax1, bins=2*len(labels))\n",
    "    ax1.set_yscale('log')\n",
    "\n",
    "    ax2 = plt.subplot(1, 3, 2); plt.title('Histogram of Ranking Score (Train)')\n",
    "    #hist1, bins1 = np.histogram(rank_train_['rank'], bins=5)\n",
    "    bins1_train = np.r_[np.arange(rmin1_, rmax1_, 0.5), rmax1_]  # [-1.5, -1.0, -0.5, 0, 0.5, 1.0, 1.5, 2.0]\n",
    "    for l in labels:\n",
    "        ix = train_df_[train_df_['label'] == l].index\n",
    "        rank_train_.loc[ix, 'rank'].hist(ax=ax2, bins=bins1_train, label='Label=%d' % l, alpha=0.7)\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_ylim(ymin=0.1)\n",
    "    ax2.set_xlabel('Ranking Score')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    ax3 = plt.subplot(1, 3, 3); plt.title('Histogram of Ranking Probability (Train)')\n",
    "    #hist2, bins2 = np.histogram(rank_train_['probability'], bins=5)\n",
    "    bins2_train = np.r_[np.arange(pmin1_, pmax1_, 0.0001), pmax1_]\n",
    "    for l in labels:\n",
    "        ix = train_df_[train_df_['label'] == l].index\n",
    "        rank_train_.loc[ix, 'probability'].hist(ax=ax3, bins=bins2_train, label='Label=%d' % l, alpha=0.7)\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.set_ylim(ymin=0.1)\n",
    "    ax3.set_xlabel('Ranking Probability')\n",
    "    plt.legend(loc='upper right')\n",
    "    print('NOTE: labels of POI in training set is query specific')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2.5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Recommendation based on POI Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend trajectories based on ranking of POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_rank == True:\n",
    "    recdict_rank = dict()\n",
    "    cost = 1000\n",
    "    cnt = 1\n",
    "    for i in range(len(trajid_set_all)):\n",
    "        tid = trajid_set_all[i]\n",
    "        t = extract_traj(tid, traj_all)\n",
    "        \n",
    "        # trajectory too short\n",
    "        if len(t) < 3: continue\n",
    "            \n",
    "        # TSP, generally an ILP very hard for solvers (GUROBI/CBC)\n",
    "        if t[0] == t[-1]: continue\n",
    "\n",
    "        if uspecific == True:\n",
    "            user = traj_user.loc[tid, 'userID']\n",
    "            trajid_set_user = traj_user[traj_user['userID'] == user].index.tolist()\n",
    "            trajid_set_other = traj_user[traj_user['userID'] != user].index.tolist()\n",
    "            assert(tid in trajid_set_user)\n",
    "            trajid_set_add = [x for x in trajid_set_user if x != tid]\n",
    "            trajid_list_train = trajid_set_other.copy()\n",
    "            assert(KX > 0)\n",
    "            for k in range(KX):\n",
    "                trajid_list_train = trajid_list_train + trajid_set_add\n",
    "        else:\n",
    "            trajid_list_train = trajid_set_all[:i] + trajid_set_all[i+1:]\n",
    "        \n",
    "        poi_info = calc_poi_info(trajid_list_train, traj_all, poi_all)\n",
    "        \n",
    "        # trajectory too long\n",
    "        if len(t) > poi_info.shape[0]: continue\n",
    "            \n",
    "        # start/end is not in training set\n",
    "        if not (t[0] in poi_info.index and t[-1] in poi_info.index): continue\n",
    "        \n",
    "        print(t, '#%d ->' % cnt); cnt += 1; sys.stdout.flush()\n",
    "\n",
    "        # POI popularity based ranking\n",
    "        poi_info.sort_values(by='popularity', ascending=False, inplace=True)\n",
    "        ranks1 = poi_info.index.tolist()\n",
    "        rec_pop = [t[0]] + [x for x in ranks1 if x not in {t[0], t[-1]}][:len(t)-2] + [t[-1]]\n",
    "\n",
    "        # POI feature based ranking\n",
    "        train_df = gen_train_df(trajid_list_train, df_columns, traj_all, poi_info, poi_distmat, query_id_dict)\n",
    "        ranksvm = RankSVM(ranksvm_dir, useLinear=True)\n",
    "        ranksvm.train(train_df, cost=cost)\n",
    "        test_df = gen_test_df(t[0], t[-1], len(t), df_columns, poi_info, poi_distmat, query_id_dict)\n",
    "        rank_df = ranksvm.predict(test_df)\n",
    "        rank_df.sort_values(by='rank', ascending=False, inplace=True)\n",
    "        ranks2 = rank_df['poiID'].tolist()\n",
    "        rec_feature = [t[0]] + [x for x in ranks2 if x not in {t[0], t[-1]}][:len(t)-2] + [t[-1]]\n",
    "\n",
    "        recdict_rank[tid] = {'REAL':t, 'REC_POP':rec_pop, 'REC_FEATURE':rec_feature}\n",
    "\n",
    "        print(' '*10, rec_pop)\n",
    "        print(' '*10, rec_feature); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if run_rank == True:\n",
    "    pickle.dump(recdict_rank, open(frecdict_rank, 'bw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_rank == True:\n",
    "    F11_rank = []\n",
    "    F12_rank = []\n",
    "    for key in sorted(recdict_rank.keys()):\n",
    "        F11_rank.append(calc_F1(recdict_rank[key]['REAL'], recdict_rank[key]['REC_POP']))\n",
    "        F12_rank.append(calc_F1(recdict_rank[key]['REAL'], recdict_rank[key]['REC_FEATURE']))\n",
    "    print('POP    : %.3f, %.3f' % (np.mean(F11_rank), np.std(F11_rank)))\n",
    "    print('FEATURE: %.3f, %.3f' % (np.mean(F12_rank), np.std(F12_rank)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Factorise Transition Probabilities in POI Feature Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate a transition matrix for each feature of POI, transition probabilities (matrix) between different POI features (vector) is obtrained by the [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product) of the individual transition matrix corresponding to each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 POI Features for Factorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI features used to factorise transition matrix of Markov Chain with POI features (vector) as states:\n",
    "- Category of POI\n",
    "- Popularity of POI (discritize with uniform log-scale bins, #bins <=5 )\n",
    "- The number of POI visits (discritize with uniform log-scale bins, #bins <=5 )\n",
    "- The average visit duration of POI (discritise with uniform log-scale bins, #bins <= 5)\n",
    "- The neighborhood relationship between POIs (clustering POI(lat, lon) using k-means, #clusters <= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the number of transition first, then normalise each row while taking care of zero by adding each cell a number $k=1$. ~~a small number (i.e. $0.2$ times the minimum value of that row) if there exists a zero cell.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalise_transmat(transmat_cnt):\n",
    "    transmat = transmat_cnt.copy()\n",
    "    assert(isinstance(transmat, pd.DataFrame))\n",
    "    for row in range(transmat.index.shape[0]):\n",
    "        #nonzeroidx = np.nonzero(transmat.iloc[row])[0].tolist()    \n",
    "        #if len(nonzeroidx) < transmat.columns.shape[0]:\n",
    "        #    if len(nonzeroidx) == 0:  # all zeros in row\n",
    "        #        transmat.iloc[row] = 1 / transmat.columns.shape[0]  # uniform distribution\n",
    "        #    else:\n",
    "        #        minv = np.min(transmat.iloc[row, nonzeroidx])\n",
    "        #        EPS = 0.2 * minv  # row-specific smooth factor\n",
    "        #        #zeroidx = list(set(range(len(transmat.columns))) - set(nonzeroidx))\n",
    "        #        #transmat.iloc[row, zeroidx] = EPS\n",
    "        #        transmat.iloc[row] += EPS\n",
    "        #        rowsum = np.sum(transmat.iloc[row])\n",
    "        #        assert(rowsum > 0)\n",
    "        #        transmat.iloc[row] /= rowsum\n",
    "        #else:\n",
    "        #    assert(len(nonzeroidx) == transmat.columns.shape[0])  # all non-zero in row\n",
    "        #    transmat.iloc[row] /= np.sum(transmat.iloc[row])\n",
    "        rowsum = np.sum(transmat.iloc[row] + 1)\n",
    "        assert(rowsum > 0)\n",
    "        transmat.iloc[row] = (transmat.iloc[row] + 1) / rowsum\n",
    "    return transmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POIs in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_train_ = traj_all[traj_all['trajID'].isin(trajid_set_all)]['poiID'].unique().tolist()\n",
    "poi_train_.sort()\n",
    "#poi_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Transition Matrix between POI Cateogries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_cats = poi_all.loc[poi_train_, 'poiCat'].unique().tolist()\n",
    "poi_cats.sort()\n",
    "poi_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transmat_cat_cnt = pd.DataFrame(data=np.zeros((len(poi_cats), len(poi_cats)), dtype=np.float), \\\n",
    "                                columns=poi_cats, index=poi_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of transitions between POI categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tid in trajid_set_all:\n",
    "    t = extract_traj(tid, traj_all)\n",
    "    if len(t) > 1:\n",
    "        for pi in range(len(t)-1):\n",
    "            p1 = t[pi]\n",
    "            p2 = t[pi+1]\n",
    "            cat1 = poi_all.loc[p1, 'poiCat']\n",
    "            cat2 = poi_all.loc[p2, 'poiCat']\n",
    "            transmat_cat_cnt.loc[cat1, cat2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_cat_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise each row to obtain transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_cat = normalise_transmat(transmat_cat_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the above steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_transmat_cat(trajid_list, traj_all, poi_info, poi_cats=poi_cats):    \n",
    "    transmat_cat_cnt = pd.DataFrame(data=np.zeros((len(poi_cats), len(poi_cats)), dtype=np.float), \\\n",
    "                                    columns=poi_cats, index=poi_cats)\n",
    "    for tid in trajid_list:\n",
    "        t = extract_traj(tid, traj_all)\n",
    "        if len(t) > 1:\n",
    "            for pi in range(len(t)-1):\n",
    "                p1 = t[pi]\n",
    "                p2 = t[pi+1]\n",
    "                cat1 = poi_info.loc[p1, 'poiCat']\n",
    "                cat2 = poi_info.loc[p2, 'poiCat']\n",
    "                transmat_cat_cnt.loc[cat1, cat2] += 1\n",
    "    return normalise_transmat(transmat_cat_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Transition Matrix between POI Popularity Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_pops = poi_info_.loc[poi_train_, 'popularity']\n",
    "#sorted(poi_pops.unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretize POI popularity with uniform log-scale bins (#bins $\\le 5$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expo_pop1 = np.log10(max(1, min(poi_pops)))\n",
    "expo_pop2 = np.log10(max(poi_pops))\n",
    "print(expo_pop1, expo_pop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbins_pop = BIN_CLUSTER\n",
    "logbins_pop = np.logspace(np.floor(expo_pop1), np.ceil(expo_pop2), nbins_pop+1)\n",
    "logbins_pop[0] = 0  # deal with underflow\n",
    "if uspecific == True:\n",
    "    logbins_pop[-1] = KX * logbins_pop[-1]  # deal with overflow\n",
    "logbins_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = pd.Series(poi_pops).hist(figsize=(5, 3), bins=logbins_pop)\n",
    "ax.set_xlim(xmin=0.1)\n",
    "ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_pop_cnt = pd.DataFrame(data=np.zeros((nbins_pop, nbins_pop), dtype=np.float), \\\n",
    "                                columns=np.arange(1, nbins_pop+1), index=np.arange(1, nbins_pop+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of transitions between POI popularity classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tid in trajid_set_all:\n",
    "    t = extract_traj(tid, traj_all)\n",
    "    if len(t) > 1:\n",
    "        for pi in range(len(t)-1):\n",
    "            p1 = t[pi]\n",
    "            p2 = t[pi+1]\n",
    "            pop1 = poi_info_.loc[p1, 'popularity']\n",
    "            pop2 = poi_info_.loc[p2, 'popularity']\n",
    "            pc1, pc2 = np.digitize([pop1, pop2], logbins_pop)\n",
    "            transmat_pop_cnt.loc[pc1, pc2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_pop_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise each row to obtain transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_pop = normalise_transmat(transmat_pop_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_pop_cnt + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(transmat_pop_cnt.iloc[3] + 1) / np.sum(transmat_pop_cnt.iloc[3] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the above steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_transmat_pop(trajid_list, traj_all, poi_info, logbins_pop=logbins_pop):\n",
    "    nbins = len(logbins_pop) - 1\n",
    "    transmat_pop_cnt = pd.DataFrame(data=np.zeros((nbins, nbins), dtype=np.float), \\\n",
    "                                    columns=np.arange(1, nbins+1), index=np.arange(1, nbins+1))\n",
    "    for tid in trajid_list:\n",
    "        t = extract_traj(tid, traj_all)\n",
    "        if len(t) > 1:\n",
    "            for pi in range(len(t)-1):\n",
    "                p1 = t[pi]\n",
    "                p2 = t[pi+1]\n",
    "                pop1 = poi_info.loc[p1, 'popularity']\n",
    "                pop2 = poi_info.loc[p2, 'popularity']\n",
    "                pc1, pc2 = np.digitize([pop1, pop2], logbins_pop)\n",
    "                transmat_pop_cnt.loc[pc1, pc2] += 1\n",
    "    return normalise_transmat(transmat_pop_cnt), logbins_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3.4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Transition Matrix between the Number of POI Visit Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_visits = poi_info_.loc[poi_train_, 'nVisit']\n",
    "#sorted(poi_visits.unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretize the number of POI visit with uniform log-scale bins (#bins $\\le 5$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expo_visit1 = np.log10(max(1, min(poi_visits)))\n",
    "expo_visit2 = np.log10(max(poi_visits))\n",
    "print(expo_visit1, expo_visit2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbins_visit = BIN_CLUSTER\n",
    "logbins_visit = np.logspace(np.floor(expo_visit1), np.ceil(expo_visit2), nbins_visit+1)\n",
    "logbins_visit[0] = 0  # deal with underflow\n",
    "if uspecific == True:\n",
    "    logbins_visit[-1] = KX * logbins_visit[-1]  # deal with overflow\n",
    "logbins_visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = pd.Series(poi_visits).hist(figsize=(5, 3), bins=logbins_visit)\n",
    "ax.set_xlim(xmin=0.1)\n",
    "ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_visit_cnt = pd.DataFrame(data=np.zeros((nbins_visit, nbins_visit), dtype=np.float), \\\n",
    "                                  columns=np.arange(1, nbins_visit+1), index=np.arange(1, nbins_visit+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of transitions between POI popularity classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tid in trajid_set_all:\n",
    "    t = extract_traj(tid, traj_all)\n",
    "    if len(t) > 1:\n",
    "        for pi in range(len(t)-1):\n",
    "            p1 = t[pi]\n",
    "            p2 = t[pi+1]\n",
    "            visit1 = poi_info_.loc[p1, 'nVisit']\n",
    "            visit2 = poi_info_.loc[p2, 'nVisit']\n",
    "            vc1, vc2 = np.digitize([visit1, visit2], logbins_visit)\n",
    "            transmat_visit_cnt.loc[vc1, vc2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_visit_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise each row to obtain transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_visit = normalise_transmat(transmat_visit_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_visit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the above steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_transmat_visit(trajid_list, traj_all, poi_info, logbins_visit=logbins_visit):\n",
    "    nbins = len(logbins_visit) - 1\n",
    "    transmat_visit_cnt = pd.DataFrame(data=np.zeros((nbins, nbins), dtype=np.float), \\\n",
    "                                      columns=np.arange(1, nbins+1), index=np.arange(1, nbins+1))\n",
    "    for tid in trajid_list:\n",
    "        t = extract_traj(tid, traj_all)\n",
    "        if len(t) > 1:\n",
    "            for pi in range(len(t)-1):\n",
    "                p1 = t[pi]\n",
    "                p2 = t[pi+1]\n",
    "                visit1 = poi_info.loc[p1, 'nVisit']\n",
    "                visit2 = poi_info.loc[p2, 'nVisit']\n",
    "                vc1, vc2 = np.digitize([visit1, visit2], logbins_visit)\n",
    "                transmat_visit_cnt.loc[vc1, vc2] += 1\n",
    "    return normalise_transmat(transmat_visit_cnt), logbins_visit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3.5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Transition Matrix between POI Average Visit Duration Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_durations = poi_info_.loc[poi_train_, 'avgDuration']\n",
    "#sorted(poi_durations.unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretize POI average visit duration with uniform log-scale bins (#bins $\\le 5$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expo_duration1 = np.log10(max(1, min(poi_durations)))\n",
    "expo_duration2 = np.log10(max(poi_durations))\n",
    "print(expo_duration1, expo_duration2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbins_duration = BIN_CLUSTER\n",
    "logbins_duration = np.logspace(np.floor(expo_duration1), np.ceil(expo_duration2), nbins_duration+1)\n",
    "logbins_duration[0] = 0  # deal with underflow\n",
    "if uspecific == True:\n",
    "    logbins_duration[-1] = KX * logbins_duration[-1]  # deal with overflow\n",
    "logbins_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = pd.Series(poi_durations).hist(figsize=(5, 3), bins=logbins_duration)\n",
    "ax.set_xlim(xmin=0.1)\n",
    "ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_duration_cnt = pd.DataFrame(data=np.zeros((nbins_duration, nbins_duration), dtype=np.float), \\\n",
    "                                     columns=np.arange(1, nbins_duration+1), index=np.arange(1, nbins_duration+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of transitions between POI average visit duration classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tid in trajid_set_all:\n",
    "    t = extract_traj(tid, traj_all)\n",
    "    if len(t) > 1:\n",
    "        for pi in range(len(t)-1):\n",
    "            p1 = t[pi]\n",
    "            p2 = t[pi+1]\n",
    "            d1 = poi_info_.loc[p1, 'avgDuration']\n",
    "            d2 = poi_info_.loc[p2, 'avgDuration']\n",
    "            dc1, dc2 = np.digitize([d1, d2], logbins_duration)\n",
    "            transmat_duration_cnt.loc[dc1, dc2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_duration_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise each row to obtain transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transmat_duration = normalise_transmat(transmat_duration_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the above steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_transmat_duration(trajid_list, traj_all, poi_info, logbins_duration=logbins_duration):\n",
    "    nbins = len(logbins_duration) - 1\n",
    "    transmat_duration_cnt = pd.DataFrame(data=np.zeros((nbins, nbins), dtype=np.float), \\\n",
    "                                         columns=np.arange(1, nbins+1), index=np.arange(1, nbins+1))\n",
    "    for tid in trajid_list:\n",
    "        t = extract_traj(tid, traj_all)\n",
    "        if len(t) > 1:\n",
    "            for pi in range(len(t)-1):\n",
    "                p1 = t[pi]\n",
    "                p2 = t[pi+1]\n",
    "                d1 = poi_info.loc[p1, 'avgDuration']\n",
    "                d2 = poi_info.loc[p2, 'avgDuration']\n",
    "                dc1, dc2 = np.digitize([d1, d2], logbins_duration)\n",
    "                transmat_duration_cnt.loc[dc1, dc2] += 1\n",
    "    return normalise_transmat(transmat_duration_cnt), logbins_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3.6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Transition Matrix between POI Neighborhood Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute POI neighborhoods, i.e., clustering POI (lat, lon) in training set using k-means, #clusters $\\le 5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans in scikit-learn seems unable to use custom distance metric and no implementation of [Haversine formula](http://en.wikipedia.org/wiki/Great-circle_distance), use Euclidean distance to approximate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.cluster import KMeans\n",
    "X = poi_all.loc[poi_train_, ['poiLon', 'poiLat']]\n",
    "nclusters = BIN_CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=nclusters)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = kmeans.predict(X)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_clusters = pd.DataFrame(data=clusters, index=poi_train_)\n",
    "poi_clusters.index.name = 'poiID'\n",
    "poi_clusters.rename(columns={0:'clusterID'}, inplace=True)\n",
    "#poi_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff = poi_all.loc[poi_train_, ['poiLon', 'poiLat']].max() - poi_all.loc[poi_train_, ['poiLon', 'poiLat']].min()\n",
    "ratio = diff['poiLon'] / diff['poiLat']\n",
    "ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plot of POI coordinates with clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "height = 4; width = int(round(ratio)*height)\n",
    "plt.figure(figsize=[width, height])\n",
    "plt.scatter(poi_all.loc[poi_train_, 'poiLon'], poi_all.loc[poi_train_, 'poiLat'], c=clusters, s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_neighbor_cnt = pd.DataFrame(data=np.zeros((nclusters, nclusters), dtype=np.float), \\\n",
    "                                     columns=np.arange(nclusters), index=np.arange(nclusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of transitions between POIs in different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tid in trajid_set_all:\n",
    "    t = extract_traj(tid, traj_all)\n",
    "    if len(t) > 1:\n",
    "        for pi in range(len(t)-1):\n",
    "            p1 = t[pi]\n",
    "            p2 = t[pi+1]\n",
    "            c1 = poi_clusters.loc[p1, 'clusterID']\n",
    "            c2 = poi_clusters.loc[p2, 'clusterID']\n",
    "            transmat_neighbor_cnt.loc[c1, c2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_neighbor_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise each row to obtain transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transmat_neighbor = normalise_transmat(transmat_neighbor_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the above steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_transmat_neighbor(trajid_list, traj_all, poi_info, poi_clusters=poi_clusters):\n",
    "    nclusters = len(poi_clusters['clusterID'].unique())\n",
    "    transmat_neighbor_cnt = pd.DataFrame(data=np.zeros((nclusters, nclusters), dtype=np.float), \\\n",
    "                                         columns=np.arange(nclusters), index=np.arange(nclusters))\n",
    "    for tid in trajid_list:\n",
    "        t = extract_traj(tid, traj_all)\n",
    "        if len(t) > 1:\n",
    "            for pi in range(len(t)-1):\n",
    "                p1 = t[pi]\n",
    "                p2 = t[pi+1]\n",
    "                c1 = poi_clusters.loc[p1, 'clusterID']\n",
    "                c2 = poi_clusters.loc[p2, 'clusterID']\n",
    "                transmat_neighbor_cnt.loc[c1, c2] += 1\n",
    "    return normalise_transmat(transmat_neighbor_cnt), poi_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3.7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Transition Matrix between POIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate transition probabilities (matrix) between different POI features (vector) using the [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product) of individual transition matrix corresponding to each feature, i.e., POI category, POI popularity (discritized), POI average visit duration (discritized) and POI neighborhoods (clusters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from scipy.linalg import kron\n",
    "transmat_value = transmat_cat.values\n",
    "for transmat in [transmat_pop, transmat_visit, transmat_duration, transmat_neighbor]:\n",
    "    transmat_value = kron(transmat_value, transmat.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index of Kronecker product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transmat_ix = list(itertools.product(transmat_cat.index, transmat_pop.index, transmat_visit.index, \\\n",
    "                                     transmat_duration.index, transmat_neighbor.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_feature = pd.DataFrame(data=transmat_value, index=transmat_ix, columns=transmat_ix)\n",
    "transmat_feature.index.name = '(poiTheme, popularity, avgDuration, clusterID)'\n",
    "#transmat_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(transmat_cat.shape)\n",
    "print(transmat_pop.shape)\n",
    "print(transmat_visit.shape)\n",
    "print(transmat_duration.shape)\n",
    "print(transmat_neighbor.shape)\n",
    "print(transmat_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transmat_feature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product) should be normalised to obtain transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from scipy.misc import logsumexp\n",
    "#logtransmat_feature = np.log(transmat_feature)\n",
    "#for row in range(logtransmat_feature.shape[0]):\n",
    "#    logtransmat_feature.iloc[row] -= logsumexp(logtransmat_feature.iloc[row])\n",
    "#logtransmat_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#logtransmat_feature.max().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=[13, 10])\n",
    "#sns.heatmap(np.exp(logtransmat_feature), xticklabels=False, yticklabels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with features without corresponding POIs and feature with more than one corresponding POIs. (*Before Normalisation*)\n",
    "- For features without corresponding POIs, just remove the rows and columns from the matrix obtained by Kronecker product.\n",
    "- For different POIs with the exact same feature, \n",
    "  - Let POIs with the same feature as a POI group,\n",
    "  - The *incoming* **transition value (i.e., unnormalised transition probability)** of this POI group \n",
    "    should be divided uniformly among the group members, \n",
    "    *which corresponds to choose a group member uniformly at random in the incoming case*.\n",
    "  - The *outgoing* transition value should be duplicated (i.e., the same) among all group members, \n",
    "    **as we were already in that group in the outgoing case**.\n",
    "  - For each POI in the group, the allocation transition value of the *self-loop of the POI group* is similar to \n",
    "    that in the *outgoing* case, **as we were already in that group**, so just duplicate and then divide uniformly among \n",
    "    the transitions from this POI to other POIs in the same group, \n",
    "    *which corresponds to choose a outgoing transition uniformly at random from all outgoing transitions\n",
    "    excluding the self-loop of this POI*.\n",
    "- **Concretely**, for a POI group with $n$ POIs, \n",
    "    1. If the *incoming* transition value of POI group is $m_1$,\n",
    "       then the corresponding *incoming* transition value for each group member is $\\frac{m_1}{n}$.\n",
    "    1. If the *outgoing* transition value of POI group is $m_2$,\n",
    "       then the corresponding *outgoing* transition value for each group member is also $m_2$.\n",
    "    1. If the transition value of *self-loop of the POI group* is $m_3$,\n",
    "       then transition value of *self-loop of individual POIs* should be $0$,  \n",
    "       and *other in-group transitions* with value $\\frac{m_3}{n-1}$\n",
    "       as the total number of outgoing transitions to other POIs in the same group is $n-1$ (excluding the self-loop),\n",
    "       i.e. $n-1$ choose $1$.\n",
    "       \n",
    "**NOTE**: execute the above division before or after row normalisation will lead to the same result, *as the division itself does NOT change the normalising constant of each row (i.e., the sum of each row before normalising)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = ['poiCat', 'popularity', 'nVisit', 'avgDuration', 'clusterID']\n",
    "poi_features = pd.DataFrame(data=np.zeros((len(poi_train_), len(feature_names))), \\\n",
    "                            columns=feature_names, index=poi_train_)\n",
    "poi_features.index.name = 'poiID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_features['poiCat'] = poi_info_.loc[poi_train_, 'poiCat']\n",
    "poi_features['popularity'] = np.digitize(poi_info_.loc[poi_train_, 'popularity'], logbins_pop)\n",
    "poi_features['nVisit'] = np.digitize(poi_info_.loc[poi_train_, 'nVisit'], logbins_visit)\n",
    "poi_features['avgDuration'] = np.digitize(poi_info_.loc[poi_train_, 'avgDuration'], logbins_duration)\n",
    "poi_features['clusterID'] = poi_clusters.loc[poi_train_, 'clusterID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ix = transmat_feature.index[0]; ix1 = transmat_feature.index[1]\n",
    "# simply use df.loc[tuple,:] will trigger an error as pandas treat the tuple as multiindex\n",
    "# see https://github.com/pydata/pandas/issues/7548\n",
    "#transmat_feature.loc[(ix,),]   # OK\n",
    "#transmat_feature.loc[(ix,),:]  # OK\n",
    "transmat_feature.loc[(ix,), (ix1,)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_logtransmat = pd.DataFrame(data=np.zeros((len(poi_train_), len(poi_train_)), dtype=np.float), \\\n",
    "                               columns=poi_train_, index=poi_train_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy transition values between features with corresponding POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for p1 in poi_logtransmat.index:\n",
    "    rix = tuple(poi_features.loc[p1])\n",
    "    for p2 in poi_logtransmat.columns:\n",
    "        cix = tuple(poi_features.loc[p2])\n",
    "        value_ = transmat_feature.loc[(rix,), (cix,)]\n",
    "        #print(value_.values)\n",
    "        poi_logtransmat.loc[p1, p2] = value_.values[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_logtransmat.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with POI features that correspond to more than once POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_dup = dict()\n",
    "for poi in poi_features.index:\n",
    "    key = tuple(poi_features.loc[poi])\n",
    "    if key in features_dup:\n",
    "        features_dup[key].append(poi)\n",
    "    else:\n",
    "        features_dup[key] = [poi]\n",
    "features_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feature in sorted(features_dup.keys()):\n",
    "    n = len(features_dup[feature])\n",
    "    if n > 1:\n",
    "        group = features_dup[feature]\n",
    "        v1 = poi_logtransmat.loc[group[0], group[0]]  # transition value of self-loop of POI group\n",
    "        \n",
    "        # divide incoming transition value (i.e. unnormalised transition probability) uniformly among group members\n",
    "        for poi in group:\n",
    "            poi_logtransmat[poi] /= n\n",
    "        \n",
    "        # outgoing transition value has already been duplicated (value copied above)\n",
    "        \n",
    "        # duplicate & divide transition value of self-loop of POI group uniformly among all outgoing transitions,\n",
    "        # from a POI to all other POIs in the same group (excluding POI self-loop)\n",
    "        v2 = v1 / (n - 1)\n",
    "        for pair in itertools.permutations(group, 2):\n",
    "            poi_logtransmat.loc[pair[0], pair[1]] = v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the transition value of all self-loops of POIs to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for p1 in poi_logtransmat.index:\n",
    "    poi_logtransmat.loc[p1, p1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_logtransmat.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise and compute log probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for p1 in poi_logtransmat.index:\n",
    "    rowsum = poi_logtransmat.loc[p1].sum()\n",
    "    assert(rowsum > 0)\n",
    "    logrowsum = np.log10(rowsum)\n",
    "    for p2 in poi_logtransmat.columns:\n",
    "        if p1 == p2:\n",
    "            poi_logtransmat.loc[p1, p2] = -np.inf  # deal with log(0) explicitly\n",
    "        else:\n",
    "            poi_logtransmat.loc[p1, p2] = np.log10(poi_logtransmat.loc[p1, p2]) - logrowsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_logtransmat.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot transition matrix heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob_mat = np.power(10, poi_logtransmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[13, 10])\n",
    "#plt.imshow(prob_mat, interpolation='none', cmap=plt.cm.hot)  # OK\n",
    "#ticks = prob_mat.index\n",
    "#plt.xticks(np.arange(prob_mat.shape[0]), ticks)\n",
    "#plt.yticks(np.arange(prob_mat.shape[0]), ticks)\n",
    "#plt.xlabel('POI ID')\n",
    "#plt.ylabel('POI ID')\n",
    "sns.heatmap(prob_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(prob_mat.loc[2, 1])\n",
    "#print(poi_all.loc[[2, 17]])\n",
    "#d_ = calc_dist(poi_all.loc[2, 'poiLon'], poi_all.loc[2, 'poiLat'], poi_all.loc[1, 'poiLon'], poi_all.loc[1, 'poiLat'])\n",
    "#print('Distance between POI %d and %d is %.3f km.' % (2, 1, d_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the above steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_poi_logtransmat(trajid_list, traj_all, poi_info):\n",
    "    transmat_cat                        = gen_transmat_cat(trajid_list, traj_all, poi_info)\n",
    "    transmat_pop,      logbins_pop      = gen_transmat_pop(trajid_list, traj_all, poi_info)\n",
    "    transmat_visit,    logbins_visit    = gen_transmat_visit(trajid_list, traj_all, poi_info)\n",
    "    transmat_duration, logbins_duration = gen_transmat_duration(trajid_list, traj_all, poi_info)\n",
    "    transmat_neighbor, poi_clusters     = gen_transmat_neighbor(trajid_list, traj_all, poi_info)\n",
    "\n",
    "    # Kronecker product\n",
    "    transmat_ix = list(itertools.product(transmat_cat.index, transmat_pop.index, transmat_visit.index, \\\n",
    "                                         transmat_duration.index, transmat_neighbor.index))\n",
    "    transmat_value = transmat_cat.values\n",
    "    for transmat in [transmat_pop, transmat_visit, transmat_duration, transmat_neighbor]:\n",
    "        transmat_value = kron(transmat_value, transmat.values)\n",
    "    transmat_feature = pd.DataFrame(data=transmat_value, index=transmat_ix, columns=transmat_ix)\n",
    "    \n",
    "    poi_train = poi_info.index.tolist()\n",
    "    feature_names = ['poiCat', 'popularity', 'nVisit', 'avgDuration', 'clusterID']\n",
    "    poi_features = pd.DataFrame(data=np.zeros((len(poi_train), len(feature_names))), \\\n",
    "                                columns=feature_names, index=poi_train)\n",
    "    poi_features.index.name = 'poiID'\n",
    "    poi_features['poiCat'] = poi_info.loc[poi_train, 'poiCat']\n",
    "    poi_features['popularity'] = np.digitize(poi_info.loc[poi_train, 'popularity'], logbins_pop)\n",
    "    poi_features['nVisit'] = np.digitize(poi_info.loc[poi_train, 'nVisit'], logbins_visit)\n",
    "    poi_features['avgDuration'] = np.digitize(poi_info.loc[poi_train, 'avgDuration'], logbins_duration)\n",
    "    poi_features['clusterID'] = poi_clusters.loc[poi_train, 'clusterID']\n",
    "    \n",
    "    # shrink the result of Kronecker product and deal with POIs with the same features\n",
    "    poi_logtransmat = pd.DataFrame(data=np.zeros((len(poi_train), len(poi_train)), dtype=np.float), \\\n",
    "                                   columns=poi_train, index=poi_train)\n",
    "    for p1 in poi_logtransmat.index:\n",
    "        rix = tuple(poi_features.loc[p1])\n",
    "        for p2 in poi_logtransmat.columns:\n",
    "            cix = tuple(poi_features.loc[p2])\n",
    "            value_ = transmat_feature.loc[(rix,), (cix,)]\n",
    "            poi_logtransmat.loc[p1, p2] = value_.values[0, 0]\n",
    "    \n",
    "    # group POIs with the same features\n",
    "    features_dup = dict()\n",
    "    for poi in poi_features.index:\n",
    "        key = tuple(poi_features.loc[poi])\n",
    "        if key in features_dup:\n",
    "            features_dup[key].append(poi)\n",
    "        else:\n",
    "            features_dup[key] = [poi]\n",
    "            \n",
    "    # deal with POIs with the same features\n",
    "    for feature in sorted(features_dup.keys()):\n",
    "        n = len(features_dup[feature])\n",
    "        if n > 1:\n",
    "            group = features_dup[feature]\n",
    "            v1 = poi_logtransmat.loc[group[0], group[0]]  # transition value of self-loop of POI group\n",
    "            \n",
    "            # divide incoming transition value (i.e. unnormalised transition probability) uniformly among group members\n",
    "            for poi in group:\n",
    "                poi_logtransmat[poi] /= n\n",
    "                \n",
    "            # outgoing transition value has already been duplicated (value copied above)\n",
    "            \n",
    "            # duplicate & divide transition value of self-loop of POI group uniformly among all outgoing transitions,\n",
    "            # from a POI to all other POIs in the same group (excluding POI self-loop)\n",
    "            v2 = v1 / (n - 1)\n",
    "            for pair in itertools.permutations(group, 2):\n",
    "                poi_logtransmat.loc[pair[0], pair[1]] = v2\n",
    "                            \n",
    "    # normalise each row\n",
    "    for p1 in poi_logtransmat.index:\n",
    "        poi_logtransmat.loc[p1, p1] = 0\n",
    "        rowsum = poi_logtransmat.loc[p1].sum()\n",
    "        assert(rowsum > 0)\n",
    "        logrowsum = np.log10(rowsum)\n",
    "        for p2 in poi_logtransmat.columns:\n",
    "            if p1 == p2:\n",
    "                poi_logtransmat.loc[p1, p2] = -np.inf  # deal with log(0) explicitly\n",
    "            else:\n",
    "                poi_logtransmat.loc[p1, p2] = np.log10(poi_logtransmat.loc[p1, p2]) - logrowsum\n",
    "    return poi_logtransmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3.8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Recommendation based on POI Transition Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dynamic programming to find a possibly non-simple path, i.e., walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_path(V, E, ps, pe, L, withNodeWeight=False, alpha=0.5):\n",
    "    assert(isinstance(V, pd.DataFrame))\n",
    "    assert(isinstance(E, pd.DataFrame))\n",
    "    assert(ps in V.index)\n",
    "    assert(pe in V.index)\n",
    "    # with sub-tours in trajectory, this is not the case any more, but it is nonsense to recommend such trajectories\n",
    "    assert(2 < L <= V.index.shape[0])  \n",
    "    if withNodeWeight == True:\n",
    "        assert(0 < alpha < 1)\n",
    "    beta = 1 - alpha\n",
    "    \n",
    "    A = pd.DataFrame(data=np.zeros((L-1, V.shape[0]), dtype=np.float), columns=V.index, index=np.arange(2, L+1))\n",
    "    B = pd.DataFrame(data=np.zeros((L-1, V.shape[0]), dtype=np.int),   columns=V.index, index=np.arange(2, L+1))\n",
    "    A += np.inf\n",
    "    for v in V.index:\n",
    "        if v != ps:\n",
    "            if withNodeWeight == True:\n",
    "                A.loc[2, v] = alpha * (V.loc[ps, 'weight'] + V.loc[v, 'weight']) + beta * E.loc[ps, v]  # ps--v\n",
    "            else:\n",
    "                A.loc[2, v] = E.loc[ps, v]  # ps--v\n",
    "            B.loc[2, v] = ps\n",
    "    \n",
    "    for l in range(3, L+1):\n",
    "        for v in V.index:\n",
    "            if withNodeWeight == True: # ps-~-v1---v\n",
    "                values = [A.loc[l-1, v1] + alpha * V.loc[v, 'weight'] + beta * E.loc[v1, v] for v1 in V.index]\n",
    "            else:\n",
    "                values = [A.loc[l-1, v1] + E.loc[v1, v] for v1 in V.index]  # ps-~-v1---v   \n",
    "            \n",
    "            minix = np.argmin(values)\n",
    "            A.loc[l, v] = values[minix]\n",
    "            B.loc[l, v] = V.index[minix]\n",
    "            \n",
    "    path = [pe]\n",
    "    v = path[-1]\n",
    "    l = L\n",
    "    #while v != ps:  #incorrect if 'ps' happens to appear in the middle of a path\n",
    "    while l >= 2:\n",
    "        path.append(B.loc[l, v])\n",
    "        v = path[-1]\n",
    "        l -= 1\n",
    "    path.reverse()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use integer linear programming (ILP) to find a simple path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_path_ILP(V, E, ps, pe, L, withNodeWeight=False, alpha=0.5):\n",
    "    assert(isinstance(V, pd.DataFrame))\n",
    "    assert(isinstance(E, pd.DataFrame))\n",
    "    assert(ps in V.index)\n",
    "    assert(pe in V.index)\n",
    "    assert(2 < L <= V.index.shape[0])\n",
    "    if withNodeWeight == True:\n",
    "        assert(0 < alpha < 1)\n",
    "    beta = 1 - alpha\n",
    "    \n",
    "    p0 = str(ps); pN = str(pe); N = V.index.shape[0]\n",
    "    \n",
    "    # deal with np.inf which will cause ILP solver failure\n",
    "    Edges = E.copy()\n",
    "    INF = 1e6\n",
    "    for p in Edges.index:\n",
    "        Edges.loc[p, p] = INF \n",
    "    maxL = np.max(Edges.values.flatten())\n",
    "    if maxL > INF:\n",
    "        for p in Edges.index:\n",
    "            Edges.loc[p, p] = maxL    \n",
    "    \n",
    "    # REF: pythonhosted.org/PuLP/index.html\n",
    "    pois = [str(p) for p in V.index] # create a string list for each POI\n",
    "    pb = pulp.LpProblem('MostLikelyTraj', pulp.LpMinimize) # create problem\n",
    "    # visit_i_j = 1 means POI i and j are visited in sequence\n",
    "    visit_vars = pulp.LpVariable.dicts('visit', (pois, pois), 0, 1, pulp.LpInteger) \n",
    "    # a dictionary contains all dummy variables\n",
    "    dummy_vars = pulp.LpVariable.dicts('u', [x for x in pois if x != p0], 2, N, pulp.LpInteger)\n",
    "    \n",
    "    # add objective\n",
    "    objlist = []\n",
    "    if withNodeWeight == True:\n",
    "        objlist.append(alpha * V.loc[int(p0), 'weight'])\n",
    "    for pi in [x for x in pois if x != pN]:     # from\n",
    "        for pj in [y for y in pois if y != p0]: # to\n",
    "            if withNodeWeight == True:\n",
    "                objlist.append(visit_vars[pi][pj] * (alpha*V.loc[int(pj), 'weight'] + beta*Edges.loc[int(pi), int(pj)]))\n",
    "            else:\n",
    "                objlist.append(visit_vars[pi][pj] * Edges.loc[int(pi), int(pj)])\n",
    "    pb += pulp.lpSum(objlist), 'Objective'\n",
    "    \n",
    "    # add constraints, each constraint should be in ONE line\n",
    "    pb += pulp.lpSum([visit_vars[p0][pj] for pj in pois if pj != p0]) == 1, 'StartAt_p0'\n",
    "    pb += pulp.lpSum([visit_vars[pi][pN] for pi in pois if pi != pN]) == 1, 'EndAt_pN'\n",
    "    if p0 != pN:\n",
    "        pb += pulp.lpSum([visit_vars[pi][p0] for pi in pois]) == 0, 'NoIncoming_p0'\n",
    "        pb += pulp.lpSum([visit_vars[pN][pj] for pj in pois]) == 0, 'NoOutgoing_pN'\n",
    "    pb += pulp.lpSum([visit_vars[pi][pj] for pi in pois if pi != pN for pj in pois if pj != p0]) == L-1, 'Length'\n",
    "    for pk in [x for x in pois if x not in {p0, pN}]:\n",
    "        pb += pulp.lpSum([visit_vars[pi][pk] for pi in pois if pi != pN]) == \\\n",
    "              pulp.lpSum([visit_vars[pk][pj] for pj in pois if pj != p0]), 'ConnectedAt_' + pk\n",
    "        pb += pulp.lpSum([visit_vars[pi][pk] for pi in pois if pi != pN]) <= 1, 'Enter_' + pk + '_AtMostOnce'\n",
    "        pb += pulp.lpSum([visit_vars[pk][pj] for pj in pois if pj != p0]) <= 1, 'Leave_' + pk + '_AtMostOnce'\n",
    "    for pi in [x for x in pois if x != p0]:\n",
    "        for pj in [y for y in pois if y != p0]:\n",
    "            pb += dummy_vars[pi] - dummy_vars[pj] + 1 <= (N - 1) * (1 - visit_vars[pi][pj]), \\\n",
    "                    'SubTourElimination_' + pi + '_' + pj\n",
    "    #pb.writeLP(\"traj_tmp.lp\")\n",
    "    # solve problem\n",
    "    pb.solve(pulp.PULP_CBC_CMD(options=['-threads', '6', '-strategy', '1', '-maxIt', '2000000'])) # CBC\n",
    "    #gurobi_options = [('TimeLimit', '7200'), ('Threads', '8'), ('NodefileStart', '0.9'), ('Cuts', '2')]\n",
    "    #pb.solve(pulp.GUROBI_CMD(options=gurobi_options)) # GUROBI\n",
    "    visit_mat = pd.DataFrame(data=np.zeros((len(pois), len(pois)), dtype=np.float), index=pois, columns=pois)\n",
    "    for pi in pois:\n",
    "        for pj in pois: visit_mat.loc[pi, pj] = visit_vars[pi][pj].varValue\n",
    "\n",
    "    # build the recommended trajectory\n",
    "    recseq = [p0]\n",
    "    while True:\n",
    "        pi = recseq[-1]\n",
    "        pj = visit_mat.loc[pi].idxmax()\n",
    "        assert(round(visit_mat.loc[pi, pj]) == 1)\n",
    "        recseq.append(pj); \n",
    "        #print(recseq); sys.stdout.flush()\n",
    "        if pj == pN: return [int(x) for x in recseq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_tran == True:\n",
    "    recdict_tran = dict()\n",
    "    cnt = 1\n",
    "    for i in range(len(trajid_set_all)):\n",
    "        tid = trajid_set_all[i]\n",
    "        t = extract_traj(tid, traj_all)\n",
    "        \n",
    "        # trajectory too short\n",
    "        if len(t) < 3: continue\n",
    "            \n",
    "        # TSP, generally an ILP very hard for solvers (GUROBI/CBC)\n",
    "        if t[0] == t[-1]: continue\n",
    "\n",
    "        if uspecific == True:\n",
    "            user = traj_user.loc[tid, 'userID']\n",
    "            trajid_set_user = traj_user[traj_user['userID'] == user].index.tolist()\n",
    "            trajid_set_other = traj_user[traj_user['userID'] != user].index.tolist()\n",
    "            assert(tid in trajid_set_user)\n",
    "            trajid_set_add = [x for x in trajid_set_user if x != tid]\n",
    "            trajid_list_train = trajid_set_other.copy()\n",
    "            assert(KX > 0)\n",
    "            for k in range(KX):\n",
    "                trajid_list_train = trajid_list_train + trajid_set_add\n",
    "        else:\n",
    "            trajid_list_train = trajid_set_all[:i] + trajid_set_all[i+1:]\n",
    "            \n",
    "        poi_info = calc_poi_info(trajid_list_train, traj_all, poi_all)\n",
    "        \n",
    "        # trajectory too long\n",
    "        if len(t) > poi_info.shape[0]: continue\n",
    "            \n",
    "        # start/end is not in training set\n",
    "        if not (t[0] in poi_info.index and t[-1] in poi_info.index): continue\n",
    "        \n",
    "        print(t, '#%d ->' % cnt); cnt += 1; sys.stdout.flush()\n",
    "        \n",
    "        poi_logtransmat = gen_poi_logtransmat(trajid_list_train, traj_all, poi_info)\n",
    "        nodes = poi_info.copy()\n",
    "        edges = poi_logtransmat.copy()\n",
    "        edges = -1 * edges  # edge weight is negative log of transition probability\n",
    "\n",
    "        # DP\n",
    "        rec_dp = find_path(nodes, edges, t[0], t[-1], len(t))\n",
    "\n",
    "        # ILP\n",
    "        rec_ilp = find_path_ILP(nodes, edges, t[0], t[-1], len(t))\n",
    "\n",
    "        recdict_tran[tid] = {'REAL':t, 'REC_DP':rec_dp, 'REC_ILP':rec_ilp}\n",
    "        print(' '*10, rec_dp)\n",
    "        print(' '*10, rec_ilp); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if run_tran == True:\n",
    "    pickle.dump(recdict_tran, open(frecdict_tran, 'bw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_tran == True:\n",
    "    F11_tran = []\n",
    "    F12_tran = []\n",
    "    for tid in sorted(recdict_tran.keys()):\n",
    "        F11_tran.append(calc_F1(recdict_tran[tid]['REAL'], recdict_tran[tid]['REC_DP']))\n",
    "        F12_tran.append(calc_F1(recdict_tran[tid]['REAL'], recdict_tran[tid]['REC_ILP']))\n",
    "    print('DP : %.3f, %.3f' % (np.mean(F11_tran), np.std(F11_tran)))\n",
    "    print('ILP: %.3f, %.3f' % (np.mean(F12_tran), np.std(F12_tran)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Combine POI Ranking with Factorised Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE that:\n",
    "- Ranking is over POIs (in POI space)\n",
    "- Transition probabilities of Markov Chain of POI features (in POI feature space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two approaches to combine ranking with MC:\n",
    "- recommend trajectory in POI space: \n",
    "  - transform transition matrix of feature vector into transition matrix of POIs,\n",
    "  - recommend a sequence of POIs.\n",
    "- recommend trajectory in POI feature space: \n",
    "  - transform ranking probability of POIs into ranking probability of POI features,\n",
    "  - recommend a sequence of features, \n",
    "  - transform the squence of features into a sequence of POIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement the first approach, i.e., recommend trajectory in POI space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two methods for recommendation:\n",
    "- find a simple path with maximum objective in a digraph. (by negating log probabilities, it can be reduced to a finding shortest path will $k$-edges, which is probably NP-hard as it is the same as TSP when $L=|P|$, dynamic programming can be used for non-simple path.)\n",
    "- make greedy choice (of POI or POI feature vector) step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use dynamic programming to find a possibly non-simple path and use ILP to find a simple path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_comb == True:\n",
    "    recdict_comb = dict()\n",
    "    cnt = 1\n",
    "    for i in range(len(trajid_set_all)):\n",
    "        tid = trajid_set_all[i]\n",
    "        t = extract_traj(tid, traj_all)\n",
    "        \n",
    "        # trajectory too short\n",
    "        if len(t) < 3: continue\n",
    "        \n",
    "        # TSP, generally an ILP very hard for solvers (GUROBI/CBC)\n",
    "        if t[0] == t[-1]: continue\n",
    "\n",
    "        if uspecific == True:\n",
    "            user = traj_user.loc[tid, 'userID']\n",
    "            trajid_set_user = traj_user[traj_user['userID'] == user].index.tolist()\n",
    "            trajid_set_other = traj_user[traj_user['userID'] != user].index.tolist()\n",
    "            assert(tid in trajid_set_user)\n",
    "            trajid_set_add = [x for x in trajid_set_user if x != tid]\n",
    "            trajid_list_train = trajid_set_other.copy()\n",
    "            assert(KX > 0)\n",
    "            for k in range(KX):\n",
    "                trajid_list_train = trajid_list_train + trajid_set_add\n",
    "        else:\n",
    "            trajid_list_train = trajid_set_all[:i] + trajid_set_all[i+1:]\n",
    "\n",
    "        poi_info = calc_poi_info(trajid_list_train, traj_all, poi_all)\n",
    "        \n",
    "        # trajectory too long\n",
    "        if len(t) > poi_info.shape[0]: continue\n",
    "            \n",
    "        # start/end is not in training set\n",
    "        if not (t[0] in poi_info.index and t[-1] in poi_info.index): continue\n",
    "        \n",
    "        print(t, '#%d ->' % cnt); cnt += 1; sys.stdout.flush()\n",
    "        \n",
    "        train_df = gen_train_df(trajid_list_train, df_columns, traj_all, poi_info, poi_distmat, query_id_dict)\n",
    "        ranksvm = RankSVM(ranksvm_dir, useLinear=True)\n",
    "        ranksvm.train(train_df)\n",
    "        test_df = gen_test_df(t[0], t[-1], len(t), df_columns, poi_info, poi_distmat, query_id_dict)\n",
    "        rank_df = ranksvm.predict(test_df)\n",
    "        rank_df.set_index('poiID', inplace=True)\n",
    "\n",
    "        nodes = rank_df.copy()\n",
    "        nodes['weight'] = -np.log(nodes['probability'])  # node weight is negative log of ranking probability\n",
    "        nodes.drop('probability', axis=1, inplace=True)\n",
    "        edges = poi_logtransmat.copy()\n",
    "        edges = -1 * edges  # edge weight is negative log of transition probability\n",
    "\n",
    "        # DP\n",
    "        rec_dp = find_path(nodes, edges, t[0], t[-1], len(t), withNodeWeight=True, alpha=ALPHA)\n",
    "\n",
    "        # ILP\n",
    "        rec_ilp = find_path_ILP(nodes, edges, t[0], t[-1], len(t), withNodeWeight=True, alpha=ALPHA)\n",
    "\n",
    "        recdict_comb[tid] = {'REAL':t, 'REC_DP':rec_dp, 'REC_ILP':rec_ilp} # save recommended trajectories\n",
    "        print(' '*10, rec_dp)\n",
    "        print(' '*10, rec_ilp); sys.stdout.flush()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if run_comb == True:\n",
    "    pickle.dump(recdict_comb, open(frecdict_comb, 'bw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_comb == True:\n",
    "    F11_comb = []\n",
    "    F12_comb = []\n",
    "    for tid in sorted(recdict_comb.keys()):\n",
    "        F11_comb.append(calc_F1(recdict_comb[tid]['REAL'], recdict_comb[tid]['REC_DP']))\n",
    "        F12_comb.append(calc_F1(recdict_comb[tid]['REAL'], recdict_comb[tid]['REC_ILP']))\n",
    "    print('DP : %.3f, %.3f' % (np.mean(F11_comb), np.std(F11_comb)))\n",
    "    print('ILP: %.3f, %.3f' % (np.mean(F12_comb), np.std(F12_comb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Random Guessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the two approaches of random guessing: combinatorial and experimental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import comb\n",
    "from math import factorial\n",
    "def rand_guess(npoi, length):\n",
    "    assert(length <= npoi)\n",
    "    if length == npoi: return 1\n",
    "    N = npoi - 2\n",
    "    m = length - 2 # number of correct POIs\n",
    "    k = m\n",
    "    expected_F1 = 0\n",
    "    while k >= 0:\n",
    "        F1 = (k + 2) / length\n",
    "        prob = comb(m, k) * comb(N-m, m-k) / comb(N, m)\n",
    "        expected_F1 += prob * F1\n",
    "        k -= 1\n",
    "    return expected_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_guess(20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1_rand1 = []\n",
    "F1_rand2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------------------------------------------------------------] 100%"
     ]
    }
   ],
   "source": [
    "if run_rand == True:\n",
    "    #recdict_rand = dict()\n",
    "    cnt = 1\n",
    "    total0 = traj_all[traj_all['trajLen'] > 2]['trajID'].unique().shape[0]\n",
    "    poi_dict = dict()\n",
    "    for tid in trajid_set_all:\n",
    "        tr = extract_traj(tid, traj_all)\n",
    "        for poi in tr:\n",
    "            if poi in poi_dict: poi_dict[poi] += 1\n",
    "            else: poi_dict[poi] = 1\n",
    "    \n",
    "    for i in range(len(trajid_set_all)):\n",
    "        tid = trajid_set_all[i]\n",
    "        t = extract_traj(tid, traj_all)\n",
    "        \n",
    "        # trajectory too short\n",
    "        if len(t) < 3: continue\n",
    "        \n",
    "        # TSP, generally an ILP very hard for solvers (GUROBI/CBC)\n",
    "        if t[0] == t[-1]: continue\n",
    "            \n",
    "        pois = [x for x in sorted(poi_dict.keys()) if poi_dict[x] > 1]\n",
    "        \n",
    "        # trajectory too long\n",
    "        if len(t) > len(pois): continue\n",
    "            \n",
    "        # start/end is not in training set\n",
    "        if not (t[0] in pois and t[-1] in pois): continue\n",
    "        \n",
    "        #print(t, '#%d ->' % cnt); cnt += 1; sys.stdout.flush()\n",
    "        print_progress(cnt, total0); cnt += 1\n",
    "        \n",
    "        F1_rand1.append(rand_guess(len(pois), len(t)))\n",
    "        pois1 = [x for x in pois if x not in {t[0], t[-1]}]\n",
    "        rec_ix = np.random.choice(len(pois1), len(t)-2, replace=False)\n",
    "        rec_rand = [t[0]] + list(np.array(pois1)[rec_ix]) + [t[-1]]\n",
    "        F1_rand2.append(calc_F1(t, rec_rand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinatorial F1: mean=0.613, std=0.093\n"
     ]
    }
   ],
   "source": [
    "print('Combinatorial F1: mean=%.3f, std=%.3f' % (np.mean(F1_rand1), np.std(F1_rand1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimental  F1: mean=0.617, std=0.120\n"
     ]
    }
   ],
   "source": [
    "print('Experimental  F1: mean=%.3f, std=%.3f' % (np.mean(F1_rand2), np.std(F1_rand2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
