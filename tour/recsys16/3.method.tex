\section{POI, Query and Path}
\label{method}

We would like to recommend a particular tour such that the user will visit a sequence of points of interests (POIs), denoted $p_1, \ldots, p_L$, that maximises utility. We are given the desired start ($p_1=p_s$) and end point ($p_L=p_e$), and an associated number $l$ of POIs desired, from which we propose a trajectory through the city. In Figure~\ref{fig:problems}, an example tour is shown in light blue, which starts at the POI denoted as a red square, visits two intermediate POIs, and terminates on the fourth POI. The tour of length 3 can be modeled as a sequence of directed edges in a graph containing all POIs in the city as nodes.

The training data consists of a set of tours of varying length in a particular city. We assume that all POIs in the city are visited at least once, and hence can construct a graph with POIs as nodes and potentially multiple directed edges between each pair of nodes. We extract the category, popularity, total number of visits and average visit duration for each POI. The set of categories are shown in Table~XX in the appendix, and the popularity is defined as the number of distinct users that visited the POI.

As a baseline approach, we recommend the trajectory based on the popularity of POIs only, that is we always suggest the top-$k$ most visited POIs for all visitors. This baseline approach, called \textsc{PoiPopularity}, ignores the start and end location, and its only adaptation to a particular request is to adjust $k$ to match the desired length.

\cheng{I have removed reference to user specific recommendations.}


\subsection{Ranking using the origin and destination}
\label{ranksvm}

In addition to popularity, we also can rank the recommended POIs based on the other three POI specific features (category, total visits and average duration).
We can learn a ranking of POIs by using rankSVM with linear kernel and $L2$ loss\cite{lranksvm},
\begin{displaymath}
\min_{\mathbf{w_r}} \frac{1}{2} \mathbf{w_r}^T \mathbf{w_r} +
                    C_r \sum_{(p_i, p_j) \in \mathcal{P}}
                    \max \left( 0, 1 - \mathbf{w_r}^T (\mathbf{f}_{p_i} - \mathbf{f}_{p_j}) \right)^2
\end{displaymath}
where $\mathbf{w_r}$ is a vector of parameters,
$C_r > 0$ is the regularization parameter and
$\mathbf{f}_{p_i}$ is the feature vector described below.

Furthermore, since we are constrained by the fact that trajectories have to be of length $L$ and start and end at certain points, we hope to improve the recommendation by using this information.
In other words, using the query $(p_s, p_e, L)$ we can construct new features by contrasting
candidate POIs with $p_s$ and $p_e$.

For each of the POI features above, we construct two new features by taking the difference of
the feature in POI $p$ with $(p_s, p_e)$ respectively.
For the category, we set the feature to one when their categories are the same.
For popularity and visit duration, we take the real valued difference.
The distance from POI $p$ to $p_s$ (and $p_e$) is computed using the haversine formula \cite{haversine},
\begin{displaymath}
  d(p, p_s) = 2 R_1 \arcsin \sqrt{ \sin^2 \frac{\Delta \phi}{2} +
    \cos \phi_p \cos \phi_{p_s} \sin^2 \frac{\Delta \lambda}{2} }
\end{displaymath}
where $R_1 = 6371.0088$ km is the mean earth radius \cite{earth_radius},
$\Delta \phi = \phi_p - \phi_{p_s}$, $\Delta \lambda = \lambda_p - \lambda_{p_s}$,
and $\phi_p$, $\lambda_p$ are the latitude and longitude of POI $p$ respectively.
Lastly we also include the required length $L$ of the trajectory as a feature for rankSVM.

For training the rankSVM, the labels are generated using the number of occurrences of
POI $p$ in trajectories grouped by query $(p_s, p_e, L)$,
without counting the occurrence of $p$ when it is the origin or destination POI of a trajectory.
We create another algorithm to recommend trajectory by utilising
the ranking of POIs described above,
the pseudo code of this algorithm, \textsc{PoiRank}, is described in Table~\ref{alg:poirank}.

\cheng{Is there a way to show the performance of rankSVM? If so, add to appendix.}

\begin{table}
\centering
\small
\begin{tabular}{rl}
\hline
\multicolumn{2}{l}{\textsc{PoiRank}: recommend trajectory by ranking POIs.} \\
\hline
 1:& Input: $\mathcal{P}, p_s, p_e, L$ \\
 2:& $\mathcal{T} = \{p_s\}$ \\
 3:& Train a rankSVM using POI and query related features \\
 4:& Produce a rank $<_{p_i, p_j} \subset \mathcal{P}^2$ w.r.t. query $(p_s, p_e, L)$\\
 5:& \textbf{repeat} \\
 6:&\hspace{10pt} Choose POI $p$ with highest rank from $\mathcal{P} \setminus \mathcal{T}$ \\
 7:&\hspace{10pt} Add $p$ to $\mathcal{T}$ \\
 8:&\textbf{until} $|\mathcal{T}| = L-1$ \\
 9:& Add $p_e$ to $\mathcal{T}$ \\
10:& \textbf{return} $\mathcal{T}$ \\
\hline
\end{tabular}
\caption{\textsc{PoiRank}: recommend trajectory by ranking POIs.}
\label{alg:poirank}
\end{table}


\subsection{Transition probabilities}
\label{transition}
%
%\begin{itemize}
%  \item Edge
As mentioned in section \ref{problem}, the transition from one POI to another in a trajectory
corresponds to a directed edge in a graph with POIs as nodes.

%  \item Factorised transition matrix - maximum likelihood
We model these transitions using a Markov Chain with POIs as states.
the transition probability from POI $p_i$ to POI $p_j$ is factorized according to
the following individual POI features.
\begin{enumerate}
\item The category of POI, $P_{\textsc{cat}}(p_j | p_i)$
      denotes the transition probability from the category of $p_i$ to the category of $p_j$.
\item The popularity of POI, which was first discritized with uniform intervals in log-scale,
      and $P_{\textsc{pop}}(p_j | p_i)$ denotes the transition probability from the interval with the popularity of
      $p_i$ to the interval with the popularity of $p_j$.
\item The total number of visits of POI, similarly, it was first discritized with uniform intervals in log-scale,
      and $P_N(p_j | p_i)$ denotes the transition probability from the interval with $N(p_i)$
      to the interval with $N(p_j)$.
\item The average visit duration of POI, and again, it was first discritized with uniform intervals in log-scale,
      and $P_{\bar{V}}(p_j | p_i)$ denotes the transition probability from the interval with $\bar{V}(p_i)$
      to the interval with $\bar{V}(p_j)$.
\item The neighborhood relationship between $p_i$ and $p_j$,
      which was represented by the geographical clusters of POIs that $p_i$ and $p_j$ reside in,
      and $P(c_{p_j} | c_{p_i})$ denotes the transition probability from the cluster with
      $p_i$ (i.e., $c_{p_i}$) to the cluster with $p_j$ (i.e., $c_{p_j}$).
\end{enumerate}

Assuming independence between these features,
the transition probability from $p_i$ to $p_j$ can be factorized as follows,
\begin{displaymath}
    P(p_j | p_i) =
    \begin{cases}
    \hfill 0, \hfill & i = j \\
    \hfill \frac{1}{Z_i} P_{\textsc{cat}}(p_j | p_i) \times P_{\textsc{pop}}(p_j | p_i) \times P_N(p_j | p_i) \\
    \hfill \times P_{\bar{V}}(p_j | p_i) \times P(c_{p_j} | c_{p_i}), \hfill & i \ne j
    \end{cases}
\end{displaymath}
where $p_i, p_j \in \mathcal{P}$ and $Z_i$ is a normalizing constant.

% Kronecker product
We compute the transition probabilities of the above individual POI features
using maximum likelihood estimation,
i.e., counting the number of transitions for each pair of features then normalizing each row,
taking care of zeros by adding a small number $\epsilon$
\footnote{In our experiments, $\epsilon = 1$.}
to each number before normalisation,
which results a transition matrix for each of the above POI features.

By computing the Kronecker product of transition matrices of all the POI features,
we get an unnormalised transition matrix of POI features.
However, to obtain the transition probabilities between each POI pair $(p_i, p_j)$,
there are two cases needs to be dealt with properly:
\begin{enumerate}
\item POI features which represent POIs that do not exist in $\mathcal{P}$,
\item POI features that corresponds to more than one POIs in $\mathcal{P}$.
\end{enumerate}

% deal with feature vector without corresponding POI or with more than one POIs.
For the first case,
the corresponding rows and columns in the result matrix of Kronecker product are simply removed.

The second case was a bit subtle.
Let POIs with exactly the same features be a POI group,
the transition probabilities associated with POIs in the same group are computed as follows:
\begin{itemize}
\item The incoming (unnormalised) transition probability of the group was divided uniformly among POIs
      in the same group, which is equivalent to choose a POI in the group uniformly at random;
\item The outgoing (unnormalised) transition probability of each POI should be the same as the
      outgoing transition probability of the POI group, as one had already been in the POI group in this case;
\item The self-loop of the POI group represents the transitions between POIs in the same group,
      suppose the (unnormalised) transition probability from a POI group to itself is $P_o$,
      and the number of POIs in the group is $N_o$,
      the transition probability from $p_i$ to $p_j$ in the same group is
      \begin{displaymath}
          P(p_j | p_i) =
          \begin{cases}
              \hfill 0, \hfill & i = j \\
              \hfill \frac{P_o}{N_o - 1}, \hfill & i \ne j \\
          \end{cases}
      \end{displaymath}
\end{itemize}
Finally, the unnormalised outgoing transition probabilities of each POI were normalized to form
a valid probability distribution
\footnote{Note that dealing with the second case before or after the normalization leads to
the same transition probabilities, which can be easily proved.}.

%  \item Viterbi
%  \item \textsc{Markov}
Given the transition matrix of POIs $A$, the log likelihood of trajectory $\mathcal{T}$ is
\begin{displaymath}
    \ell(\mathcal{T}) = \sum_{j=1}^{|\mathcal{T}|-1} \log P(\mathcal{T}_{j+1} | \mathcal{T}_j)
\end{displaymath}

We can recommend a trajectory with respect to query $(p_s, p_e, L)$ by maximising the likelihood
using a variant of Viterbi algorithm, the pseudo code of this algorithm,
i.e., \textsc{Markov} is shown below.
%\end{itemize}

\begin{table}
\centering
\small
\begin{tabular}{rl}
\hline
\multicolumn{2}{l}{\textsc{Markov}: recommend trajectory by maximising its likelihood.} \\
\hline
 1:& Input: $\mathcal{P}, A, p_s, p_e, L$ \\
 2:&\textbf{for} $p \in \mathcal{P}$ \\
 3:&\hspace{10pt} $M_1[2, p] = \log A_{p_s, p}$ \\
 4:&\hspace{10pt} $M_2[2, p] = p_s$ \\
 5:&\textbf{end for} \\
 6:&\textbf{for} $l=3$ \textbf{to} $L$ \\
 7:&\hspace{10pt}\textbf{for} $p \in \mathcal{P}$ \\
 8:&\hspace{20pt}   \(\displaystyle M_1[l, p] = \max_{p' \in \mathcal{P}} \{ M_1[l-1, p'] + \log A_{p', p} \) \\
 9:&\hspace{20pt}   \(\displaystyle M_2[l, p] = \argmax_{p' \in \mathcal{P}} \{ M_1[l-1, p'] + \log A_{p', p} \) \\
10:&\hspace{10pt}\textbf{end for} \\
11:&\textbf{end for} \\
% //trace back to find the actual path
12:&$\mathcal{T} = \{p_e\}$ \\
13:&$p = \mathcal{T}[0]$ \\
14:&$l = L$ \\
15:&\textbf{repeat} \\
16:&\hspace{10pt}$\mathcal{T}$.prepend$(M_2[l, p])$ \\
17:&\hspace{10pt}$p = \mathcal{T}[0]$ \\
18:&\hspace{10pt}$l = l - 1$ \\
19:&\textbf{until} $l < 2$ \\
20:&\textbf{return} $\mathcal{T}$ \\
\hline
\end{tabular}
%\caption{}
%\label{}
\end{table}


\subsection{Performance metric}
\label{metric}
%
% F1 vs Kendall's $\tau$.
To evaluate the performance of different trajectory recommendation algorithms,
we employ the trajectory F$_1$-score\cite{ijcai15} to measure the POIs that are
correctly recommended. Let $\mathcal{T}$ be the trajectory that was visited in the real world,
and $\hat{\mathcal{T}}$ be the recommended trajectory,
$\mathcal{P}_{\mathcal{T}}$ be the set of POIs visited in $\mathcal{T}$,
and $\mathcal{P}_{\hat{\mathcal{T}}}$ be the set of POIs visited in $\hat{\mathcal{T}}$,
trajectory F$_1$-scores was defined as
\begin{displaymath}
    F_1 = \frac{2 |\mathcal{P}_{\mathcal{T}} \cap \mathcal{P}_{\hat{\mathcal{T}}}|}
               {|\mathcal{P}_{\mathcal{T}}| + |\mathcal{P}_{\hat{\mathcal{T}}}|}
\end{displaymath}

A perfect trajectory F$_1$-score (i.e., F$_1 = 1$) means the POIs in the recommended trajectory are exactly
the same POIs as those in the ground truth, and a $0$ trajectory F$_1$-score means that unfortunately none of
the POIs in the real trajectory was recommended.

In addition, to measure the quality of recommended visiting order of POIs, 
we propose a new metric $F_{1,\textsc{order}}$,
\begin{displaymath}
F_{1,\textsc{order}} = \frac{2 \times P_{\textsc{order}} \times R_{\textsc{order}}} 
                           {P_{\textsc{order}} + R_{\textsc{order}}}
\end{displaymath}
where 
\begin{displaymath}
P_{\textsc{order}} = \frac{N_c} {|\hat{\mathcal{T}}|(|\hat{\mathcal{T}}|-1) / 2}, 
R_{\textsc{order}} = \frac{N_c} {|\mathcal{T}|(|\mathcal{T}|-1) / 2}
\end{displaymath}
and $N_c$ is the number of POI pairs $(p_j, p_k)$ that satisfies the following 
constraints\footnote{We define $F_{1,\textsc{order}}=0$ if $N_c$ is $0$.}:
\begin{align*}
    (p_j \prec_{\mathcal{T}} p_k ~\land~ p_j \prec_{\hat{\mathcal{T}}} p_k) & ~\lor~
    (p_j \succ_{\mathcal{T}} p_k ~\land~ p_j \succ_{\hat{\mathcal{T}}} p_k) \\
    p_j \ne p_k, &~~ p_j, p_k \in \mathcal{P}_{\mathcal{T}} \cap \mathcal{P}_{\hat{\mathcal{T}}} \\
    j \ne k, &~~ 1 \le j, k \le |\mathcal{T}|
\end{align*}

A perfect trajectory $F_{1,\textsc{order}}$, i.e., $F_{1,\textsc{order}} = 1$, means that both the POIs and their visiting order in the
recommended trajectory are exactly the same as these in the real trajectory,
and $F_{1,\textsc{order}} = 0$ means that none of the recommended POI pairs was actually visited in the real trajectory.


\section{Tour Recommendation}
\label{recommendation}

\cheng{Describe combining} \\
Now that we got rankings of POIs as well as transition probabilities,
we want to leverage both of them when recommending trajectories.

\subsection{Walks vs Paths}
\label{walkpath}
%
%\begin{itemize}
%  \item No sub tours - tottering
One problem of \textsc{Markov} algorithm described in section \ref{transition} is the recommended
trajectory contains sub-tours (or tottering), i.e., some POIs were visited more than once.
We can eliminate sub-tours by specifying additional constraints when recommending trajectories.

%  \item ILP
In particular, we maximise the trajectory likelihood using an integer linear programming with
sub-tour elimination constraints adapted from the Travelling Salesman Problem\cite{opt98},
specifically, given a set of POIs $\mathcal{P}$, transition matrix $A$ with transition probabilities
between POIs in $\mathcal{P}$ and a query $(p_s, p_e, L)$,
we recommend a trajectory by solving the following integer linear programming:
\begin{align*}
\text{Maximize} & \sum_{i=1}^{N-1} \sum_{j=2}^N x_{ij} \log A_{p_i, p_j} \\
s.t. & x_{ij} \in \{0, 1\}, \forall i, j = 1, \cdots, N \\
     & \sum_{j=2}^N x_{1j} = \sum_{i=1}^{N-1} x_{iN} = 1 \\
     & \sum_{i=1}^{N-1} x_{ik} = \sum_{j=2}^N x_{kj} \le 1, \forall k=2, \cdots, N-1 \\
     & \sum_{i=1}^{N-1} \sum_{j=2}^N = L-1 \\
     & u_i - u_j + 1 \le (N-1) (1-x_{ij}), \forall i, j = 2, \cdots, N
\end{align*}
where $x_{ij}$ is a binary decision variable which determines whether transition from $p_i$ to $p_j$
occurred in the recommended trajectory.
For brevity, we assume $x_{i1}$ and $x_{1j}$ corresponds to the incoming and outgoing transitions of POI $p_s$,
similarly, $x_{iN}$ and $x_{Nj}$ corresponds to the incoming and outgoing transitions of POI $p_e$.
The second constraint restricts that only one outgoing (and incoming) transition for $p_s$ ($p_e$)
is permitted, i.e., the recommended trajectory should start from $p_s$ and end at $p_e$.
The third constraint restricts that any POI should be visited at most once and the fourth constraint
restrict that only $L-1$ transitions between POIs are permitted, i.e., the number of visited POIs should be
exactly $L$ (including $p_s$ and $p_e$).
The last constraint restricts that no sub-tours are permitted in the recommended trajectory.

%  \item \textsc{MarkovPath}
%\end{itemize}
The algorithm that uses the above approach to recommend trajectory was denoted by \textsc{MarkovPath}.


\subsection{POI ranking and transitions}
To recommend the \textit{most likely} trajectory with respect to query $(p_s, p_e, L)$,
we want to combine the ranking of POIs with the transition probabilities.
First, we transform the ranking scores of POIs with respect to query $(p_s, p_e, L)$
to a probability distribution using softmax function
\begin{displaymath}
    %P(y=1 |p) = \frac{1}{1 + e^{A f(x) + B}, p \in P
    P_R(p_j |(p_s, p_e, L)) = \frac{e^{R(p_j)}}{\sum_j e^{R(p_j)}}
\end{displaymath}
where $R(p_j)$ is the ranking score of POI $p_j$ with respect to query $(p_s, p_e, L)$.

%\begin{itemize}
%  \item Combine node and edge
we want to maximize both the product of ranking probabilities of POIs in the recommended trajectory and
the likelihood of the recommended trajectory.
Specifically, we want to maximize the following two quantities at the same time.
For brevity, we use $x$ to denote the constraint $(p_s, p_e, L)$ and $y$ to denote the
recommended trajectory $(p_{j_1}, \dots, p_{j_L})$ where $p_s = p_{j_1}$ and $p_e = p_{j_L}$.
\begin{itemize}
\item The logarithm of the product of ranking probabilities of POIs in the recommended 
trajectory\footnote{We exclude the origin and destination POIs.}:
      \begin{displaymath}
          %\ell_R(y) = \sum_{k=1}^L \log P_R(p_{j_k} | x)
          \ell_R(y) = \sum_{k=2}^{L-1} \log P_R(p_{j_k} | x)
      \end{displaymath}
\item The log likelihood of recommended trajectory:
      \begin{displaymath}
          \ell(y) = \sum_{k=1}^{L-1} \log P(p_{j_{k+1}} | p_{j_k})
      \end{displaymath}
\end{itemize}

%  \item Heuristics: \textsc{Rank+Markov}, \textsc{Rank+MarkovPath}
One heuristic is to optimize the following objective:
\begin{displaymath}
    \alpha \ell_R(y) + (1-\alpha) \ell(y)
\end{displaymath}
where $\alpha \in [0, 1]$ is parameter to trade-off the importance between the ranking of POIs
and the transitions between POIs in the recommended trajectory.
i.e.,
\begin{align*}
    & \argmax_{y \in \mathcal{P}^L} \alpha \ell_R(y) + (1-\alpha) \ell(y) \\
   =& \argmin_{y \in \mathcal{P}^L} - \alpha \ell_R(y) - (1-\alpha) \ell(y) \\
   =& \argmin_{y \in \mathcal{P}^L} - \alpha \sum_{k=2}^{L-1} \log P_R(p_{j_k} | x) -
      (1-\alpha) \sum_{k=1}^{L-1} \log P(p_{j_{k+1}} | p_{j_k})
\end{align*}
such that
\begin{align*}
    p_{j_1} &= p_s, ~ p_{j_L} = p_e \\
    p_{j_k} &\in \mathcal{P}, 1 \le k \le L \\
    \alpha  &\in [0, 1]
\end{align*}

We optimize this objective by adapting the Viterbi algorithm using recursive relation
\begin{equation}
    \label{eq:min}
    M_1[l+1, p] = \min_{p' \in \mathcal{P}} \{ M_1[l, p'] - \alpha \log P_R(p|x) - (1-\alpha) \log P(p|p') \}
\end{equation}
and
\begin{equation}
    \label{eq:argmin}
    M_2[l+1, p] = \argmin_{p' \in \mathcal{P}} \{ M_1[l, p'] - \alpha \log P_R(p|x) - (1-\alpha) \log P(p|p') \}
\end{equation}
where $M_1[l, p]$ stores the minimum value that associated with the (partial) trajectory
that starts at $p_s$ and ends at $p$ with exactly $l$ POIs,
$M_2[l, p]$ stores the predecessor of $p$ in the (partial) trajectory.

The minimum objective value is $M_1[L, p_e]$,
and the corresponding trajectory can be found by tracing back from $M_2[L, p_e]$.
The pseudo code of the above algorithm is show in table \ref{algo:dp}.


\begin{table}
\centering
%\small
\begin{tabular}{rl}
\hline
\multicolumn{2}{l}{\textbf{Algorithm}: \textsc{Rank+Markov}} \\
\hline
 1:& Input: $p_s, p_e, L$ \\
 2:&\textbf{for} $p \in \mathcal{P}$ \\
% 3:&\hspace{10pt}$M_1[2, p] = - \alpha(\log P_R(p|x) + \log P_R(p_s|x)) - (1-\alpha) \log P(p|p_s)$ \\
 3:&\hspace{10pt}$M_1[2, p] = - \alpha \log P_R(p|x) - (1-\alpha) \log P(p|p_s)$ \\
 4:&\hspace{10pt}$M_2[2, p] = p_s$ \\
 5:&\textbf{end for} \\
 6:&\textbf{for} $l=2$ \textbf{to} $L-1$ \\
 7:&\hspace{10pt}\textbf{for} $p \in \mathcal{P}$ \\
 8:&\hspace{20pt}   Compute $M_1[l+1, p]$ using equation (\ref{eq:min}) \\
 9:&\hspace{20pt}   Compute $M_2[l+1, p]$ using equation (\ref{eq:argmin}) \\
%\(\displaystyle M_1[l+1, p] = \min_{p' \in \mathcal{P}} \{ M_1[l, p'] - \alpha \log P_R(p|x) - (1-\alpha) \log P(p|p') \)
%\(\displaystyle M_2[l+1, p] = \argmin_{p' \in \mathcal{P}} \{ M_1[l, p'] - \alpha \log P_R(p|x) - (1-\alpha) \log P(p|p') \)
10:&\hspace{10pt}\textbf{end for} \\
11:&\textbf{end for} \\
% //trace back to find the actual path
12:&$\mathcal{T}= [p_e]$ \\
13:&$p = \mathcal{T}[0]$ \\
14:&$l = L$ \\
15:&\textbf{repeat} \\
16:&\hspace{10pt}$\mathcal{T}$.prepend$(M_2[l, p])$ \\
17:&\hspace{10pt}$p = \mathcal{T}[0]$ \\
18:&\hspace{10pt}$l = l - 1$ \\
19:&\textbf{until} $l < 2$ \\
20:&\textbf{return} $\mathcal{T}$ \\
\hline
\end{tabular}
    \caption{Recommend trajectory by utilising both POI ranking and transition}
\label{algo:dp}
\end{table}

Similar to the \textsc{Markov} algorithm,
the recommended trajectory by \textsc{Rank+Markov} could also contain sub-tours (e.g., tottering),
we can use similar approach to restrict that no sub-tour exists in the recommended trajectory
by maximising the following objective using integer linear programming
\begin{displaymath}
    \text{Maximize}  \sum_{i=1}^{N-1} \sum_{j=2}^N x_{ij} (\alpha \log P_R(p_j | x) + (1-\alpha) \log A_{p_i, p_j})
\end{displaymath}
The constraints are the same as those in algorithm \textsc{MarkovPath}.
We denote this algorithm as \textsc{Rank+MarkovPath}.

\subsection{Structured SVM}
\label{ssvm}
%  \item \textsc{StructuredSVM}
As trajectory is a sequence of POI visits,
thus, the recommended trajectory with respect to constraint $x = (p_s, p_e, L)$
can be viewed as a chain of $L$ variables,
with the first and last variables been observed, the states of variables are the set of POIs $\mathcal{P}$,
by exploiting the interactions between neighboring variables,
hopefully we can improve the quality of the recommended trajectories.

% describe joint feature vector (node/unary, edge/pairwise)
Structured prediction is able to incorporate both the features of variables (i.e., unary features) and
the features of interactions between neighboring variables (i.e., pairwise features) to make a prediction, i.e.,
\begin{displaymath}
    y^* = \argmax_{y \in \mathcal{P}^L} \sum_{j=1}^L \mathbf{w_u}^T \phi_j(x, y_j) +
                                        \sum_{j=1}^{L-1} \mathbf{w_p}^T \phi_{j, j+1}(x, y_j, y_{j+1})
\end{displaymath}
where $\phi_j$ is the unary features of the $j$-th variable and $\phi_{j, j+1}$ is the pairwise features between
the $j$-th and $(j+1)$-th variables, $x = (p_s, p_e, L)$ is the constraint, $\mathbf{w_u}$ and $\mathbf{w_p}$ are the
parameters of unary and pairwise features respectively.

In the settings of trajectory recommendation, the ranking probabilities of POIs
with respect to constraint $x = (p_s, p_e, L)$ were used to capture the unary features of individual variables
and the transition probabilities between POIs were utilized to capture the pairwise features
between the neighboring variables, in particular,
the unary features of the first and last variables are binary vectors
with true values at the corresponding POIs and false values anywhere else,
the unary features of the other $L-2$ variables are the ranking probabilities of POIs in $\mathcal{P}$, i.e.,
\begin{displaymath}
    \phi_j(x, y_j) = P_R(y_j | x), y_j \in \mathcal{P}
\end{displaymath}
Pairwise features between the $j$-th variable and the $(j+1)$-th variable $\phi_{j, j+1}$ was defined as
\begin{align*}
    \phi_{j, j+1}(x, y_j, y_{j+1}) &= \phi_{j-1, j}(x, y_{j-1}, y_j) \times A \\
                                 j &=2, \dots, L-2
\end{align*}
where $\phi_{j-1, j}$ is the pairwise features between the $(j-1)$-th and $j$-th variables,
and $A$ is the transition matrix between POIs described in section \ref{method:transition}.
In particular, the pairwise features between the first and the second variables is the
outgoing transition probabilities of the first variable,
and the pairwise features between the second-last and the last variables are a probability distribution
over all POIs in $\mathcal{P}$ where the probability mass is dominated by the variable corresponding to POI $p_e$,
all other POIs in $\mathcal{P} \setminus p_e$ are simply uniformly distributed.

% describe unary/pairwise potentials?

% describe SSVM training (1-slack formulation)
To estimate the parameters $\mathbf{w_u}$ and $\mathbf{w_p}$, we train a structured support vector machine
using the 1-slack formulation\cite{ssvm09},
\begin{align*}
    \min_{\mathbf{w}, \xi \ge 0} ~~& \frac{1}{2} \mathbf{w}^T \mathbf{w} + C \xi \\
    s.t. ~~& \frac{1}{N} \mathbf{w}^T \sum_{i=1}^N \delta(\hat{y^{(i)}}) \ge
                  \frac{1}{N} \sum_{i=1}^N \Delta(y^{(i)}, \hat{y^{(i)}}) - \xi \\
         ~~& \forall \hat{y^{(i)}} \in \mathcal{P}^{|y^{(i)}|}, i = 1, \cdots, N
\end{align*}
where $\mathbf{w} = [\mathbf{w_u}^T, \mathbf{w_p}^T]^T$ is the parameter vector,
$N$ is the total number of trajectories in training set, $C$ is the regularization parameter,
$\xi$ is the slack variable, and
\begin{displaymath}
    \delta(\hat{y^{(i)}}) = \Psi(x^{(i)}, y^{(i)}) - \Psi(x^{(i)}, \hat{y^{(i)}})
\end{displaymath}
where $\Psi(x, y)$ is the joint feature vector which is a composite of unary features and pairwise features of the
$i$-th example in training set,
$\Delta(y^{(i)}, \hat{y^{(i)}})$ is the loss associated with the $i$-th trajectory in training set and
its corresponding recommended trajectory, and Hamming loss was used in this work.

% describe SSVM inference (y^* = argmax_y w^T psi(x, y), viterbi?)
%\end{itemize}

\subsection{Related Work}
%
%Describe IJCAI15 paper, and show \textsc{PersTourL}.

The work that most similar to our approach was the \textsc{PersTour} algorithm proposed in \cite{ijcai15},
it formulated trajectory recommendation as an Orienteering Problem and use integer linear programming to
maximise an objective which is a composite of the total POI popularity in recommended trajectory and
the total user interest at the recommended POIs.
A user's interest on a specific category of POIs is modelled as the summation of ratios of the his/her
actual visit duration at these POIs over the average visit duration in general.
Furthermore, \textsc{PersTour} utilise a time budget to restrict the recommended trajectory besides the
origin and destination.

To make comparison with \textsc{PersTour}, the time budget was replaced with the total number of POIs to visit
and was denoted by algorithm \textsc{PersTour-L} in this paper.
