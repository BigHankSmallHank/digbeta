\section{Proposed Method}
\label{method}


\subsection{Problem Formulation}
\label{method:formulation}
For a set of Point/Place of Interest (POI) $\mathcal{P}$ and a set of users $\mathcal{U}$,
a trajectory $\mathcal{T}^u$ of user $u$ is an ordered sequence of POIs,
i.e.,
\begin{displaymath}
    \mathcal{T}^u = ((p_1, t_{p_1}^s, t_{p_{1}}^e), \dots, (p_L, t_{p_L}^s, t_{p_L}^e)), 
    u \in \mathcal{U}, 
    p_j \in \mathcal{P}, 1 \le j \le L
\end{displaymath}
where $t_p^s$ and $t_p^e$ is the start and end time of user $u$ at POI $p$ respectively.

Given a set of trajectories $\{ \mathcal{T}_u^i \}, u \in \mathcal{U}$ with visited POIs in $\mathcal{P}$, 
a start POI $p_s$, a destination $p_e$ and an integer $2 < L \le |\mathcal{P}|$,
trajectory recommendation is to recommend the \textit{most likely} trajectory $(p_s, \dots, p_e)$ to user $u$ such that
the number of visited POIs is $L$.


\subsection{Ranking of POIs}
\label{method:ranking}
% ranking of POIs focus on modeling the (personalised) preference of POIs
We first produce a ranking of POIs, i.e., $<_{p_i, p_j} \subset \mathcal{P}^2$,
with respect to constraint $(p_s, p_e, L)$ 
using rankSVM with linear kernel and $L2$ loss\cite{lranksvm}, 
i.e.,
\begin{displaymath}
\min_{\mathbf{w_r}} \frac{1}{2} \mathbf{w_r}^T \mathbf{w_r} +
                  C_r \sum_{(i, j) \in P} \max \left( 0, 1 - \mathbf{w_r}^T (\mathbf{x}_i - \mathbf{x}_j) \right)^2
\end{displaymath}
where $\mathbf{w_r}$ is a vector of parameters, 
$C_r > 0$ is the regularization parameter and 
$\mathbf{x}_i$ is the feature vector of POI $p_i$.
Features used for ranking POIs are:
\begin{itemize}
\item Popularity of a POI, which is defined as the number of distinct users that visited the POI\cite{ht10}.
\item The total number of visits of a POI,
      \begin{displaymath}
          N(p) = \sum_{u \in \mathcal{U}} \sum_i \sum_{p_j \in \mathcal{T}_u^i} \delta(p_j = p)
      \end{displaymath}
\item Average visit duration of POI\cite{ijcai15},
      \begin{displaymath}
          \bar{V}(p) = \frac{1}{N(p)} \sum_{u \in \mathcal{U}} \sum_i \sum_{p_j \in \mathcal{T}_u^i} (t_{p_j}^s - t_{p_j}^e) \delta(p_j = p)
      \end{displaymath}
\item Whether the category (e.g., structures, entertainment and shopping etc.) is the same as that of $p_s$ (and $p_e$).
\item Distance from POI $p$ to $p_s$ (and $p_e$) using haversine formula \cite{haversine},
      \begin{displaymath}
      d = 2 R_1 \arcsin \sqrt{ \sin^2 \left( \frac{\Delta \phi}{2} \right) + 
           \cos \phi_p \cos \phi_{p_s} \sin^2 \left( \frac{\Delta \lambda}{2} \right) }
      \end{displaymath}
            where $R_1 = 6371.0088$ km is the mean earth radius \cite{earth_radius}, 
            $\Delta \phi = \phi_p - \phi_{p_s}$, $\Delta \lambda = \lambda_p - \lambda_{p_s}$,
            and $\phi_p$, $\lambda_p$ are the latitude and longitude of POI $p$ respectively.
\item The difference in popularity of POI $p$ from $p_s$ (and $p_e$),
      i.e., $\Delta Pop = Pop(p) - Pop(p_s)$.
\item The difference in average visit duration of POI $p$ from that of $p_s$ (and $p_e$),
      i.e., $\Delta \bar{V} = \bar{V}(p) - \bar{V}(p_s)$
\item The number of POIs in the expected trajectory, i.e. $L$
\end{itemize}

To generate the target/label of a POI in training set,
we first group trajectories with the same start POI $p_s'$, end POI $p_e'$ and the number of visited POIs $L'$ together,
each group of trajectories form a query,
and the target/label of POI $p$ in a particular query (i.e., trajectory group) are the number of occurrence
in trajectories of that query, 
without counting the occurrence of $p$ as start or end POI of a trajectory.

The ranking scores of POIs are transformed to probability distribution using softmax function
\begin{displaymath}
    %P(y=1 |p) = \frac{1}{1 + e^{A f(x) + B}, p \in P
    P_R(p_j |(p_s, p_e, L)) = \frac{e^{R(p_j)}}{\sum_j e^{R(p_j)}}
\end{displaymath}
where $R(p_j)$ is the ranking score of POI $p_j$.


\subsection{Factorizing Transition Probabilities between POIs}
\label{method:transition}
To deal with data sparsity,
the transition probability between a pair of POIs $(p_i, p_j)$ was factorized into the product of
transition probabilities between the individual features of that POI.
\begin{enumerate}
\item The category of POI, $P(Cat(p_j) | Cat(p_i))$
      is the transition probability from the category of $p_i$ to the category of $p_j$.
\item The popularity of POI, which was first discritized with uniform intervals in log-scale,
      and $P(Pop(p_j) | Pop(p_i))$ is the transition probability from the interval with $Pop(p_i)$ 
      to the interval with $Pop(p_j)$.
\item The total number of visits of POI, similarly, it was first discritized with uniform intervals in log-scale,
      and $P(N(p_j) | N(p_i))$ is the transition probability from the interval with $N(p_i)$ 
      to the interval with $N(p_j)$.
\item The average visit duration of POI, and again, it was first discritized with uniform intervals in log-scale,
      and $P(\bar{V}(p_j) | \bar{V}(p_i))$ is the transition probability from the interval with $\bar{V}(p_i)$ 
      to the interval with $\bar{V}(p_j)$.
\item The neighborhood relationship between $p_i$ and $p_j$,
      which was represented by the geographical clusters of POIs that $p_i$ and $p_j$ reside in,
      and $P(Neighb(p_i, p_j)) = P(c_{p_j} | c_{p_i})$ is the transition probability from the cluster with 
      $p_i$ (i.e., $c_{p_i}$) to the cluster with $p_j$ (i.e., $c_{p_j}$).
\end{enumerate}

Assuming independence between these features,
the transition probability between $p_i$ and $p_j$ can be factorized as follows,
\begin{align*}
    & P(p_j | p_i) \\
   =& \frac{1}{Z_i} P(Cat(p_j) | Cat(p_i)) \times P(Pop(p_j) | Pop(p_i)) \times \\
    & P(N(p_j) | N(p_i)) \times P(\bar{V}(p_j) | \bar{V}(p_i)) \times P(Neighb(p_i, p_j)) \\
    & i \ne j, p_i, p_j \in \mathcal{P}
\end{align*}
where $Z_i$ is a normalizing constant.

%POIs are grouped into several clusters according to their geographical coordinates using K-means
%to reflect their neighborhood relationships.

% Kronecker product
We compute the transition probabilities of the above individual POI features 
using maximum likelihood estimation, 
i.e., counting the number of transitions for each pair of features then normalizing each row,
taking care of zeros by adding a small number $\epsilon$
\footnote{In our experiments, $\epsilon$ for a specific row in the transition matrix equals $0.2$ times 
the minimum non-zero value in that row.}
to each number,
which results a transition matrix for each of the above POI features.

By computing the Kronecker product of transition matrices of all the POI features,
we get an unnormalized transition matrix of POI features.
However, to obtain the transition probabilities between each POI pair $(p_i, p_j), i \ne j$,
there are two cases needs to be dealt with:
\begin{enumerate}
\item POI features which represent POIs that do not exist in $\mathcal{P}$,
\item POI features that corresponds to more than one POIs in $\mathcal{P}$.
\end{enumerate}

% deal with feature vector without corresponding POI or with more than one POIs.
For the first case, 
the corresponding rows and columns in the result matrix of Kronecker product are simply removed.

The second case was a bit subtle.
POIs with exactly the same features are treated as a POI group,
the incoming transition probability of the group was divided uniformly among POIs in the same group,
which corresponds to choose a POI in the group uniformly at random;
the outgoing transition probability of each POI is set the same as the outgoing transition probability of the POI group,
as one had already been in the POI group in this case;
the self-loop of the POI group represents the transitions between POIs in the group,
suppose the transition probability from a POI group to itself is $P_o$,
for each POI $p_i$ in the group, the transition probability from $p_i$ to any other POI $p_j$ in the same group
is $\frac{P_o}{M-1}$, where $M$ is the number of POIs in the group.
\footnote{Self-loop of individual POIs are forbidden here, so $P(p_i | p_i) = 0$ for all $p_i \in \mathcal{P}$.}

Finally, we normalize each row of the transition matrix of POIs to get transition probabilities for each pair of POIs.
\footnote{Note that dealing with the second case before or after row normalization leads to the same transition probabilities, 
as the process does not change the normalizing constant of each row of the transition matrix.}


\subsection{Recommend Trajectories}
\label{method:recommend}
%\subsection{Probabilistic Model incorporating both ranking and transition}
% ranking of POIs focus on modeling the (population) preference of POIs
% MC focus on modeling the transition patterns between POIs
% we can capture both aspects by combining them

To recommend the \textit{most likely} trajectory with respect to constraint $(p_s, p_e, L)$,
we want to maximize the ranking probabilities of POIs in the trajectory as well as
the likelihood of the trajectory w.r.t. the Markov Chain corresponds to the transition matrix of POIs.
Specifically, we want to maximize both of the following two quantities.
For brevity, we use $x$ to denote the constraint $(p_s, p_e, L)$ and $y$ to denote the corresponding
recommended trajectory $(p_{j_1}, \dots, p_{j_L})$.

\begin{itemize}
\item The ranking probabilities of POIs in the recommended trajectory:
      \begin{displaymath}
          \ell_R(y) = \sum_{k=1}^L \log P_R(p_{j_k} | x)
      \end{displaymath}
\item The log likelihood of recommended trajectory:
      \begin{displaymath}
          \ell(y) = \sum_{k=1}^{L-1} \log P(p_{j_{k+1}} | p_{j_k})
      \end{displaymath}
\end{itemize}
where $p_{j_1} = p_s$ and $p_{j_L} = p_e$.

Thus, we can optimize the following objective:
\begin{displaymath}
    \alpha \ell_R(y) + (1-\alpha) \ell(y)
\end{displaymath}
where $\alpha \in [0, 1]$ is parameter to trade-off the importance between the ranking probabilities 
and likelihood of the recommended trajectory.
i.e.,
\begin{align*}
    & \argmax_{y \in \mathcal{P}^L} \alpha \ell_R(y) + (1-\alpha) \ell(y) \\
   =& \argmax_{y \in \mathcal{P}^L} \alpha \sum_{k=1}^L \log P_R(p_{j_k} | x)) + 
      (1-\alpha) \sum_{k=1}^{L-1} \log P(p_{j_{k+1}} | p_{j_k}) \\
   =& \argmin_{y \in \mathcal{P}^L} - \alpha \sum_{k=1}^L \log P_R(p_{j_k} | x) -
      (1-\alpha) \sum_{k=1}^{L-1} \log P(p_{j_{k+1}} | p_{j_k}) 
\end{align*}
such that
\begin{align*}
    p_{j_1} &= p_s \\
    p_{j_L} &= p_e \\
    p_{j_k} &\in \mathcal{P}, 1 \le k \le L \\
    \alpha  &\in [0, 1]
\end{align*}

One way to find such a trajectory is transforming the above optimization problem to 
find a shortest path in a weighted graph.

Concretely,
we create a weighted directed graph $G$ with vertices $V$ that corresponds to the set of POIs $\mathcal{P}$ and 
edges $E$ which corresponds to transitions between POIs in $\mathcal{P}$,
the weight of vertex $v_{p}, p \in \mathcal{P}$ is set to the negative logarithm of the ranking probability of POI $p$, 
and the weight of directed edge $(v_p, v_{p'})$ is the negative logarithm of the transition probability from $p$ to $p'$,
i.e.,
\begin{align*}
    w(v_{p})       & = -\log(P_R(p | x)) \\
    w(v_p, v_{p'}) & = -\log(P(p' | p))
\end{align*}

Thus, compute the most likely trajectory with respect to constraint $x = (p_s, p_e, L)$ is
equivalent to find a path (not necessarily a simple path) from vertex $v_{p_s}$ to
$v_{p_e}$ with exactly $L$ vertices and minimum total path weights,
i.e.,
\begin{displaymath}
    \argmin_{(v_1, \dots, v_L)} \alpha \sum_{i=1}^{L} w(v_i) + \beta \sum_{i=1}^{L-1} w(v_i, v_{i+1})
\end{displaymath}
such that
\begin{align*}
    v_1 &= v_{p_s} \\
    v_L &= v_{p_e} 
\end{align*}
where $\beta = 1 - \alpha$.
The path can be found using dynamic programming, 
with array $M_1[l, v]$ stores the minimum total weights of path 
that starts at vertex $v_{p_s}$ and ends at vertex $v$ with 
exactly $l$ vertices in path,
array $M_2[l, v]$ stores the predecessor of $v$ in that path,
and compute a path with $l+1$ vertices by simply using the following recursive relations,
\begin{align*}
    M_1[l+1, v] &= \min_{v' \in Pa_v} \{ M_1[l, v'] + \alpha w(v) + \beta w(v', v) \} \\
    M_2[l+1, v] &= \argmin_{v' \in Pa_v} \{ M_1[l, v'] + \alpha w(v) + \beta w(v', v) \} 
\end{align*}
where $Pa_v$ is the parent of vertex $v$ in $G$,
i.e., 
there is a directed edge from $v'$ to $v$, $v' \in Pa_v$.

The minimum path weight is $M_1[L, v_{p_e}]$,
and the actual path can be found by tracing back from $M_2[L, v_{p_e}]$,
the complete algorithm to find this path is shown in figure \ref{algo:dp},
the sequence of POIs corresponding to vertices in that path is the 
trajectory we would like to recommend with respect to constraint $ x = (p_s, p_e, L)$.

\begin{table}
\centering
\begin{tabular}{rl}
\hline
\multicolumn{2}{l}{\textbf{Algorithm 1} for finding a path with minimum weight.} \\
\hline
 1:& Input: $V, E, p_s, p_e, L$ \\
 2:&\textbf{for} $v \in V$ \\
 3:&\hspace{10pt}\textbf{if} $(v_{p_s}, v) \in E$ \\
 4:&\hspace{20pt}   $M_1[2, v] = w(v_{p_s}, v) + \alpha w(v) + \beta w(v_{p_s})$ \\
 5:&\hspace{20pt}   $M_2[2, v] = v_{p_s}$ \\
 6:&\hspace{10pt}\textbf{else} \\
 7:&\hspace{20pt}   $M_1[2, v] = +\infty$ \\
 8:&\textbf{end for} \\
 9:&\textbf{for} $l=3$ \textbf{to} $L$ \\
10:&\hspace{10pt}\textbf{for} $v \in V$ \\
11:&\hspace{20pt}   \(\displaystyle M_1[l, v] = \min_{v' \in Pa_v} \{ M_1[l-1, v'] + \alpha w(v) + \beta w(v', v) \} \) \\
12:&\hspace{20pt}   \(\displaystyle M_2[l, v] = \argmin_{v' \in Pa_v} \{ M_1[l-1, v'] + \alpha w(v) + \beta w(v', v) \} \)\\
13:&\hspace{10pt}\textbf{end for} \\
14:&\textbf{end for} \\
% //trace back to find the actual path
15:&$path = [v_{p_e}]$ \\
16:&$v = path[0]$ \\
17:&$l = L$ \\
18:&\textbf{repeat} \\
19:&\hspace{10pt}$path$.prepend$(M_2[l, v])$ \\
20:&\hspace{10pt}$v = path[0]$ \\
21:&\hspace{10pt}$l = l - 1$ \\
22:&\textbf{until} $l < 2$ \\
23:&\textbf{return} $path$ \\
\hline
\end{tabular}
%\caption{Path Finding}
\label{algo:dp}
\end{table}


\subsection{Recommend Trajectories via Structured Prediction}
\label{method:structured}
Trajectory is a sequence of POI visits,
the recommended trajectory with respect to constraint $x = (p_s, p_e, L)$ can be viewed as a chain of $L$ variables, 
with the first and last variables been observed, the states of variables are the set of POIs $\mathcal{P}$,
by exploiting the interactions between neighbors, hopefully we can improve the recommendation.

% describe joint feature vector (node/unary, edge/pairwise)
Structured prediction is able to incorporate both the features of variables (i.e., unary features) and 
the features of interactions between different variables (i.e., pairwise features) to make a prediction, i.e.,
\begin{displaymath}
    y^* = \argmax_{y \in \mathcal{P}^L} \sum_{j=1}^L \mathbf{w_u}^T \phi_j(x, y_j) + 
                                        \sum_{j=1}^{L-1} \mathbf{w_p}^T \phi_{j, j+1}(x, y_j, y_{j+1})
\end{displaymath}
where $\phi_j$ is the unary features of the $j$-th variable and $\phi_{j, j+1}$ is the pairwise features between
the $j$-th and $(j+1)$-th variables, $x = (p_s, p_e, L)$ is the constraint, $\mathbf{w_u}$ and $\mathbf{w_p}$ are the 
parameters of unary and pairwise features respectively.

In the settings of trajectory recommendation, the ranking probabilities of POIs w.r.t constraint $x = (p_s, p_e, L)$
and the transition probabilities were used to capture the unary features of individual variables and pairwise features
between the neighboring variables, in particular, the unary features of the first and last variables are binary vectors 
with true values at the corresponding POIs and false values anywhere else,
the unary features of the other $L-2$ variables are the ranking probabilities of POIs in $\mathcal{P}$, i.e.,
\begin{displaymath}
    \phi_j(x, y_j) = P_R(y_j | x), p \in \mathcal{P}
\end{displaymath}
Pairwise features between the $j$-th variable and the $(j+1)$-th variable $\phi_{j, j+1}$ was defined as
\begin{displaymath}
    \phi_{j, j+1}(x, y_j, y_{j+1}) = \phi_{j-1, j}(x, y_{j-1}, y_j) \times A 
\end{displaymath}
where $j=2, \dots, L-2$, $\phi_{j-1, j}$ is the pairwise features between the $(j-1)$-th and $j$-th variables, 
and $A$ is the transition matrix between POIs in $\mathcal{P}$.
The pairwise features between the first and second variables is the outgoing transition probabilities of the first variable,
and the pairwise features between the second-last and last variables are a probability distribution over all POIs in 
$\mathcal{P}$ where the probability mass is dominated by the variable corresponding to POI $p_e$,
all other POIs $\mathcal{P} \setminus p_e$ are simply uniformly distributed.

% describe unary/pairwise potentials?

% describe SSVM training (1-slack formulation)
To estimate the parameters $\mathbf{w_u}$ and $\mathbf{w_p}$, we train a structured support vector machine using the 1-slack
formulation\cite{ssvm09},
\begin{align*}
    \min_{\mathbf{w}, \xi \ge 0} ~~& \frac{1}{2} \mathbf{w}^T \mathbf{w} + C \xi \\
    s.t. ~~& \frac{1}{N} \mathbf{w}^T \sum_{i=1}^N \delta(\hat{y^{(i)}}) \ge 
             \frac{1}{N} \sum_{i=1}^N \Delta(y^{(i)}, \hat{y^{(i)}}) - \xi \\
           & \forall \hat{y^{(i)}} \in \mathcal{P}^{|y^{(i)}|}, i = 1, \cdots, N
\end{align*}
where $\mathbf{w} = [\mathbf{w_u}^T, \mathbf{w_p}^T]^T$ is the parameter vector, 
$N$ is the total number of examples in training set, $C$ is the regularization parameter,
$\xi$ is the slack variable, $\delta(\hat{y^{(i)}}) = \Psi(x^{(i)}, y^{(i)}) - \Psi(x^{(i)}, \hat{y^{(i)}})$ and 
$\Psi(x, y)$ is the joint feature vector which is a composite of unary features and pairwise features of the 
$i$-th example in training set, $\Delta(y^{(i)}, \hat{y^{(i)}})$ is the loss associated with the actual and 
the recommended trajectory of the $i$-th example, which Hamming loss was used in this work.

% describe SSVM inference (y^* = argmax_y w^T psi(x, y), viterbi?)
