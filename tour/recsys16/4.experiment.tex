\section{Experimental Results}
\label{experiment}

\subsection{Dataset}
\label{experiment:dataset}
We experiment on five trajectory datasets, four of them were provided by \cite{ijcai15}.
Trajectories in these datasets were extracted from Yahoo! Flickr Creative Commons 100M 
(a.k.a. YFCC100M) dataset\cite{thomee2016yfcc100m} using metadata of photos and videos 
such as geographical locations, timestamps, user identifications etc, 
the details of building trajectories are covered in \cite{ht10} and \cite{ijcai15} and
we refer interested readers to these papers.
Statistics of the four trajectory datasets are described in table \ref{table:data}.
%
The time that a user arrived a POI was approximated by the time the first photo taken by the user at that POI,
similarly, the time that a user leaved a POI was approximated by the time the last photo taken by the user at 
that POI \cite{ht10, ijcai15}.
An example of trajectory with four POIs from Toronto dataset was shown in figure \ref{fig:traj}, 
%TODO: explain what is a POI/photo in the figure
where the four colored marker icons represent the four POIs, 
and the colored round icons represent a sample of photos assign to these POIs.


\begin{figure}
\centering
\epsfig{file=traj_eg.eps, width=3.5in}
\caption{An example of trajectory with four POIs}
\label{fig:traj}
\end{figure}

\begin{table*}
\centering
\begin{tabular}{lrrrrr} \hline
\textbf{Dataset} & \textbf{\#Photos} & \textbf{\#POI Visits} & \textbf{\#Trajectories} & \textbf{\#Users} \\ \hline
Edinburgh & 82,060 & 33,944 & 5,028 & 1,454 \\ 
Glasgow & 29,019 & 11,434 & 2,227 & 601 \\ 
Melbourne & 94,142 & 23,995 & 5,106 & 1,000 \\ 
Osaka & 392,420 & 7,747 & 1,115 & 450 \\ 
Toronto & 157,505 & 39,419 & 6,057 & 1,395 \\ 
\hline
\end{tabular}
\caption{Statistics of trajectory dataset}
\label{table:data}
\end{table*}


\subsection{Experimental Settings}
We use leave-one-out cross validation to evaluate different trajectory recommendation algorithms,
i.e., when evaluating a trajectory of a specific user, all other trajectories of this user as well as 
all trajectories of other users were used to train the recommendation algorithms.
In addition, we compared the experimental results on four trajectory datasets among the following $10$ algorithms.

\textsc{Random}: choose POIs uniformly at random (without replacement) 
    from the set of POIs $\mathcal{P} \setminus \{p_s, p_e \}$ to visit.

\textsc{PersTour}\cite{ijcai15}: personalised trajectory recommendation algorithm with time budget constraint.

\textsc{PersTour-L}: the same setting as \textsc{PersTour} but constraining the total number of POIs to visit 
    instead of the time budget.

\textsc{PoiPopularity}: choose POIs according to the ranking based on POI popularity only.

\textsc{PoiRank}: choose POIs according to the ranking based on POI specific and query specific features 
    described in section \ref{ranksvm}.

\textsc{Markov}: recommend trajectory according to the factorized transition matrix described in section \ref{transition},
    using Viterbi algorithm to compute the most likely trajectory with respect to query $(p_s, p_e, L)$.

\textsc{MarkovPath}: the same as \textsc{Markov}, but with extra restriction that no sub-tours exist in the 
    recommended trajectory.

\textsc{Rank+Markov}: a heuristic algorithm to combine POI ranking and the factorized transition matrix 
    to recommend the most likely trajectory with respect to query $(p_s, p_e, L)$.

\textsc{Rank+MarkovPath}: the same as \textsc{Rank+Markov}, but with extra restriction that no sub-tours exist in 
    the recommended trajectory.

\textsc{StructuredSVM}: structured prediction algorithm using POI rankings as well as transition probabilities
    between POIs to recommend trajectory as described in section \ref{ssvm}.

% Parameters for IJCAI methods
The trade-off parameter for both \textsc{PersTour} and \textsc{PersTour-L} were $0.5$.
% Parameters for rankSVM
The regularization parameter of rankSVM was $10$ for all algorithms that utilising POI rankings, namely, 
\textsc{PoiRank}, \textsc{Rank+Markov} and \textsc{Rank+MarkovPath}.
% Parameters for transition matrix
When computing the transition probabilities between POIs,
POI popularity, the total number of visit at POI and the average visit duration at POI were discritized using
$5$ bins uniformly in log-space, furthermore, POIs were grouped into $5$ clusters using K-means according to
their geographical locations.
% Parameters for Heuristics
The trade-off parameters $\alpha$ was set to $0.5$ in both \textsc{Rank+Markov} and \textsc{Rank+MarkovPath} algorithms.
% Parameters for SSVM
The regularization parameter to train \textsc{StructuredSVM} was $1$.


\subsection{Results}
% tell the story: justify each row in the table, make comparison.
The F$_1$-scores of the nine algorithms on all four datasets are shown in table \ref{table:f1}.
The information utilized by these algorithms except \textsc{Random} range from POI specific and query specific 
information to transition patterns between different POIs and so on, table \ref{table:character} summarize the 
characteristics of all the nine algorithms.

From table \ref{table:f1}, one can observe that all algorithms that capture POI specific information 
outperform the one that does not use it, namely the \textsc{Random} baseline algorithm, 
with one exception of \textsc{PersTour} on Melbourne dataset,
which indicates the POI specific information is very helpful for recommending trajectories.
%
% time constraint is better than length constraint
Another observation is \textsc{PersTour} get much better trajectory F$_1$-scores on four out of five datasets 
than \textsc{PersTour-L}, which is equivalent to \textsc{PersTour} except the total number of POIs was used to 
constrain recommendation, which means the total time consumed in a trajectory was more helpful than the number 
of POIs one should visit.
%
% comparison between ijcai and other methods
However, algorithms that did not utilise the total time consumed in a trajectory can outperform the one that 
used this information (i.e., \textsc{PersTour}) on four out of five datasets, 
by learning to rank POIs based on POI specific and query specific features and exploiting the transition patterns 
between POIs.

% strong performance of POI popularity based ranking on Edinburgh data
POI features seem to be very helpful for recommending trajectories.
In particular, recommendation based on ranking POI popularity only 
(i.e., \textsc{PoiPopularity}) yields very good performance,
especially on Edinburgh dataset, where it surprisingly outperforms all 
other algorithms, including those much more sophisticated than this simple ranking method.
%
% strong performance of POI feature based ranking on Toronto and Glasgow data
Furthermore, \textsc{PoiRank} performs very well in four out of five datasets, 
especially on Toronto and Melbourne datasets, where it got better results than any other methods, 
while on both Edinburgh and Glasgow datasets, it was the second best performer, 
which indicates that learning to rank is very helpful for recommending trajectories.

% the performance of transition-only methods
Recommendation by exploiting only transition patterns did not performs quite well,
but when supported by learning to rank, it improves a lot, which results the best performer on Osaka dataset 
and the second or third best performer on all other datasets, namely \textsc{Rank+MarkovPath}.

% sub-tours hurt
In addition, we found that sub-tours hurt trajectory recommendation when compare the performance between
algorithms with or without sub-tours restrictions, namely, \textsc{Markov} versus \textsc{MarkovPath} and 
\textsc{Rank+Markov} versue \textsc{Rank+MarkovPath}.
% 
% why structured prediction didn't work very well
This discovery could also be the reason that the sophisticated structured prediction algorithm, 
i.e., \textsc{StructuredSVM}, since it utilised the same features as both \textsc{Rank+Markov} and 
\textsc{Rank+MarkovPath}, but only got comparable or slightly better results than \textsc{Rank+Markov} 
and cannot outperform \textsc{Rank+MarkovPath} on four out of five datasets, and was only doing slightly 
better on Toronto dataset, even it took advantage of a significantly larger number of parameters.

% performance comparison in terms of tau
On the other hand, when compare the performance of different recommendation algorithms in terms of trajectory $\tau$
which try to measure the quality of visiting orders of POIs in recommended trajectories,
as shown in table \ref{table:tau}
\footnote{We can not compute trajectory $\tau$ for \textsc{PersTour} because the number of POIs in recommend trajectory 
is not guaranteed to equal the number of POIs in real trajectory.},
\textsc{StructuredSVM} became the best performer in three out of five datasets,
this is not unexpected as \textsc{StructuredSVM} can tune much more parameters than any other algorithms when training,
which means it can utilise the transition patterns between POIs better,
as a result, when POIs in a recommended trajectory also appear in the ground truth, 
there is a better chance that the visiting order among these POIs are also consistent with 
that in the real trajectory.

% 
The last interesting observation that we want to point out is the performance of ranking based algorithms
on Edinburgh, Glasgow and Toronto datasets, when measured in terms of both trajectory F$_1$ score and trajectory
$\tau$, \textsc{PoiPopularity} performed very well on Edinburgh dataset, and \textsc{PoiRank} yielded very good
performance on both Glasgow and Toronto datasets.
This observation indicates that most tourists visiting these three cities not only just visiting
places that are very popular, but also following similar visiting orders when visiting these places.

\begin{table*}
\centering
\begin{tabular}{l|ccccc} \hline
 & Edinburgh & Glasgow & Melbourne & Osaka & Toronto \\ \hline
\textsc{Random} & $0.570\pm0.139$ & $0.632\pm0.124$ & $0.558\pm0.149$ & $0.621\pm0.117$ & $0.621\pm0.128$ \\
\textsc{PersTour}\cite{ijcai15} & $0.656\pm0.223$ & $\mathbf{0.802\pm0.213}$ & $0.491\pm0.211$ & $0.702\pm0.230$ & $0.720\pm0.215$ \\
\textsc{PersTour-L} & $0.651\pm0.143$ & $0.660\pm0.102$ & $0.578\pm0.140$ & $0.691\pm0.138$ & $0.642\pm0.112$ \\
\textsc{PoiPopularity} & $\mathbf{0.701\pm0.160}$ & $0.745\pm0.166$ & $0.621\pm0.136$ & $0.661\pm0.128$ & $0.679\pm0.120$ \\
\textsc{PoiRank} & $\mathit{0.694\pm0.157}$ & $\mathit{0.777\pm0.171}$ & $\mathbf{0.626\pm0.137}$ & $0.679\pm0.112$ & $\mathbf{0.748\pm0.166}$ \\
\textsc{Markov} & $0.629\pm0.172$ & $0.714\pm0.168$ & $0.577\pm0.168$ & $0.679\pm0.162$ & $0.663\pm0.157$ \\
\textsc{MarkovPath} & $0.678\pm0.148$ & $0.731\pm0.167$ & $0.596\pm0.147$ & $0.706\pm0.154$ & $0.689\pm0.140$ \\
\textsc{Rank+Markov} & $0.642\pm0.171$ & $0.736\pm0.176$ & $0.598\pm0.169$ & $0.701\pm0.171$ & $0.689\pm0.170$ \\
\textsc{Rank+MarkovPath} & $0.684\pm0.151$ & $0.760\pm0.170$ & $\mathit{0.625\pm0.150}$ & $\mathbf{0.719\pm0.161}$ & $0.724\pm0.152$ \\
\textsc{StructuredSVM} & $0.659\pm0.186$ & $0.727\pm0.173$ & $0.597\pm0.171$ & $\mathit{0.715\pm0.170}$ & $\mathit{0.728\pm0.186}$ \\
\hline
\end{tabular}
\caption{Performance comparison on four datasets in terms of trajectory F$_1$-score. 
         For each dataset (i.e., a column), the best method is shown in bold, the second best is shown in italic.}
\label{table:f1}
\end{table*}


\begin{table*}
\centering
\begin{tabular}{l|ccccc} \hline
 & Edinburgh & Glasgow & Melbourne & Osaka & Toronto \\ \hline
\textsc{Random} & $0.259\pm0.155$ & $0.318\pm0.165$ & $0.248\pm0.148$ & $0.305\pm0.145$ & $0.309\pm0.166$ \\
\textsc{PersTour-L} & $0.350\pm0.206$ & $0.349\pm0.163$ & $0.267\pm0.144$ & $0.415\pm0.243$ & $0.329\pm0.158$ \\
\textsc{PoiPopularity} & $\mathit{0.421\pm0.257}$ & $0.503\pm0.296$ & $0.310\pm0.179$ & $0.361\pm0.194$ & $0.378\pm0.203$ \\
\textsc{PoiRank} & $0.410\pm0.246$ & $\mathbf{0.557\pm0.311}$ & $0.315\pm0.187$ & $0.367\pm0.162$ & $\mathit{0.501\pm0.294}$ \\
\textsc{Markov} & $0.404\pm0.229$ & $0.472\pm0.282$ & $0.296\pm0.191$ & $0.442\pm0.259$ & $0.406\pm0.231$ \\
\textsc{MarkovPath} & $0.390\pm0.235$ & $0.479\pm0.292$ & $0.293\pm0.187$ & $0.445\pm0.268$ & $0.401\pm0.235$ \\
\textsc{Rank+Markov} & $0.411\pm0.237$ & $0.522\pm0.293$ & $\mathbf{0.344\pm0.202}$ & $\mathit{0.475\pm0.278}$ & $0.449\pm0.263$ \\
\textsc{Rank+MarkovPath} & $0.395\pm0.238$ & $\mathit{0.523\pm0.303}$ & $0.327\pm0.214$ & $0.470\pm0.284$ & $0.455\pm0.268$ \\
\textsc{StructuredSVM} & $\mathbf{0.423\pm0.264}$ & $0.505\pm0.282$ & $\mathit{0.335\pm0.215}$ & $\mathbf{0.499\pm0.293}$ & $\mathbf{0.511\pm0.312}$ \\
\hline
\end{tabular}
\caption{Performance comparison on four datasets in terms of $\tau$.
         For each dataset (i.e., a column), the best method is shown in bold, the second best is shown in italic.}
\label{table:tau}
\end{table*}


\begin{table*}
\centering
\begin{tabular}{l|cccccc} \hline
                                    & Query    & POI      & Transition & No sub-tours & Joint    \\ \hline
\textsc{Random}                     & $\times$ & $\times$ & $\times$   & $\times$     & $\times$ \\ 
\textsc{PersTour}\cite{ijcai15}     & $\times$ & $\surd$  & $\times$   & $\surd$      & $\times$ \\
\textsc{PersTour-L}                 & $\times$ & $\surd$  & $\times$   & $\surd$      & $\times$ \\
\textsc{PoiPopularity}              & $\times$ & $\surd$  & $\times$   & $\times$     & $\times$ \\ 
\textsc{PoiRank}                    & $\surd$  & $\surd$  & $\times$   & $\times$     & $\times$ \\
\textsc{Markov}                     & $\times$ & $\surd$  & $\surd$    & $\times$     & $\times$ \\
\textsc{MarkovPath}                 & $\times$ & $\surd$  & $\surd$    & $\surd$      & $\times$ \\
\textsc{Rank} + \textsc{Markov}     & $\surd$  & $\surd$  & $\surd$    & $\times$     & $\times$ \\
\textsc{Rank} + \textsc{MarkovPath} & $\surd$  & $\surd$  & $\surd$    & $\surd$      & $\times$ \\
\textsc{StructuredSVM}              & $\surd$  & $\surd$  & $\surd$    & $\times$     & $\surd$  \\ \hline
\end{tabular}
\caption{Characteristics of different algorithms}
\label{table:character}
\end{table*}



\subsection{Avoid Peeking}
When working with machine learning algorithms, to make sure the reported performance is a good approximation
of the generalization performance, it is critical to prevent information in test set from leaking into
training set.
Many algorithms in the above comparison utilizing both learning to rank and factorized transition matrix,
e.g., \textsc{Rank+Markov}, \textsc{Rank+MarkovPath} and \textsc{StructuredSVM},
both of them need to be trained or parameters be estimated before being utilized in other algorithms.
Features such as popularity of a POI, the number of visits of a POI and the average visit duration at a POI are
determined by not only the POI itself but also trajectories in training set, let's call them aggregated features as they are 
computed by aggregating a set of trajectories.
To make sure the prediction performance is reliable, it is very important to exclude trajectories in test set 
when computing aggregated features.
Unfortunately, it is quite easy, especially when utilizing multiple levels of machine learning models,
to use all data, including those in test set, to compute aggregated features and many researchers and 
practitioners did not realize some bits of information in test set were leaked into training set via these aggregated features.

%One may argue that many of these features will not change much when computed with or without data in test set,
%but in certain areas, such as aerodynamics, some decisions are very sensitive to the quantity of certain features.
%Nevertheless, the exact impact still needs further investigation.


