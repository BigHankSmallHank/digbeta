{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active Trajectory Recommendation without Personalisation\n",
    "====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: Before running this notebook, please run script `src/ijcai15_setup.py` to setup data properly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "Transition Features\n",
    "1. POI category (a transition matrix between different categories)\n",
    "1. POI popularity (a transition matrix between different class of popularity)\n",
    "1. POI pair distance (a transition matrix between different class of distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommend Trajectories\n",
    "Given (start, end) and trajectory length $l$ for a specific user $u$, we'll recommend a trajectory to user $u$ as follows:\n",
    "- Compute/enumerate all trajectories of length $l$ with (start, end) as candidates\n",
    "- Use a uniform prior for all candidates\n",
    "- Compute the likelihood of candidates using the above transition features estimated from travelling sequences in the training set\n",
    "- Sort candidates by their posterior probabilities (i.e. $\\text{prior} \\times \\text{likelihood}$) in descending order\n",
    "- Recommend a trajectory from the top $K$ (e.g. 5) candidates with probability proportional to its posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** that we use the same leave-one-out cross validation approach described in the [ijcai15 paper](#https://sites.google.com/site/limkwanhui/publications/2015-IJCAI-personalTour.pdf?attredirects=0), concretely, for each user $u$ in the dataset, choose one trajectory (length >= 3) from all trajectories of $u$ uniformly at random, this trajectory is used as the ground truth to measure the performance of the recommendation (i.e. compute the [precision, recall and F1-score](./ijcai15.ipynb#sec2.1)), all other trajectories are used to train/estimate parameters. So the training set will change when iterating through all users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "#from numba import jit\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nfeatures = 8 # number of features\n",
    "EPS = 1e-12 # smooth, deal with 0 probability\n",
    "random.seed(987654321) # control random choice when splitting training/testing set\n",
    "np.random.seed(987654321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/data-ijcai15'\n",
    "fvisit = os.path.join(data_dir, 'userVisits-Osak.csv')\n",
    "fcoord = os.path.join(data_dir, 'photoCoords-Osak.csv')\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Glas.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Glas.csv')\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Edin.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Edin.csv')\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Toro.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Toro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "suffix = fvisit.split('-')[-1].split('.')[0]\n",
    "fseqpart = os.path.join(data_dir, 'seqPart-' + suffix + '.pkl')\n",
    "frand = os.path.join(data_dir, 'F1-rand-' + suffix + '.pkl')\n",
    "flc = os.path.join(data_dir, 'F1-lc-' + suffix + '.pkl')\n",
    "fse = os.path.join(data_dir, 'F1-se-' + suffix + '.pkl')\n",
    "fid = os.path.join(data_dir, 'F1-id-' + suffix + '.pkl')\n",
    "forand = os.path.join(data_dir, 'seq-rand-' + suffix + '.pkl')\n",
    "folc = os.path.join(data_dir, 'seq-lc-' + suffix + '.pkl')\n",
    "fose = os.path.join(data_dir, 'seq-se-' + suffix + '.pkl')\n",
    "foid = os.path.join(data_dir, 'seq-id-' + suffix + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visits = pd.read_csv(fvisit, sep=';')\n",
    "coords = pd.read_csv(fcoord, sep=';')\n",
    "# merge data frames according to column 'photoID'\n",
    "assert(visits.shape[0] == coords.shape[0])\n",
    "traj = pd.merge(visits, coords, on='photoID')\n",
    "#traj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#photo</th>\n",
       "      <th>#photo/user</th>\n",
       "      <th>#poi</th>\n",
       "      <th>#seq</th>\n",
       "      <th>#seq/user</th>\n",
       "      <th>#user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Osak</th>\n",
       "      <td>7747</td>\n",
       "      <td>17.215556</td>\n",
       "      <td>27</td>\n",
       "      <td>1115</td>\n",
       "      <td>2.477778</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      #photo  #photo/user  #poi  #seq  #seq/user  #user\n",
       "Osak    7747    17.215556    27  1115   2.477778    450"
      ]
     },
     "execution_count": 1186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_photo = traj['photoID'].unique().shape[0]\n",
    "num_user = traj['userID'].unique().shape[0]\n",
    "num_poi = traj['poiID'].unique().shape[0]\n",
    "num_seq = traj['seqID'].unique().shape[0]\n",
    "pd.DataFrame({'#photo': num_photo, '#user': num_user, '#poi': num_poi, '#seq': num_seq, \\\n",
    "              '#photo/user': num_photo/num_user, '#seq/user': num_seq/num_user}, index=[str(suffix)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Compute POI Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute POI (Longitude, Latitude) as the average coordinates of the assigned photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_coords = traj[['poiID', 'photoLon', 'photoLat']].groupby('poiID').mean()\n",
    "poi_coords.reset_index(inplace=True)\n",
    "poi_coords.rename(columns={'photoLon':'poiLon', 'photoLat':'poiLat'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract POI category and visiting frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_catfreq = traj[['poiID', 'poiTheme', 'poiFreq']].groupby('poiID').first()\n",
    "poi_catfreq.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_all = pd.merge(poi_catfreq, poi_coords, on='poiID')\n",
    "poi_all.set_index('poiID', inplace=True)\n",
    "#poi_all.to_csv(fpoi, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Construct Travelling Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>seqID</th>\n",
       "      <th>poiID</th>\n",
       "      <th>arrivalTime</th>\n",
       "      <th>departureTime</th>\n",
       "      <th>#photo</th>\n",
       "      <th>poiDuration(sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10297518@N00</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1277719324</td>\n",
       "      <td>1277720832</td>\n",
       "      <td>6</td>\n",
       "      <td>1508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10307040@N08</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1382608644</td>\n",
       "      <td>1382608644</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10307040@N08</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1382607812</td>\n",
       "      <td>1382607812</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10307040@N08</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>1382607761</td>\n",
       "      <td>1382607774</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10307040@N08</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1382607879</td>\n",
       "      <td>1382608628</td>\n",
       "      <td>9</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         userID  seqID  poiID  arrivalTime  departureTime  #photo  \\\n",
       "0  10297518@N00      1     20   1277719324     1277720832       6   \n",
       "1  10307040@N08      2      6   1382608644     1382608644       1   \n",
       "2  10307040@N08      2      8   1382607812     1382607812       1   \n",
       "3  10307040@N08      2     21   1382607761     1382607774       2   \n",
       "4  10307040@N08      2     22   1382607879     1382608628       9   \n",
       "\n",
       "   poiDuration(sec)  \n",
       "0              1508  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                13  \n",
       "4               749  "
      ]
     },
     "execution_count": 1190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_all = traj[['userID', 'seqID', 'poiID', 'dateTaken']].copy().groupby(['userID', 'seqID', 'poiID'])\\\n",
    "          .agg([np.min, np.max, np.size])\n",
    "seq_all.columns = seq_all.columns.droplevel()\n",
    "seq_all.reset_index(inplace=True)\n",
    "seq_all.rename(columns={'amin':'arrivalTime', 'amax':'departureTime', 'size':'#photo'}, inplace=True)\n",
    "seq_all['poiDuration(sec)'] = seq_all['departureTime'] - seq_all['arrivalTime']\n",
    "seq_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_user = seq_all[['userID', 'seqID', 'poiID']].copy().groupby(['userID', 'seqID']).agg(np.size)\n",
    "seq_user.reset_index(inplace=True)\n",
    "seq_user.rename(columns={'poiID':'seqLen'}, inplace=True)\n",
    "seq_user.set_index('seqID', inplace=True)\n",
    "#seq_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_seq(seqid, seq_all):\n",
    "    seqi = seq_all[seq_all['seqID'] == seqid].copy()\n",
    "    seqi.sort(columns=['arrivalTime'], ascending=True, inplace=True)\n",
    "    return seqi['poiID'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compute Transition Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Basic Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\text{Pr}(\\text{Cat}_j ~\\vert~ \\text{Cat}_i)$:\n",
    "   the transition probability from a POI of category $\\text{Cat}_i$ to a POI of category $\\text{Cat}_j$\n",
    "- $\\text{Pr}(\\text{Pop}_j ~\\vert~ \\text{Pop}_i)$:\n",
    "   the transition probability from a POI of Popularity class $\\text{Pop}_i$ to a POI of Popularity class $\\text{Pop}_j$\n",
    "- $\\text{Pr}(\\text{Dist}_j ~\\vert~ \\text{Dist}_i)$:\n",
    "   the transition probability from a POI-POI pair with distance (between the two) class $\\text{Dist}_i$ to a POI-POI pair with distance (between the two) class $\\text{Dist}_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoothing by adding each cell a small number (i.e. $0.2$ times the minimum value of that row) if there exists a zero cell, adding a uniform count (i.e. 1) for all-zero rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth_obsmat(obsmat):\n",
    "    assert(isinstance(obsmat, pd.DataFrame))\n",
    "    for row in range(obsmat.index.shape[0]):\n",
    "        nonzeroidx = np.nonzero(obsmat.iloc[row])[0].tolist()\n",
    "        if len(nonzeroidx) == 0: # all zero in this row\n",
    "            obsmat.iloc[row] = 1  # smooth\n",
    "        elif len(nonzeroidx) < obsmat.columns.shape[0]: # some cells are zero\n",
    "            minv = np.min(obsmat.iloc[row, nonzeroidx])\n",
    "            EPS = 0.2 * minv  # row-specific smooth factor\n",
    "            obsmat.iloc[row] += EPS\n",
    "    return obsmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalise_transmat(transmat):\n",
    "    assert(isinstance(transmat, pd.DataFrame))\n",
    "    assert(np.all(transmat.dtypes == np.float))\n",
    "    for row in range(transmat.index.shape[0]):\n",
    "        rowsum = np.sum(transmat.iloc[row])\n",
    "        assert(rowsum > 0)\n",
    "        transmat.iloc[row] /= rowsum\n",
    "    return transmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute posterior: $\\log(\\text{posterior}) = \\log(\\text{likelihood}) + \\log(\\text{prior})$ and then normalise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_log_transmat(obs_log_transmat, prior_log_transmat):\n",
    "    assert(isinstance(obs_log_transmat, pd.DataFrame))\n",
    "    assert(isinstance(prior_log_transmat, pd.DataFrame))\n",
    "    assert(np.all(obs_log_transmat.get_values() < 0))\n",
    "    assert(np.all(prior_log_transmat.get_values() < 0))\n",
    "    assert(np.all(obs_log_transmat.index == prior_log_transmat.index))\n",
    "    assert(np.all(obs_log_transmat.columns == prior_log_transmat.columns))\n",
    "    log_transmat = obs_log_transmat + prior_log_transmat\n",
    "    for row in range(log_transmat.index.shape[0]):\n",
    "        logrowsum = scipy.misc.logsumexp(log_transmat.iloc[row]) # logsumexp(vec) = log(sum(exp(vec)))\n",
    "        log_transmat.iloc[row] -= logrowsum # normalisation\n",
    "    return log_transmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Transition Probabilities between POI Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model transition probabilities between POI categories, i.e.\n",
    "$\\text{Pr}(\\text{Cat}_{\\text{POI}_j} ~\\vert~ \\text{Cat}_{\\text{POI}_i})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the number of transition first, then normalise each row while taking care of zero by smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poi_cat_log_transmat1(seq, poi_info, poi_cats, seq_all, prior_log_transmat=None):\n",
    "    assert(len(poi_cats) > 0)\n",
    "    assert(len(seq) > 1)\n",
    "    \n",
    "    # observation\n",
    "    poi_cat_obsmat = pd.DataFrame(data=np.zeros((len(poi_cats), len(poi_cats)), dtype=np.float), \\\n",
    "                                  index=poi_cats, columns=poi_cats)\n",
    "    for j in range(len(seq)-1):\n",
    "        poi1 = seq[j]\n",
    "        poi2 = seq[j+1]\n",
    "        cat1 = poi_info.loc[poi1, 'poiTheme']\n",
    "        cat2 = poi_info.loc[poi2, 'poiTheme']\n",
    "        poi_cat_obsmat.loc[cat1, cat2] += 1\n",
    "    poi_cat_obsmat = smooth_obsmat(poi_cat_obsmat)\n",
    "    \n",
    "    if prior_log_transmat is None: \n",
    "        prior_log_transmat = pd.DataFrame(data=np.log((1/len(poi_cats)) * np.ones((len(poi_cats), len(poi_cats)), \\\n",
    "                                          dtype=np.float)), index=poi_cats, columns=poi_cats) # uniform prior\n",
    "        \n",
    "    return calc_log_transmat(np.log(normalise_transmat(poi_cat_obsmat)), prior_log_transmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poi_cat_log_transmat2(seqid_set, poi_info, poi_cats, seq_all):\n",
    "    assert(len(poi_cats) > 0)\n",
    "    \n",
    "    # observation\n",
    "    poi_cat_obsmat = pd.DataFrame(data=np.zeros((len(poi_cats), len(poi_cats)), dtype=np.float), \\\n",
    "                                  index=poi_cats, columns=poi_cats)\n",
    "    for seqid in seqid_set:\n",
    "        seq = extract_seq(seqid, seq_all)\n",
    "        for j in range(len(seq)-1):\n",
    "            poi1 = seq[j]\n",
    "            poi2 = seq[j+1]\n",
    "            cat1 = poi_info.loc[poi1, 'poiTheme']\n",
    "            cat2 = poi_info.loc[poi2, 'poiTheme']\n",
    "            poi_cat_obsmat.loc[cat1, cat2] += 1\n",
    "    poi_cat_obsmat = smooth_obsmat(poi_cat_obsmat)\n",
    "    \n",
    "    # uniform prior\n",
    "    prior_log_transmat = pd.DataFrame(data=np.log((1/len(poi_cats)) * np.ones((len(poi_cats), len(poi_cats)), \\\n",
    "                                      dtype=np.float)), index=poi_cats, columns=poi_cats)\n",
    "    \n",
    "    return calc_log_transmat(np.log(normalise_transmat(poi_cat_obsmat)), prior_log_transmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Transition Probabilities between POI Popularity Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model transition probabilities between POI popularities, i.e.\n",
    "$\\text{Pr}(\\text{Pop}_{\\text{POI}_j} ~\\vert~ \\text{Pop}_{\\text{POI}_i})$\n",
    "after discretizing POI popularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poi_info(seqid_set, poi_all, seq_all):\n",
    "    poi_info = seq_all[seq_all['seqID'].isin(seqid_set)][['poiID', 'seqID']].copy()\n",
    "    poi_info = poi_info.groupby('poiID').agg(np.size)\n",
    "    poi_info.rename(columns={'seqID':'popularity'}, inplace=True)\n",
    "    poi_info['poiTheme'] = poi_all.loc[poi_info.index, 'poiTheme']\n",
    "    poi_info['poiLon'] = poi_all.loc[poi_info.index, 'poiLon']\n",
    "    poi_info['poiLat'] = poi_all.loc[poi_info.index, 'poiLat']  \n",
    "    return poi_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Compute Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poi_pop_log_transmat1(seq, poi_info, bins_pop, seq_all, prior_log_transmat=None):\n",
    "    assert(len(seq) > 1)\n",
    "    pop_class = np.arange(1, len(bins_pop))\n",
    "    \n",
    "    # observation\n",
    "    poi_pop_obsmat = pd.DataFrame(data=np.zeros((len(pop_class), len(pop_class)), dtype=np.float), \\\n",
    "                                  index=pop_class, columns=pop_class)\n",
    "    for j in range(len(seq)-1):\n",
    "        poi1 = seq[j]\n",
    "        poi2 = seq[j+1]\n",
    "        pop1 = poi_info.loc[poi1, 'popularity']\n",
    "        pop2 = poi_info.loc[poi2, 'popularity']\n",
    "        pc1 = np.digitize([pop1], bins_pop)[0]\n",
    "        pc2 = np.digitize([pop2], bins_pop)[0]\n",
    "        poi_pop_obsmat.loc[pc1, pc2] += 1\n",
    "    poi_pop_obsmat = smooth_obsmat(poi_pop_obsmat)\n",
    "    \n",
    "    if prior_log_transmat is None:\n",
    "        prior_log_transmat = pd.DataFrame(data=np.log((1/len(pop_class)) * np.ones((len(pop_class), len(pop_class)), \\\n",
    "                                          dtype=np.float)), index=pop_class, columns=pop_class) # uniform prior\n",
    "    \n",
    "    return calc_log_transmat(np.log(normalise_transmat(poi_pop_obsmat)), prior_log_transmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poi_pop_log_transmat2(seqid_set, poi_info, bins_pop, seq_all):\n",
    "    pop_class = np.arange(1, len(bins_pop))\n",
    "    \n",
    "    # observation\n",
    "    poi_pop_obsmat = pd.DataFrame(data=np.zeros((len(pop_class), len(pop_class)), dtype=np.float), \\\n",
    "                                  index=pop_class, columns=pop_class)\n",
    "    for seqid in seqid_set:\n",
    "        seq = extract_seq(seqid, seq_all)\n",
    "        for j in range(len(seq)-1):\n",
    "            poi1 = seq[j]\n",
    "            poi2 = seq[j+1]\n",
    "            pop1 = poi_info.loc[poi1, 'popularity']\n",
    "            pop2 = poi_info.loc[poi2, 'popularity']\n",
    "            pc1 = np.digitize([pop1], bins_pop)[0]\n",
    "            pc2 = np.digitize([pop2], bins_pop)[0]\n",
    "            poi_pop_obsmat.loc[pc1, pc2] += 1\n",
    "    poi_pop_obsmat = smooth_obsmat(poi_pop_obsmat)\n",
    "    \n",
    "    # uniform prior\n",
    "    prior_log_transmat = pd.DataFrame(data=np.log((1/len(pop_class)) * np.ones((len(pop_class), len(pop_class)), \\\n",
    "                                      dtype=np.float)), index=pop_class, columns=pop_class)\n",
    "    \n",
    "    return calc_log_transmat(np.log(normalise_transmat(poi_pop_obsmat)), prior_log_transmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3.4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Transition Probabilities between POI Pair Distance Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model transition probabilities between different POI pair distances, i.e.\n",
    "$\\text{Pr}(\\text{Dist}_{\\text{POI}_{i} \\to \\text{POI}_{i+1}} ~\\vert~ \\text{Dist}_{\\text{POI}_{i-1} \\to \\text{POI}_i})$\n",
    "after discretize POI pair distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Improve the distance calculation using Google maps [distance API](https://developers.google.com/maps/documentation/distance-matrix/intro) with different [travel modes](https://developers.google.com/maps/documentation/distance-matrix/intro#travel_modes) demonstrated [here](https://jakevdp.github.io/blog/2015/10/17/analyzing-pronto-cycleshare-data-with-python-and-pandas/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Compute Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_poipair_dist_log_transmat1(seq, bins_dist, poi_distmat, seq_all, prior_log_transmat=None):\n",
    "    assert(len(seq) > 1)\n",
    "    dist_class = np.arange(1, len(bins_dist))\n",
    "    \n",
    "    # observation\n",
    "    poipair_dist_obsmat = pd.DataFrame(data=np.zeros((len(dist_class), len(dist_class)), dtype=np.float), \\\n",
    "                                       index=dist_class, columns=dist_class)\n",
    "    if len(seq) >= 3:\n",
    "        for j in range(1, len(seq)-1):\n",
    "            poi1 = seq[j-1]\n",
    "            poi2 = seq[j]\n",
    "            poi3 = seq[j+1]\n",
    "            dist12 = poi_distmat.loc[poi1, poi2]\n",
    "            dist23 = poi_distmat.loc[poi2, poi3]\n",
    "            dc12 = np.digitize([dist12], bins_dist)[0]\n",
    "            dc23 = np.digitize([dist23], bins_dist)[0]\n",
    "            poipair_dist_obsmat.loc[dc12, dc23] += 1\n",
    "    poipair_dist_obsmat = smooth_obsmat(poipair_dist_obsmat)\n",
    "    \n",
    "    if prior_log_transmat is None:\n",
    "        prior_log_transmat = pd.DataFrame(data=np.log((1/len(dist_class))*np.ones((len(dist_class), len(dist_class)), \\\n",
    "                                          dtype=np.float)), index=dist_class, columns=dist_class) # uniform prior\n",
    "        \n",
    "    return calc_log_transmat(np.log(normalise_transmat(poipair_dist_obsmat)), prior_log_transmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_poipair_dist_log_transmat2(seqid_set, bins_dist, poi_distmat, seq_all):\n",
    "    dist_class = np.arange(1, len(bins_dist))\n",
    "    \n",
    "    # observation\n",
    "    poipair_dist_obsmat = pd.DataFrame(data=np.zeros((len(dist_class), len(dist_class)), dtype=np.float), \\\n",
    "                                       index=dist_class, columns=dist_class)\n",
    "    for seqid in seqid_set:\n",
    "        seq = extract_seq(seqid, seq_all)\n",
    "        if len(seq) < 3: continue\n",
    "        for j in range(1, len(seq)-1):\n",
    "            poi1 = seq[j-1]\n",
    "            poi2 = seq[j]\n",
    "            poi3 = seq[j+1]\n",
    "            dist12 = poi_distmat.loc[poi1, poi2]\n",
    "            dist23 = poi_distmat.loc[poi2, poi3]\n",
    "            dc12 = np.digitize([dist12], bins_dist)[0]\n",
    "            dc23 = np.digitize([dist23], bins_dist)[0]\n",
    "            poipair_dist_obsmat.loc[dc12, dc23] += 1\n",
    "    poipair_dist_obsmat = smooth_obsmat(poipair_dist_obsmat)\n",
    "    \n",
    "    # uniform prior\n",
    "    prior_log_transmat = pd.DataFrame(data=np.log((1/len(dist_class))*np.ones((len(dist_class), len(dist_class)), \\\n",
    "                                      dtype=np.float)), index=dist_class, columns=dist_class)\n",
    "            \n",
    "    return calc_log_transmat(np.log(normalise_transmat(poipair_dist_obsmat)), prior_log_transmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compute Trajectory Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log likelihood of trajectory $[\\text{POI}_1, \\text{POI}_2, \\dots, \\text{POI}_i, ..., \\text{POI}_N]$ is defined as\n",
    "\\begin{align}\n",
    "\\text{loglikelihood} = &~\n",
    "w_1 \\sum_{i=1}^{N-1} \\log(\\text{Pr}(\\text{Cat}_{\\text{POI}_{i+1}} ~\\vert~ \\text{Cat}_{\\text{POI}_i})) + \n",
    "w_2 \\sum_{i=1}^{N-1} \\log(\\text{Pr}(\\text{Pop}_{\\text{POI}_{i+1}} ~\\vert~ \\text{Pop}_{\\text{POI}_i})) + \\\\\n",
    "&~ w_3 \\sum_{i=2}^{N-1} \\log(\\text{Pr}(\\text{Dist}_{\\text{POI}_i \\to \\text{POI}_{i+1}} ~\\vert~ \n",
    "\\text{Dist}_{\\text{POI}_{i-1} \\to \\text{POI}_i}))\n",
    "+ \\log(\\text{Pr}(\\text{POI}_1)) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where weights $\\sum_{i=1}^3 w_i = 1$ and $\\text{Pr}(\\text{POI}_1)$ is the prior of $\\text{POI}_1$, \n",
    "we assume $\\text{Pr}(\\text{POI}_1)=1.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_seq_loglikelihood(seq, weights, poi_all, poi_info, bins_pop, bins_dist, poi_distmat, \\\n",
    "                           poi_cat_log_transmat, poi_pop_log_transmat, poipair_dist_log_transmat):\n",
    "    assert(len(seq) > 1)\n",
    "    assert(len(weights) == 3)\n",
    "    assert(abs(1 - sum(weights)) < 1e-9) # sum(weights) == 1\n",
    "    \n",
    "    cat1 = poi_all.loc[seq[0], 'poiTheme']\n",
    "    cat2 = poi_all.loc[seq[1], 'poiTheme']\n",
    "    logL = weights[0] * poi_cat_log_transmat.loc[cat1, cat2]\n",
    "    \n",
    "    pop1 = 1; pop2 = 1  # just this very observation if not in poi_info\n",
    "    if seq[0] in poi_info.index:\n",
    "        pop1 = poi_info.loc[seq[0], 'popularity']\n",
    "    if seq[1] in poi_info.index:\n",
    "        pop2 = poi_info.loc[seq[1], 'popularity']\n",
    "    pc1 = np.digitize([pop1], bins_pop)[0]\n",
    "    pc2 = np.digitize([pop2], bins_pop)[0]\n",
    "    logL += weights[1] * poi_pop_log_transmat.loc[pc1, pc2]\n",
    "    \n",
    "    for j in range(1, len(seq)-1):\n",
    "        poi1 = seq[j-1]\n",
    "        poi2 = seq[j]\n",
    "        poi3 = seq[j+1]\n",
    "        cat2 = poi_all.loc[poi2, 'poiTheme']\n",
    "        cat3 = poi_all.loc[poi3, 'poiTheme']\n",
    "        logL += weights[0] * poi_cat_log_transmat.loc[cat2, cat3]\n",
    "        \n",
    "        pop2 = 1; pop3 = 1\n",
    "        if poi2 in poi_info.index:\n",
    "            pop2 = poi_info.loc[poi2, 'popularity']\n",
    "        if poi3 in poi_info.index:\n",
    "            pop3 = poi_info.loc[poi3, 'popularity']\n",
    "        pc2 = np.digitize([pop2], bins_pop)[0]\n",
    "        pc3 = np.digitize([pop3], bins_pop)[0]\n",
    "        logL += weights[1] * poi_pop_log_transmat.loc[pc2, pc3]\n",
    "        \n",
    "        dist12 = poi_distmat.loc[poi1, poi2]\n",
    "        dist23 = poi_distmat.loc[poi2, poi3]\n",
    "        dc12 = np.digitize([dist12], bins_dist)[0]\n",
    "        dc23 = np.digitize([dist23], bins_dist)[0]\n",
    "        logL += weights[2] * poipair_dist_log_transmat.loc[dc12, dc23]\n",
    "    return logL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Trajectory Recommendation & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter estimation of the Markov Chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_parameter1(seqid_set_training, poi_cats, bins_pop, bins_dist, poi_distmat, poi_all, seq_all, \\\n",
    "                        poi_cat_log_prior=None, poi_pop_log_prior=None, poipair_dist_log_prior=None):\n",
    "    # compute POI info using training set\n",
    "    poi_info = calc_poi_info(seqid_set_training, poi_all, seq_all)\n",
    "    \n",
    "    # the last sequence is the result of current querying\n",
    "    seqid = seqid_set_training[-1]\n",
    "    seq = extract_seq(seqid, seq_all)\n",
    "    \n",
    "    # compute POI category transition matrix\n",
    "    poi_cat_log_transmat = calc_poi_cat_log_transmat1(seq, poi_info, poi_cats, seq_all, poi_cat_log_prior)\n",
    "    \n",
    "    # compute POI popularity transition matrix\n",
    "    poi_pop_log_transmat = calc_poi_pop_log_transmat1(seq, poi_info, bins_pop, seq_all, poi_pop_log_prior)\n",
    "    \n",
    "    # compute POI pair distance transition matrix\n",
    "    poipair_dist_log_transmat = calc_poipair_dist_log_transmat1(seq, bins_dist, poi_distmat, seq_all, \\\n",
    "                                                                poipair_dist_log_prior)\n",
    "    \n",
    "    return (poi_info, poi_cat_log_transmat, poi_pop_log_transmat, poipair_dist_log_transmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_parameter2(seqid_set_training, poi_cats, bins_pop, bins_dist, poi_distmat, poi_all, seq_all):\n",
    "    # compute POI info using training set\n",
    "    poi_info = calc_poi_info(seqid_set_training, poi_all, seq_all)\n",
    "    \n",
    "    # compute POI category transition matrix\n",
    "    poi_cat_log_transmat = calc_poi_cat_log_transmat2(seqid_set_training, poi_info, poi_cats, seq_all)\n",
    "    \n",
    "    # compute POI popularity transition matrix\n",
    "    poi_pop_log_transmat = calc_poi_pop_log_transmat2(seqid_set_training, poi_info, bins_pop, seq_all)\n",
    "    \n",
    "    # compute POI pair distance transition matrix\n",
    "    poipair_dist_log_transmat = calc_poipair_dist_log_transmat2(seqid_set_training, bins_dist, poi_distmat, seq_all)\n",
    "    \n",
    "    return (poi_info, poi_cat_log_transmat, poi_pop_log_transmat, poipair_dist_log_transmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn weights by optimising an cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enumerate trajectories of the same (start, end) and length (3, 4 or 5) with respect to an actual sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enum_seq345(start, end, length, poi_list): \n",
    "    assert(length in {3, 4, 5})\n",
    "    p0 = start\n",
    "    pN = end\n",
    "    \n",
    "    # enumerate sequences with length 3\n",
    "    if length == 3:\n",
    "        return [[p0, p, pN] \\\n",
    "                for p in poi_list if p not in {p0, pN}]\n",
    "    \n",
    "    # enumerate sequences with length 4\n",
    "    if length == 4:\n",
    "        return [[p0, p1, p2, pN] \\\n",
    "                for p1 in poi_list if p1 not in {p0, pN} \\\n",
    "                for p2 in poi_list if p2 not in {p0, p1, pN}]\n",
    "    \n",
    "    # enumerate sequences with length 5\n",
    "    if length == 5:\n",
    "        return [[p0, p1, p2, p3, pN] \\\n",
    "                for p1 in poi_list if p1 not in {p0, pN} \\\n",
    "                for p2 in poi_list if p2 not in {p0, p1, pN} \\\n",
    "                for p3 in poi_list if p3 not in {p0, p1, p2, pN}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_seqstr(seqstr):\n",
    "    term = re.sub('[ \\[\\]]', '', seqstr).split(',')\n",
    "    return [int(x) for x in term]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With uniform prior, the posterior probability of trajectory is proportional to its likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend_trajectory(start, end, length, poi_list, poi_info, bins_pop, bins_dist, poi_distmat, \\\n",
    "                         poi_cat_log_transmat, poi_pop_log_transmat, poipair_dist_log_transmat):    \n",
    "    # enumerate trajectory\n",
    "    enum_seqs = enum_seq345(start, end, length, poi_list)\n",
    "    \n",
    "    # compute log likelihood   \n",
    "    logL = []\n",
    "    for seq in enum_seqs:\n",
    "        logl = calc_seq_loglikelihood(seq, poi_all, poi_info, bins_pop, bins_dist, poi_distmat, \\\n",
    "                                      poi_cat_log_transmat, poi_pop_log_transmat, poipair_dist_log_transmat)\n",
    "        logL.append(logl)\n",
    "    \n",
    "    # sanity check: passed\n",
    "    seq1 = [10, 21, 28, 22]\n",
    "    pops = []\n",
    "    for i in range(len(seq1)):\n",
    "        if seq1[i] in poi_info.index: pops.append(poi_info.loc[seq1[i], 'popularity'])\n",
    "        else: pops.append(1)\n",
    "    cats = [poi_all.loc[x, 'poiTheme'] for x in seq1] # use the ugly global variable\n",
    "    dc12 = np.digitize([poi_distmat.loc[seq1[0], seq1[1]]], bins_dist)[0]\n",
    "    dc23 = np.digitize([poi_distmat.loc[seq1[1], seq1[2]]], bins_dist)[0]\n",
    "    dc34 = np.digitize([poi_distmat.loc[seq1[2], seq1[3]]], bins_dist)[0]\n",
    "    s1 = poi_cat_log_transmat.loc[cats[0], cats[1]] + poi_cat_log_transmat.loc[cats[1], cats[2]] + \\\n",
    "         poi_cat_log_transmat.loc[cats[2], cats[3]] + \\\n",
    "         poi_pop_log_transmat.loc[np.digitize([pops[0]], bins_pop)[0], np.digitize([pops[1]], bins_pop)[0]] + \\\n",
    "         poi_pop_log_transmat.loc[np.digitize([pops[1]], bins_pop)[0], np.digitize([pops[2]], bins_pop)[0]] + \\\n",
    "         poi_pop_log_transmat.loc[np.digitize([pops[2]], bins_pop)[0], np.digitize([pops[3]], bins_pop)[0]]\n",
    "    s2 = poipair_dist_log_transmat.loc[dc12, dc23] + poipair_dist_log_transmat.loc[dc23, dc34]\n",
    "    logL1 = calc_seq_loglikelihood(seq1, poi_all, poi_info, bins_pop, bins_dist, poi_distmat, \\\n",
    "                                   poi_cat_log_transmat, poi_pop_log_transmat, poipair_dist_log_transmat)\n",
    "    print(s1+s2, '-', logL1, '=', s1+s2-logL1); sys.stdout.flush()\n",
    "    \n",
    "    # dataframe of enumerated trajectories\n",
    "    enum_seq_df = pd.DataFrame(data=np.array(logL), index=[str(x) for x in enum_seqs], columns=['logLikelihood'])\n",
    "    \n",
    "    # sort by loglikeilihood in descending order\n",
    "    enum_seq_df.sort(columns=['logLikelihood'], ascending=False, inplace=True)\n",
    "    \n",
    "    # recommend trajectory\n",
    "    probs = enum_seq_df['logLikelihood'].get_values(); #print(probs)\n",
    "    probs = np.exp(probs)\n",
    "    probs /= np.sum(probs); #print(probs)\n",
    "    #enum_seq_df['probability'] = probs\n",
    "    \n",
    "    # recommend a trajectory with probability proportional to its posterior (i.e. likelihood when prior is uniform)\n",
    "    #sample = np.random.multinomial(1, probs) # catgorical/multinoulli distribution, multinomial distribution (n=1)\n",
    "    #idx = sample.nonzero()[0][0]\n",
    "    #return (parse_seqstr(enum_seq_df.index[idx]), probs[idx]) # return the recommended sequence and its probability\n",
    "    \n",
    "    # recommend the topk trajectories with their probabilities\n",
    "    k = 10\n",
    "    if enum_seq_df.shape[0] < k: \n",
    "        k = enum_seq_df.shape[0]\n",
    "    result_df = enum_seq_df.iloc[0:k].copy()\n",
    "    result_df.reset_index(inplace=True)\n",
    "    result_df.rename(columns={'index':'sequence'}, inplace=True)\n",
    "    result_df.drop('logLikelihood', axis=1, inplace=True)\n",
    "    result_df['probability'] = probs[0:k]\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use F1 measure defined [here](./ijcai15.ipynb#sec2.1) to evaluate the performance of recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_F1score(seq_act, seq_rec):\n",
    "    assert(len(seq_act) > 0)\n",
    "    assert(len(seq_rec) > 0)\n",
    "    actset = set(seq_act)\n",
    "    recset = set(seq_rec)\n",
    "    intersect = actset & recset\n",
    "    recall = len(intersect) / len(seq_act)\n",
    "    precision = len(intersect) / len(seq_rec)\n",
    "    F1score = 2. * precision * recall / (precision + recall)\n",
    "    return F1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_parallel(seqid_set, poi_list, poi_info, seq_all, bins_pop, bins_dist, poi_distmat, \\\n",
    "                      poi_cat_log_transmat, poi_pop_log_transmat, poipair_dist_log_transmat):\n",
    "    seq_act_list = [extract_seq(seqid, seq_all) for seqid in seqid_set]\n",
    "    seq_rec_df_list = Parallel(n_jobs=-2)(delayed(recommend_trajectory)\\\n",
    "                      (seq[0], seq[-1], len(seq), poi_list, poi_info, bins_pop, bins_dist, poi_distmat, \\\n",
    "                       poi_cat_log_transmat, poi_pop_log_transmat, poipair_dist_log_transmat) \\\n",
    "                      for seq in seq_act_list)    \n",
    "    seq_rec_list = [parse_seqstr(seq_rec_df.iloc[0]['sequence']) for seq_rec_df in seq_rec_df_list]\n",
    "    assert(len(seq_act_list) == len(seq_rec_list))\n",
    "    #F1scores = [calc_F1score(seq_act_list[x], seq_rec_list[x]) for x in range(len(seq_act_list))] # OK\n",
    "    F1scores = Parallel(n_jobs=-2)(delayed(calc_F1score)\\\n",
    "               (seq_act_list[x], seq_rec_list[x]) for x in range(len(seq_act_list))) # OK\n",
    "                                   \n",
    "    return F1scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Active Learning without Personalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $Traj$: a trajectory\n",
    "- $\\lvert Traj \\rvert$: the number of POIs in trajectory $Traj$\n",
    "- $P_s$: the `start` (first) POI of a trajectory\n",
    "- $P_e$: the `end` (last) POI of a trajectory\n",
    "- $u$: a specific user\n",
    "- $\\textbf{x}$: an example\n",
    "- $\\textbf{y}$: label of example $\\textbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example: $\\textbf{x} = (P_s, P_e, \\lvert Traj \\rvert)$\n",
    "- Label of Example: $\\textbf{y} = Traj$\n",
    "- Assume that all examples are i.i.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two Approaches**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach ONE** \n",
    "1. At each query step, setting the `prior` as the `posterior` of the previous step \n",
    "(using an uniform `prior` at the first step), \n",
    "1. compute the `likelihood` using the trajectory gotten by current querying, \n",
    "1. multiplying the `likelihood` and the `prior` to get the `posterior`, \n",
    "1. using maximum a posteriori (MAP) to recommend trajectory and evaluate the performance on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach TWO**\n",
    "1. At each query step, using all trajectories (including the one get by current querying) \n",
    "in current training set to compute the `likelihood`, \n",
    "1. multiplying the `likelihood` and an uniform `prior` to get the `posterior`, \n",
    "1. using maximum a posteriori (MAP) to recommend trajectory and evaluate the performance on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** that the above `prior`, `likelihood` and `posterior` are corresponding to the parameters of the probabilistic model, i.e. the transition probabilities of three Markov Chains (POI category, POI popularity, POI pair distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** : are they equivalent?  \n",
    "Generally **NO**.   \n",
    "By compute a toy example, it is easy to show that the first approach is a `count -> normalise -> multiply -> normalise` procedure, while the second approach is a `count -> sum -> normalise` procedure\n",
    "(both will multiply an uniform prior at the very beginning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>ONE</td>\n",
    "<td> count &rarr; </td>\n",
    "<td><table><tr><td>1</td><td>2</td></tr><tr><td>2</td><td>1</td></tr></table></td>\n",
    "<td><table><tr><td>1</td><td>3</td></tr><tr><td>3</td><td>1</td></tr></table></td>\n",
    "<td><table><tr><td>1</td><td>4</td></tr><tr><td>4</td><td>1</td></tr></table></td>\n",
    "<td> normalise &rarr; </td>\n",
    "<td><table><tr><td>1/3</td><td>2/3</td></tr><tr><td>2/3</td><td>1/3</td></tr></table></td>\n",
    "<td><table><tr><td>1/4</td><td>3/4</td></tr><tr><td>3/4</td><td>1/4</td></tr></table></td>\n",
    "<td><table><tr><td>1/5</td><td>4/5</td></tr><tr><td>4/5</td><td>1/5</td></tr></table></td>\n",
    "<td> multiply &rarr; </td>\n",
    "<td><table><tr><td>1/60</td><td>24/60</td></tr><tr><td>24/60</td><td>1/60</td></tr></table></td>\n",
    "<td> normalise &rarr; </td>\n",
    "<td><table><tr><td>1/25</td><td>24/25</td></tr><tr><td>24/25</td><td>1/25</td></tr></table></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TWO</td>\n",
    "<td> count &rarr; </td>\n",
    "<td><table><tr><td>1</td><td>2</td></tr><tr><td>2</td><td>1</td></tr></table></td>\n",
    "<td><table><tr><td>1</td><td>3</td></tr><tr><td>3</td><td>1</td></tr></table></td>\n",
    "<td><table><tr><td>1</td><td>4</td></tr><tr><td>4</td><td>1</td></tr></table></td>\n",
    "<td> sum &rarr; </td>\n",
    "<td><table><tr><td>3</td><td>9</td></tr><tr><td>9</td><td>3</td></tr></table></td>\n",
    "<td> normalise &rarr; </td>\n",
    "<td><table><tr><td>1/4</td><td>3/4</td></tr><tr><td>3/4</td><td>1/4</td></tr></table></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Travelling Sequences Dataset Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset (travelling sequences with length 3/4/5) into \n",
    "- training set (i.e. initial training) ~~(10%)~~, start from empty training set,\n",
    "- annotation set (i.e. simulate user annotation) (80%), \n",
    "- evaluation set (a.k.a. test set) ~~(40%)~~ (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115\n"
     ]
    }
   ],
   "source": [
    "seq_stats = seq_all[['userID', 'seqID', 'poiID']].copy().groupby(['userID', 'seqID']).agg(np.size)\n",
    "seq_stats.reset_index(inplace=True)\n",
    "seq_stats.rename(columns={'poiID':'seqLen'}, inplace=True)\n",
    "seq_stats.set_index('seqID', inplace=True)\n",
    "print(seq_stats.index.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "seqid_set_exp = seq_stats[seq_stats['seqLen'].isin({3, 4, 5})].index\n",
    "print(len(seqid_set_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a random portion of sequences as training data that span all POIs in annotation/evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_annotate0 = []\n",
    "seqid_set_evaluate0 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doCompute = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(fseqpart):\n",
    "    doCompute = False\n",
    "    (seqid_set_annotate0, seqid_set_evaluate0) = pickle.load(open(fseqpart, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doCompute:\n",
    "    idx_range = np.arange(len(seqid_set_exp))\n",
    "    num_annotate = round(len(seqid_set_exp) * 0.8)\n",
    "    np.random.shuffle(idx_range) \n",
    "    seqid_set_annotate0 = list(seqid_set_exp[idx_range[0:num_annotate]])\n",
    "    seqid_set_evaluate0 = list(seqid_set_exp[idx_range[num_annotate:]])\n",
    "    \n",
    "    pickle.dump((seqid_set_annotate0, seqid_set_evaluate0), open(fseqpart, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "37\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(seqid_set_exp))\n",
    "print(len(seqid_set_annotate0))\n",
    "print(len(seqid_set_evaluate0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Compute all states of Markov Chain using all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute POI information using all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_info0 = calc_poi_info(seqid_set_exp, poi_all, seq_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5, 6, 7, 8, 10, 11, 12, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 28]"
      ]
     },
     "execution_count": 1215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_list = poi_info0.index.tolist()\n",
    "poi_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amusement', 'Entertainment', 'Historical', 'Park']"
      ]
     },
     "execution_count": 1216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_cats = poi_info0['poiTheme'].unique().tolist()\n",
    "poi_cats.sort()\n",
    "poi_cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI popularity classes (i.e. discretization), discretization method is choosen according to [this experiment](./traj_feature.ipynb#sec3.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins_pop = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbins_pop = 5\n",
    "quantiles_pop = np.round(np.linspace(0, 1, nbins_pop+1), 2)[1:-1]\n",
    "bins_pop.append(0)\n",
    "bins_pop.extend(poi_info0['popularity'].quantile(quantiles_pop))\n",
    "bins_pop.append(poi_info0['popularity'].max() + 1)\n",
    "bins_pop = np.round(bins_pop, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   1.,   4.,   7.,  13.,  23.])"
      ]
     },
     "execution_count": 1219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI pair distance classes (i.e. discretization), discretization method is choosen according to [this experiment](./traj_feature.ipynb#sec3.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_distmat = pd.DataFrame(data=np.zeros((poi_info0.shape[0], poi_info0.shape[0]), dtype=np.float), \\\n",
    "                           index=poi_info0.index, columns=poi_info0.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_dist(longitude1, latitude1, longitude2, latitude2):\n",
    "    \"\"\"Calculate the distance (unit: km) between two places on earth\"\"\"\n",
    "    # convert degrees to radians\n",
    "    lon1 = math.radians(longitude1)\n",
    "    lat1 = math.radians(latitude1)\n",
    "    lon2 = math.radians(longitude2)\n",
    "    lat2 = math.radians(latitude2)\n",
    "    radius = 6371.009 # mean earth radius is 6371.009km, en.wikipedia.org/wiki/Earth_radius#Mean_radius\n",
    "    # The haversine formula, en.wikipedia.org/wiki/Great-circle_distance\n",
    "    dlon = math.fabs(lon1 - lon2)\n",
    "    dlat = math.fabs(lat1 - lat2)\n",
    "    return 2 * radius * math.asin( math.sqrt( \\\n",
    "               (math.sin(0.5*dlat))**2 + math.cos(lat1) * math.cos(lat2) * (math.sin(0.5*dlon))**2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(poi_info0.shape[0]):\n",
    "    for j in range(i+1, poi_info0.shape[0]):\n",
    "        poi1 = poi_info0.index[i]\n",
    "        poi2 = poi_info0.index[j]\n",
    "        dist = calc_dist(poi_info0.loc[poi1, 'poiLon'], poi_info0.loc[poi1, 'poiLat'], \\\n",
    "                         poi_info0.loc[poi2, 'poiLon'], poi_info0.loc[poi2, 'poiLat'])\n",
    "        poi_distmat.loc[poi1, poi2] = dist\n",
    "        poi_distmat.loc[poi2, poi1] = dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#poi_distmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins_dist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distdata = pd.Series([poi_distmat.iloc[x, y] for x in range(poi_info0.shape[0]) for y in range(x+1, poi_info0.shape[0])])\n",
    "nbins_dist = 5\n",
    "quantiles_dist = np.round(np.linspace(0, 1, nbins_dist+1), 2)[1:-1]\n",
    "bins_dist.append(0)\n",
    "bins_dist.extend(distdata.quantile(quantiles_dist))\n",
    "bins_dist.append(round(distdata.max()) + 1)\n",
    "bins_dist = np.round(bins_dist, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.  ,   2.29,   3.68,   5.49,   7.47,  14.  ])"
      ]
     },
     "execution_count": 1226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Random Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query strategy: choose a random example in annotation set to query (passive learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of annotation set and use the copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_training = []\n",
    "seqid_set_annotate = list(seqid_set_annotate0)\n",
    "seqid_set_evaluate = list(seqid_set_evaluate0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_order = np.arange(len(seqid_set_annotate))\n",
    "np.random.shuffle(query_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1_scores_rand = []\n",
    "seq_order_rand = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doCompute = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(frand) and os.path.exists(forand):\n",
    "    doCompute = False\n",
    "    F1_scores_rand = pickle.load(open(frand, 'rb'))\n",
    "    seq_order_rand = pickle.load(open(forand, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate parameters using the initial training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doCompute:\n",
    "    (poi_info, poi_cat_log_transmat, poi_pop_log_transmat, poipair_dist_log_transmat) = estimate_parameter(\\\n",
    "    seqid_set_training, poi_cats, bins_pop, bins_dist, poi_distmat, poi_all, seq_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5\n",
       "1 -1.609438 -1.609438 -1.609438 -1.609438 -1.609438\n",
       "2 -1.609438 -1.609438 -1.609438 -1.609438 -1.609438\n",
       "3 -1.609438 -1.609438 -1.609438 -1.609438 -1.609438\n",
       "4 -1.609438 -1.609438 -1.609438 -1.609438 -1.609438\n",
       "5 -1.609438 -1.609438 -1.609438 -1.609438 -1.609438"
      ]
     },
     "execution_count": 1236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#poi_info\n",
    "#poi_cat_log_transmat\n",
    "#poi_pop_log_transmat\n",
    "#poipair_dist_log_transmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: evaluate using the first example in test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12.2060726455 - -12.2060726455 = -1.7763568394e-15\n",
      "Actual: [21, 22, 3]              Recommended: [21, 1, 3]              F1: 0.67\n"
     ]
    }
   ],
   "source": [
    "if doCompute:\n",
    "    seqid = seqid_set_evaluate[0]\n",
    "    seq_act = extract_seq(seqid, seq_all)\n",
    "    seq_rec_df = recommend_trajectory(seq_act[0], seq_act[-1], len(seq_act), poi_list, poi_info, bins_pop, bins_dist, \\\n",
    "                                      poi_distmat, poi_cat_log_transmat, poi_pop_log_transmat, poipair_dist_log_transmat)\n",
    "    seq_rec = parse_seqstr(seq_rec_df.iloc[0]['sequence']) # choose the one with max posterior\n",
    "    F1 = calc_F1score(seq_act, seq_rec) \n",
    "    print('Actual: %-23s  Recommended: %-23s F1: %.2f' % (str(seq_act), str(seq_rec), F1))\n",
    "    seq_rec_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend trajectories using the initial training set and evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12.2060726455 - -12.2060726455 = -1.7763568394e-15\n",
      "-12.2060726455 - -12.2060726455 = -1.7763568394e-15\n",
      "-12.2060726455 - -12.2060726455 = -1.7763568394e-15\n",
      "-12.2060726455 - -12.2060726455 = -1.7763568394e-15\n",
      "-12.2060726455 - -12.2060726455 = -1.7763568394e-15\n",
      "-12.2060726455 - -12.2060726455 = -1.7763568394e-15\n",
      "-12.2060726455 - -12.2060726455 = -1.7763568394e-15\n",
      "-12.2060726455 - -12.2060726455 = -1.7763568394e-15\n",
      "-12.2060726455 - -12.2060726455 = -1.7763568394e-15\n",
      "Evaluation 0, F1 mean: 0.70, std: 0.17\n"
     ]
    }
   ],
   "source": [
    "if doCompute:\n",
    "    F1scores = evaluate_parallel(seqid_set_evaluate, poi_list, poi_info, seq_all, bins_pop, bins_dist, poi_distmat, \\\n",
    "                                 poi_cat_log_transmat, poi_pop_log_transmat, poipair_dist_log_transmat)\n",
    "    F1_scores_rand.append(F1scores)\n",
    "    print('Evaluation 0, F1 mean: %.2f, std: %.2f' % (np.mean(F1_scores_rand[-1]), np.std(F1_scores_rand[-1])))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the baseline on test set using random query strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.14580185084 - -9.14580185084 = 0.0\n",
      "-9.14580185084 - -9.14580185084 = 0.0\n",
      "-9.14580185084 - -9.14580185084 = 0.0\n",
      "-9.14580185084 - -9.14580185084 = 0.0\n",
      "-9.14580185084 - -9.14580185084 = 0.0\n",
      "-9.14580185084 - -9.14580185084 = 0.0\n",
      "-9.14580185084 - -9.14580185084 = 0.0\n",
      "-9.14580185084 - -9.14580185084 = 0.0\n",
      "-9.14580185084 - -9.14580185084 = 0.0\n",
      "Evaluation 1 /37, F1 mean: 0.66, std: 0.06\n",
      "-6.44594841694 - -6.44594841694 = 0.0\n",
      "-6.44594841694 - -6.44594841694 = 0.0\n",
      "-6.44594841694 - -6.44594841694 = 0.0\n",
      "-6.44594841694 - -6.44594841694 = 0.0\n",
      "-6.44594841694 - -6.44594841694 = 0.0\n",
      "-6.44594841694 - -6.44594841694 = 0.0\n",
      "-6.44594841694 - -6.44594841694 = 0.0\n",
      "-6.44594841694 - -6.44594841694 = 0.0\n",
      "-6.44594841694 - -6.44594841694 = 0.0\n",
      "Evaluation 2 /37, F1 mean: 0.66, std: 0.06\n",
      "-7.05671171604 - -7.05671171604 = -8.881784197e-16\n",
      "-7.05671171604 - -7.05671171604 = -8.881784197e-16\n",
      "-7.05671171604 - -7.05671171604 = -8.881784197e-16\n",
      "-7.05671171604 - -7.05671171604 = -8.881784197e-16\n",
      "-7.05671171604 - -7.05671171604 = -8.881784197e-16\n",
      "-7.05671171604 - -7.05671171604 = -8.881784197e-16\n",
      "-7.05671171604 - -7.05671171604 = -8.881784197e-16\n",
      "-7.05671171604 - -7.05671171604 = -8.881784197e-16\n",
      "-7.05671171604 - -7.05671171604 = -8.881784197e-16\n",
      "Evaluation 3 /37, F1 mean: 0.69, std: 0.03\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "Evaluation 4 /37, F1 mean: 0.73, std: 0.16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "-7.1977903143 - -7.1977903143 = -8.881784197e-16\n",
      "Evaluation 5 /37, F1 mean: 0.73, std: 0.16\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "Evaluation 6 /37, F1 mean: 0.69, std: 0.12\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "-6.21901972925 - -6.21901972925 = 0.0\n",
      "Evaluation 7 /37, F1 mean: 0.69, std: 0.12\n",
      "-8.15271717617 - -8.15271717617 = 0.0\n",
      "-8.15271717617 - -8.15271717617 = 0.0\n",
      "-8.15271717617 - -8.15271717617 = 0.0\n",
      "-8.15271717617 - -8.15271717617 = 0.0\n",
      "-8.15271717617 - -8.15271717617 = 0.0\n",
      "-8.15271717617 - -8.15271717617 = 0.0\n",
      "-8.15271717617 - -8.15271717617 = 0.0\n",
      "-8.15271717617 - -8.15271717617 = 0.0\n",
      "-8.15271717617 - -8.15271717617 = 0.0\n",
      "Evaluation 8 /37, F1 mean: 0.69, std: 0.12\n",
      "-8.32427976356 - -8.32427976356 = 0.0\n",
      "-8.32427976356 - -8.32427976356 = 0.0\n",
      "-8.32427976356 - -8.32427976356 = 0.0\n",
      "-8.32427976356 - -8.32427976356 = 0.0\n",
      "-8.32427976356 - -8.32427976356 = 0.0\n",
      "-8.32427976356 - -8.32427976356 = 0.0\n",
      "-8.32427976356 - -8.32427976356 = 0.0\n",
      "-8.32427976356 - -8.32427976356 = 0.0\n",
      "-8.32427976356 - -8.32427976356 = 0.0\n",
      "Evaluation 9 /37, F1 mean: 0.69, std: 0.12\n",
      "-8.56823262013 - -8.56823262013 = 0.0\n",
      "-8.56823262013 - -8.56823262013 = 0.0\n",
      "-8.56823262013 - -8.56823262013 = 0.0\n",
      "-8.56823262013 - -8.56823262013 = 0.0\n",
      "-8.56823262013 - -8.56823262013 = 0.0\n",
      "-8.56823262013 - -8.56823262013 = 0.0\n",
      "-8.56823262013 - -8.56823262013 = 0.0\n",
      "-8.56823262013 - -8.56823262013 = 0.0\n",
      "-8.56823262013 - -8.56823262013 = 0.0\n",
      "Evaluation 10/37, F1 mean: 0.69, std: 0.12\n",
      "-8.56166519726 - -8.56166519726 = 0.0\n",
      "-8.56166519726 - -8.56166519726 = 0.0\n",
      "-8.56166519726 - -8.56166519726 = 0.0\n",
      "-8.56166519726 - -8.56166519726 = 0.0\n",
      "-8.56166519726 - -8.56166519726 = 0.0\n",
      "-8.56166519726 - -8.56166519726 = 0.0\n",
      "-8.56166519726 - -8.56166519726 = 0.0\n",
      "-8.56166519726 - -8.56166519726 = 0.0\n",
      "-8.56166519726 - -8.56166519726 = 0.0\n",
      "Evaluation 11/37, F1 mean: 0.69, std: 0.12\n",
      "-8.31919272451 - -8.31919272451 = 0.0\n",
      "-8.31919272451 - -8.31919272451 = 0.0\n",
      "-8.31919272451 - -8.31919272451 = 0.0\n",
      "-8.31919272451 - -8.31919272451 = 0.0\n",
      "-8.31919272451 - -8.31919272451 = 0.0\n",
      "-8.31919272451 - -8.31919272451 = 0.0\n",
      "-8.31919272451 - -8.31919272451 = 0.0\n",
      "-8.31919272451 - -8.31919272451 = 0.0\n",
      "-8.31919272451 - -8.31919272451 = 0.0\n",
      "Evaluation 12/37, F1 mean: 0.69, std: 0.12\n",
      "-8.98961476194 - -8.98961476194 = 0.0\n",
      "-8.98961476194 - -8.98961476194 = 0.0\n",
      "-8.98961476194 - -8.98961476194 = 0.0\n",
      "-8.98961476194 - -8.98961476194 = 0.0\n",
      "-8.98961476194 - -8.98961476194 = 0.0\n",
      "-8.98961476194 - -8.98961476194 = 0.0\n",
      "-8.98961476194 - -8.98961476194 = 0.0\n",
      "-8.98961476194 - -8.98961476194 = 0.0\n",
      "-8.98961476194 - -8.98961476194 = 0.0\n",
      "Evaluation 13/37, F1 mean: 0.69, std: 0.12\n",
      "-9.01902185364 - -9.01902185364 = 0.0\n",
      "-9.01902185364 - -9.01902185364 = 0.0\n",
      "-9.01902185364 - -9.01902185364 = 0.0\n",
      "-9.01902185364 - -9.01902185364 = 0.0\n",
      "-9.01902185364 - -9.01902185364 = 0.0\n",
      "-9.01902185364 - -9.01902185364 = 0.0\n",
      "-9.01902185364 - -9.01902185364 = 0.0\n",
      "-9.01902185364 - -9.01902185364 = 0.0\n",
      "-9.01902185364 - -9.01902185364 = 0.0\n",
      "Evaluation 14/37, F1 mean: 0.73, std: 0.16\n",
      "-8.89472413696 - -8.89472413696 = 0.0\n",
      "-8.89472413696 - -8.89472413696 = 0.0\n",
      "-8.89472413696 - -8.89472413696 = 0.0\n",
      "-8.89472413696 - -8.89472413696 = 0.0\n",
      "-8.89472413696 - -8.89472413696 = 0.0\n",
      "-8.89472413696 - -8.89472413696 = 0.0\n",
      "-8.89472413696 - -8.89472413696 = 0.0\n",
      "-8.89472413696 - -8.89472413696 = 0.0\n",
      "-8.89472413696 - -8.89472413696 = 0.0\n",
      "Evaluation 15/37, F1 mean: 0.73, std: 0.16\n",
      "-9.89862443625 - -9.89862443625 = 0.0\n",
      "-9.89862443625 - -9.89862443625 = 0.0\n",
      "-9.89862443625 - -9.89862443625 = 0.0\n",
      "-9.89862443625 - -9.89862443625 = 0.0\n",
      "-9.89862443625 - -9.89862443625 = 0.0\n",
      "-9.89862443625 - -9.89862443625 = 0.0\n",
      "-9.89862443625 - -9.89862443625 = 0.0\n",
      "-9.89862443625 - -9.89862443625 = 0.0\n",
      "-9.89862443625 - -9.89862443625 = 0.0\n",
      "Evaluation 16/37, F1 mean: 0.73, std: 0.16\n",
      "-9.8389490314 - -9.8389490314 = 0.0\n",
      "-9.8389490314 - -9.8389490314 = 0.0\n",
      "-9.8389490314 - -9.8389490314 = 0.0\n",
      "-9.8389490314 - -9.8389490314 = 0.0\n",
      "-9.8389490314 - -9.8389490314 = 0.0\n",
      "-9.8389490314 - -9.8389490314 = 0.0\n",
      "-9.8389490314 - -9.8389490314 = 0.0\n",
      "-9.8389490314 - -9.8389490314 = 0.0\n",
      "-9.8389490314 - -9.8389490314 = 0.0\n",
      "Evaluation 17/37, F1 mean: 0.73, std: 0.16\n",
      "-9.74909270228 - -9.74909270228 = 1.7763568394e-15\n",
      "-9.74909270228 - -9.74909270228 = 1.7763568394e-15\n",
      "-9.74909270228 - -9.74909270228 = 1.7763568394e-15\n",
      "-9.74909270228 - -9.74909270228 = 1.7763568394e-15\n",
      "-9.74909270228 - -9.74909270228 = 1.7763568394e-15\n",
      "-9.74909270228 - -9.74909270228 = 1.7763568394e-15\n",
      "-9.74909270228 - -9.74909270228 = 1.7763568394e-15\n",
      "-9.74909270228 - -9.74909270228 = 1.7763568394e-15\n",
      "-9.74909270228 - -9.74909270228 = 1.7763568394e-15\n",
      "Evaluation 18/37, F1 mean: 0.73, std: 0.16\n",
      "-9.41494580229 - -9.41494580229 = 0.0\n",
      "-9.41494580229 - -9.41494580229 = 0.0\n",
      "-9.41494580229 - -9.41494580229 = 0.0\n",
      "-9.41494580229 - -9.41494580229 = 0.0\n",
      "-9.41494580229 - -9.41494580229 = 0.0\n",
      "-9.41494580229 - -9.41494580229 = 0.0\n",
      "-9.41494580229 - -9.41494580229 = 0.0\n",
      "-9.41494580229 - -9.41494580229 = 0.0\n",
      "-9.41494580229 - -9.41494580229 = 0.0\n",
      "Evaluation 19/37, F1 mean: 0.77, std: 0.17\n",
      "-9.22268374988 - -9.22268374988 = 1.7763568394e-15\n",
      "-9.22268374988 - -9.22268374988 = 1.7763568394e-15\n",
      "-9.22268374988 - -9.22268374988 = 1.7763568394e-15\n",
      "-9.22268374988 - -9.22268374988 = 1.7763568394e-15\n",
      "-9.22268374988 - -9.22268374988 = 1.7763568394e-15\n",
      "-9.22268374988 - -9.22268374988 = 1.7763568394e-15\n",
      "-9.22268374988 - -9.22268374988 = 1.7763568394e-15\n",
      "-9.22268374988 - -9.22268374988 = 1.7763568394e-15\n",
      "-9.22268374988 - -9.22268374988 = 1.7763568394e-15\n",
      "Evaluation 20/37, F1 mean: 0.69, std: 0.12\n",
      "-9.38155489553 - -9.38155489553 = 0.0\n",
      "-9.38155489553 - -9.38155489553 = 0.0\n",
      "-9.38155489553 - -9.38155489553 = 0.0\n",
      "-9.38155489553 - -9.38155489553 = 0.0\n",
      "-9.38155489553 - -9.38155489553 = 0.0\n",
      "-9.38155489553 - -9.38155489553 = 0.0\n",
      "-9.38155489553 - -9.38155489553 = 0.0\n",
      "-9.38155489553 - -9.38155489553 = 0.0\n",
      "-9.38155489553 - -9.38155489553 = 0.0\n",
      "Evaluation 21/37, F1 mean: 0.73, std: 0.16\n",
      "-9.38306878803 - -9.38306878803 = 0.0\n",
      "-9.38306878803 - -9.38306878803 = 0.0\n",
      "-9.38306878803 - -9.38306878803 = 0.0\n",
      "-9.38306878803 - -9.38306878803 = 0.0\n",
      "-9.38306878803 - -9.38306878803 = 0.0\n",
      "-9.38306878803 - -9.38306878803 = 0.0\n",
      "-9.38306878803 - -9.38306878803 = 0.0\n",
      "-9.38306878803 - -9.38306878803 = 0.0\n",
      "-9.38306878803 - -9.38306878803 = 0.0\n",
      "Evaluation 22/37, F1 mean: 0.73, std: 0.16\n",
      "-9.4618069523 - -9.4618069523 = -1.7763568394e-15\n",
      "-9.4618069523 - -9.4618069523 = -1.7763568394e-15\n",
      "-9.4618069523 - -9.4618069523 = -1.7763568394e-15\n",
      "-9.4618069523 - -9.4618069523 = -1.7763568394e-15\n",
      "-9.4618069523 - -9.4618069523 = -1.7763568394e-15\n",
      "-9.4618069523 - -9.4618069523 = -1.7763568394e-15\n",
      "-9.4618069523 - -9.4618069523 = -1.7763568394e-15\n",
      "-9.4618069523 - -9.4618069523 = -1.7763568394e-15\n",
      "-9.4618069523 - -9.4618069523 = -1.7763568394e-15\n",
      "Evaluation 23/37, F1 mean: 0.73, std: 0.16\n",
      "-9.09358381409 - -9.09358381409 = -1.7763568394e-15\n",
      "-9.09358381409 - -9.09358381409 = -1.7763568394e-15\n",
      "-9.09358381409 - -9.09358381409 = -1.7763568394e-15\n",
      "-9.09358381409 - -9.09358381409 = -1.7763568394e-15\n",
      "-9.09358381409 - -9.09358381409 = -1.7763568394e-15\n",
      "-9.09358381409 - -9.09358381409 = -1.7763568394e-15\n",
      "-9.09358381409 - -9.09358381409 = -1.7763568394e-15\n",
      "-9.09358381409 - -9.09358381409 = -1.7763568394e-15\n",
      "-9.09358381409 - -9.09358381409 = -1.7763568394e-15\n",
      "Evaluation 24/37, F1 mean: 0.73, std: 0.16\n",
      "-9.28870393814 - -9.28870393814 = 0.0\n",
      "-9.28870393814 - -9.28870393814 = 0.0\n",
      "-9.28870393814 - -9.28870393814 = 0.0\n",
      "-9.28870393814 - -9.28870393814 = 0.0\n",
      "-9.28870393814 - -9.28870393814 = 0.0\n",
      "-9.28870393814 - -9.28870393814 = 0.0\n",
      "-9.28870393814 - -9.28870393814 = 0.0\n",
      "-9.28870393814 - -9.28870393814 = 0.0\n",
      "-9.28870393814 - -9.28870393814 = 0.0\n",
      "Evaluation 25/37, F1 mean: 0.77, std: 0.17\n",
      "-9.47976295727 - -9.47976295727 = 0.0\n",
      "-9.47976295727 - -9.47976295727 = 0.0\n",
      "-9.47976295727 - -9.47976295727 = 0.0\n",
      "-9.47976295727 - -9.47976295727 = 0.0\n",
      "-9.47976295727 - -9.47976295727 = 0.0\n",
      "-9.47976295727 - -9.47976295727 = 0.0\n",
      "-9.47976295727 - -9.47976295727 = 0.0\n",
      "-9.47976295727 - -9.47976295727 = 0.0\n",
      "-9.47976295727 - -9.47976295727 = 0.0\n",
      "Evaluation 26/37, F1 mean: 0.70, std: 0.17\n",
      "-9.59373784616 - -9.59373784616 = 0.0\n",
      "-9.59373784616 - -9.59373784616 = 0.0\n",
      "-9.59373784616 - -9.59373784616 = 0.0\n",
      "-9.59373784616 - -9.59373784616 = 0.0\n",
      "-9.59373784616 - -9.59373784616 = 0.0\n",
      "-9.59373784616 - -9.59373784616 = 0.0\n",
      "-9.59373784616 - -9.59373784616 = 0.0\n",
      "-9.59373784616 - -9.59373784616 = 0.0\n",
      "-9.59373784616 - -9.59373784616 = 0.0\n",
      "Evaluation 27/37, F1 mean: 0.67, std: 0.14\n",
      "-9.37241161013 - -9.37241161013 = 0.0\n",
      "-9.37241161013 - -9.37241161013 = 0.0\n",
      "-9.37241161013 - -9.37241161013 = 0.0\n",
      "-9.37241161013 - -9.37241161013 = 0.0\n",
      "-9.37241161013 - -9.37241161013 = 0.0\n",
      "-9.37241161013 - -9.37241161013 = 0.0\n",
      "-9.37241161013 - -9.37241161013 = 0.0\n",
      "-9.37241161013 - -9.37241161013 = 0.0\n",
      "-9.37241161013 - -9.37241161013 = 0.0\n",
      "Evaluation 28/37, F1 mean: 0.67, std: 0.14\n",
      "-9.12203587393 - -9.12203587393 = 0.0\n",
      "-9.12203587393 - -9.12203587393 = 0.0\n",
      "-9.12203587393 - -9.12203587393 = 0.0\n",
      "-9.12203587393 - -9.12203587393 = 0.0\n",
      "-9.12203587393 - -9.12203587393 = 0.0\n",
      "-9.12203587393 - -9.12203587393 = 0.0\n",
      "-9.12203587393 - -9.12203587393 = 0.0\n",
      "-9.12203587393 - -9.12203587393 = 0.0\n",
      "-9.12203587393 - -9.12203587393 = 0.0\n",
      "Evaluation 29/37, F1 mean: 0.67, std: 0.14\n",
      "-9.4775890412 - -9.4775890412 = 0.0\n",
      "-9.4775890412 - -9.4775890412 = 0.0\n",
      "-9.4775890412 - -9.4775890412 = 0.0\n",
      "-9.4775890412 - -9.4775890412 = 0.0\n",
      "-9.4775890412 - -9.4775890412 = 0.0\n",
      "-9.4775890412 - -9.4775890412 = 0.0\n",
      "-9.4775890412 - -9.4775890412 = 0.0\n",
      "-9.4775890412 - -9.4775890412 = 0.0\n",
      "-9.4775890412 - -9.4775890412 = 0.0\n",
      "Evaluation 30/37, F1 mean: 0.67, std: 0.14\n",
      "-9.80772338444 - -9.80772338444 = 0.0\n",
      "-9.80772338444 - -9.80772338444 = 0.0\n",
      "-9.80772338444 - -9.80772338444 = 0.0\n",
      "-9.80772338444 - -9.80772338444 = 0.0\n",
      "-9.80772338444 - -9.80772338444 = 0.0\n",
      "-9.80772338444 - -9.80772338444 = 0.0\n",
      "-9.80772338444 - -9.80772338444 = 0.0\n",
      "-9.80772338444 - -9.80772338444 = 0.0\n",
      "-9.80772338444 - -9.80772338444 = 0.0\n",
      "Evaluation 31/37, F1 mean: 0.67, std: 0.14\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "Evaluation 32/37, F1 mean: 0.67, std: 0.14\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "-10.6547118425 - -10.6547118425 = -1.7763568394e-15\n",
      "Evaluation 33/37, F1 mean: 0.67, std: 0.14\n",
      "-10.7507854129 - -10.7507854129 = 0.0\n",
      "-10.7507854129 - -10.7507854129 = 0.0\n",
      "-10.7507854129 - -10.7507854129 = 0.0\n",
      "-10.7507854129 - -10.7507854129 = 0.0\n",
      "-10.7507854129 - -10.7507854129 = 0.0\n",
      "-10.7507854129 - -10.7507854129 = 0.0\n",
      "-10.7507854129 - -10.7507854129 = 0.0\n",
      "-10.7507854129 - -10.7507854129 = 0.0\n",
      "-10.7507854129 - -10.7507854129 = 0.0\n",
      "Evaluation 34/37, F1 mean: 0.67, std: 0.14\n",
      "-10.8438518065 - -10.8438518065 = 0.0\n",
      "-10.8438518065 - -10.8438518065 = 0.0\n",
      "-10.8438518065 - -10.8438518065 = 0.0\n",
      "-10.8438518065 - -10.8438518065 = 0.0\n",
      "-10.8438518065 - -10.8438518065 = 0.0\n",
      "-10.8438518065 - -10.8438518065 = 0.0\n",
      "-10.8438518065 - -10.8438518065 = 0.0\n",
      "-10.8438518065 - -10.8438518065 = 0.0\n",
      "-10.8438518065 - -10.8438518065 = 0.0\n",
      "Evaluation 35/37, F1 mean: 0.67, std: 0.14\n",
      "-10.6059845989 - -10.6059845989 = 0.0\n",
      "-10.6059845989 - -10.6059845989 = 0.0\n",
      "-10.6059845989 - -10.6059845989 = 0.0\n",
      "-10.6059845989 - -10.6059845989 = 0.0\n",
      "-10.6059845989 - -10.6059845989 = 0.0\n",
      "-10.6059845989 - -10.6059845989 = 0.0\n",
      "-10.6059845989 - -10.6059845989 = 0.0\n",
      "-10.6059845989 - -10.6059845989 = 0.0\n",
      "-10.6059845989 - -10.6059845989 = 0.0\n",
      "Evaluation 36/37, F1 mean: 0.67, std: 0.14\n",
      "-10.6287039365 - -10.6287039365 = 0.0\n",
      "-10.6287039365 - -10.6287039365 = 0.0\n",
      "-10.6287039365 - -10.6287039365 = 0.0\n",
      "-10.6287039365 - -10.6287039365 = 0.0\n",
      "-10.6287039365 - -10.6287039365 = 0.0\n",
      "-10.6287039365 - -10.6287039365 = 0.0\n",
      "-10.6287039365 - -10.6287039365 = 0.0\n",
      "-10.6287039365 - -10.6287039365 = 0.0\n",
      "-10.6287039365 - -10.6287039365 = 0.0\n",
      "Evaluation 37/37, F1 mean: 0.67, std: 0.14\n"
     ]
    }
   ],
   "source": [
    "if doCompute:\n",
    "    n = 1\n",
    "    for idx in query_order:\n",
    "        # choose sequence to query\n",
    "        seqid = seqid_set_annotate[idx]\n",
    "        \n",
    "        # add query result to training set, here just add the sequence id to training set\n",
    "        seq_order_rand.append(seqid)\n",
    "        seqid_set_training.append(seqid)\n",
    "        \n",
    "        # estimate parameters using current training set\n",
    "        (poi_info, poi_cat_log_transmat, poi_pop_log_transmat, poipair_dist_log_transmat) = estimate_parameter(\\\n",
    "        seqid_set_training, poi_cats, bins_pop, bins_dist, poi_distmat, poi_all, seq_all)\n",
    "        \n",
    "        # evaluate on test set\n",
    "        F1scores = evaluate_parallel(seqid_set_evaluate, poi_list, poi_info, seq_all, bins_pop, bins_dist, poi_distmat, \\\n",
    "                                     poi_cat_log_transmat, poi_pop_log_transmat, poipair_dist_log_transmat)\n",
    "        F1_scores_rand.append(F1scores)\n",
    "        print('Evaluation %-2d/%d, F1 mean: %.2f, std: %.2f' % \\\n",
    "              (n, len(seqid_set_annotate0), np.mean(F1_scores_rand[-1]), np.std(F1_scores_rand[-1])))\n",
    "        sys.stdout.flush()\n",
    "        n += 1\n",
    "        \n",
    "    #pickle.dump(F1_scores_rand, open(frand, 'wb'))\n",
    "    #pickle.dump(seq_order_rand, open(forand, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f11895d49e8>"
      ]
     },
     "execution_count": 1241,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3YAAAJeCAYAAAD4JZdiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDgAAYHCCHQAAwOD+C3aOGG1ud3TmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f11895784e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[15, 10])\n",
    "plt.boxplot(F1_scores_rand)\n",
    "plt.plot(np.arange(len(F1_scores_rand)), [np.mean(x) for x in F1_scores_rand], color='g', marker='o')\n",
    "#plt.plot(np.arange(len(seqid_set_annotate)+1), F1_random, marker='s', color='g', linestyle='--', label='random baseline')\n",
    "#plt.errorbar(np.arange(len(seqid_set_annotate0)+1), F1_mean_rand, yerr=F1_std_rand, color='g', \\\n",
    "#             linestyle='--', marker='s', label='random baseline')\n",
    "plt.ylim([0.3, 1.1])\n",
    "xticks = [10*x for x in range(int(round(len(F1_scores_rand), -1)/10))]\n",
    "plt.xticks(xticks, xticks) # xticks starts from 1\n",
    "plt.xlabel('#query')\n",
    "plt.ylabel('F1')\n",
    "#plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Least Confident Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Strategy:\n",
    "\\begin{equation}\n",
    "\\phi^{LC}(\\textbf{x}) = 1 - P(\\textbf{y}^* | \\textbf{x}; \\Theta)\n",
    "\\end{equation}\n",
    "where $\\textbf{y}^*$ is the most likely label of example $\\textbf{x}$ with respect to a probabilistic model of which the parameters are denoted by $\\Theta$.  \n",
    "This query strategy select the example $\\textbf{x}$ of maximum $\\phi^{LC}$ from all unlabelled examples in a pool to query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**:\n",
    "- This query strategy biases to sequences with more POIs, as the number of candidate trajectories grows exponentially as the number of POIs specified, the probability of the most likely candidate trajectory becomes smaller and smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of training set and annotation set and use the copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_training = list(seqid_set_training0)\n",
    "seqid_set_annotate = list(seqid_set_annotate0)\n",
    "seqid_set_evaluate = list(seqid_set_evaluate0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1_scores_lc = []\n",
    "seq_order_lc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doCompute = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(flc) and os.path.exists(folc):\n",
    "    doCompute = False\n",
    "    F1_scores_lc = pickle.load(open(flc, 'rb'))\n",
    "    seq_order_lc = pickle.load(open(folc, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend trajectories using the initial training set and evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(poi_info, poi_cat_transmat_log, poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log) = \\\n",
    "#estimate_parameter(seqid_set_training, poi_all, seq_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(poi_info)\n",
    "#print(poi_cat_transmat_log)\n",
    "#print(poi_pop_transmat_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#seq_act = [8, 19, 22]\n",
    "#seq_rec_df = recommend_trajectory(seq_act[0], seq_act[-1], len(seq_act), poi_info, poi_cat_transmat_log, \\\n",
    "#                                  poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "#print(seq_act); print(seq_rec_df); print(); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doCompute:\n",
    "    (poi_info, poi_cat_transmat_log, poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log) = \\\n",
    "    estimate_parameter(seqid_set_training, poi_all, seq_all)\n",
    "    F1scores = evaluate_parallel(seqid_set_evaluate, poi_info, seq_all, poi_cat_transmat_log, \\\n",
    "                                 poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "    F1_scores_lc.append(F1scores)\n",
    "    print('Evaluation 0, F1 mean: %.2f, std: %.2f' % (np.mean(F1_scores_lc[-1]), np.std(F1_scores_lc[-1])))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the baseline on test set using least confident query strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doCompute:\n",
    "    n = 1\n",
    "    seqs_annotate = [extract_seq(seqid, seq_all) for seqid in seqid_set_annotate]\n",
    "    # repeat until all sequences in annotation set have been queried \n",
    "    while len(seqid_set_annotate) > 0:\n",
    "        # choose sequence to query\n",
    "        # sequential: OK\n",
    "        #seq_idx = 0; max_lc = 0\n",
    "        #for idx in range(len(seqid_set_annotate)):\n",
    "        #    seq_act = seqs_annotate[idx]\n",
    "        #    seq_rec_df = recommend_trajectory(seq_act[0], seq_act[-1], len(seq_act), poi_info, poi_cat_transmat_log, \\\n",
    "        #                                      poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "        #    #print(seq_act); print(seq_rec_df); print(); sys.stdout.flush()\n",
    "        #    maxprob = seq_rec_df.iloc[0]['probability']\n",
    "        #    assert(not np.isnan(maxprob))\n",
    "        #    lc = 1 - maxprob\n",
    "        #    if lc > max_lc:\n",
    "        #        max_lc = lc; seq_idx = idx\n",
    "                \n",
    "        # parallel\n",
    "        seq_rec_df_list = Parallel(n_jobs=-2)(delayed(recommend_trajectory)\\\n",
    "                          (seq[0], seq[-1], len(seq), poi_info, poi_cat_transmat_log, \\\n",
    "                           poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log) \\\n",
    "                          for seq in seqs_annotate)\n",
    "        maxprobs = np.array([seq_rec_df.iloc[0]['probability'] for seq_rec_df in seq_rec_df_list])\n",
    "        assert(np.any(np.isnan(maxprobs)) == False) # No NaN value\n",
    "        #lc = 1 - maxprobs\n",
    "        #seq_idx = np.argmax(lc)\n",
    "        seq_idx = np.argmin(maxprobs)\n",
    "\n",
    "        # add query result to training set, here just add the sequence id to training set\n",
    "        seq_order_lc.append(seqid_set_annotate[seq_idx])\n",
    "        seqid_set_training.append(seqid_set_annotate[seq_idx])\n",
    "        print('Choose sequence %23s, LC: %.4f' % (str(seqs_annotate[seq_idx]), 1-maxprobs[seq_idx])); sys.stdout.flush()\n",
    "\n",
    "        # remove the selected example from annotation set\n",
    "        del seqid_set_annotate[seq_idx]\n",
    "        del seqs_annotate[seq_idx]\n",
    "\n",
    "        # estimate parameters using the updated training set\n",
    "        (poi_info, poi_cat_transmat_log, poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log) = \\\n",
    "        estimate_parameter(seqid_set_training, poi_all, seq_all)\n",
    "\n",
    "        # evaluate on test set\n",
    "        F1scores = evaluate_parallel(seqid_set_evaluate, poi_info, seq_all, poi_cat_transmat_log, \\\n",
    "                                     poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "        F1_scores_lc.append(F1scores)\n",
    "        print('Evaluation %-3d/%d, F1 mean: %.2f, std: %.2f' % \\\n",
    "              (n, len(seqid_set_annotate0), np.mean(F1_scores_lc[-1]), np.std(F1_scores_lc[-1])))\n",
    "        sys.stdout.flush()\n",
    "        n += 1\n",
    "        \n",
    "    pickle.dump(F1_scores_lc, open(flc, 'wb'))\n",
    "    pickle.dump(seq_order_lc, open(folc, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 10])\n",
    "plt.boxplot(F1_scores_lc, labels=np.arange(len(F1_scores_lc)))\n",
    "plt.plot(np.arange(len(F1_scores_lc)), [np.mean(x) for x in F1_scores_lc], color='g', marker='o')\n",
    "#plt.errorbar(np.arange(len(seqid_set_annotate0)+1), F1_mean_rand, yerr=F1_std_rand, \\\n",
    "#             linestyle='--', marker='s', label='random baseline')\n",
    "#plt.errorbar(np.arange(len(seqid_set_annotate0)+1), F1_mean_lc, yerr=F1_std_lc, \\\n",
    "#             linestyle='--', marker='s', label='least confident')\n",
    "xticks = [10*x for x in range(int(round(len(F1_scores_lc), -1)/10))]\n",
    "plt.xticks(xticks, xticks) # xticks starts from 1\n",
    "plt.ylim([0.3, 1.1])\n",
    "plt.xlabel('#query')\n",
    "plt.ylabel('F1')\n",
    "#plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Maximum Sequence Entropy Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\phi^{SE}(\\textbf{x}) = - \\sum_{\\hat{\\textbf{y}}} P(\\hat{\\textbf{y}} | \\textbf{x}; \\Theta) \n",
    "\\log(P(\\hat{\\textbf{y}} | \\textbf{x}; \\Theta))\n",
    "\\end{equation}\n",
    "where $\\hat{\\textbf{y}}$ ranges over all possible labels for example $\\textbf{x}$.  \n",
    "**Note** that the number of possible labels grows exponentially with $|Traj|$ in $\\textbf{x}$, to make computation feasible,\n",
    "[Kim06](http://www.aclweb.org/anthology/N06-2018) used the $N$-best possible labels to approximate, concretely, \n",
    "define **N-best Sequence Entropy** as\n",
    "\\begin{equation}\n",
    "\\phi^{NSE}(\\textbf{x}) = - \\sum_{\\hat{\\textbf{y}} \\in \\mathcal{N}} P(\\hat{\\textbf{y}} | \\textbf{x}; \\Theta) \n",
    "\\log(P(\\hat{\\textbf{y}} | \\textbf{x}; \\Theta))\n",
    "\\end{equation}\n",
    "where $\\mathcal{N} = \\{\\textbf{y}_1^*, \\dots, \\textbf{y}_N^*\\}$ is the set of the $N$ most likely labels of example $\\textbf{x}$.  \n",
    "This query strategy select the example $\\textbf{x}$ of maximum $\\phi^{SE}$ or $\\phi^{NSE}$ from all unlabelled examples in a pool to query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of training set and annotation set and use the copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_training = list(seqid_set_training0)\n",
    "seqid_set_annotate = list(seqid_set_annotate0)\n",
    "seqid_set_evaluate = list(seqid_set_evaluate0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1_scores_se = []\n",
    "seq_order_se = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doCompute = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(fse) and os.path.exists(fose):\n",
    "    doCompute = False\n",
    "    F1_scores_se = pickle.load(open(fse, 'rb'))\n",
    "    seq_order_se = pickle.load(open(fose, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend trajectories using the initial training set and evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doCompute:\n",
    "    (poi_info, poi_cat_transmat_log, poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log) = \\\n",
    "    estimate_parameter(seqid_set_training, poi_all, seq_all)\n",
    "    F1scores = evaluate_parallel(seqid_set_evaluate, poi_info, seq_all, poi_cat_transmat_log, \\\n",
    "                                 poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "    F1_scores_se.append(F1scores)\n",
    "    print('Evaluation 0, F1 mean: %.2f, std: %.2f' % (np.mean(F1_scores_se[-1]), np.std(F1_scores_se[-1])))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute sequence entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_sequence_entropy(start, end, length, poi_info, poi_cat_transmat_log, poi_pop_transmat_log, \\\n",
    "                          poipair_distclass_mat, poipair_dist_transmat_log):    \n",
    "    # enumerate trajectory\n",
    "    poi_list = poi_info.index.tolist()\n",
    "    enum_seqs = enum_seq345(start, end, length, poi_list)\n",
    "\n",
    "    # sequential\n",
    "    logL = []\n",
    "    for seq in enum_seqs:\n",
    "        logl = calc_seq_loglikelihood(seq, poi_info, poi_cat_transmat_log, poi_pop_transmat_log, \\\n",
    "                                      poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "        logL.append(logl)\n",
    "    \n",
    "    probs = np.array(logL)\n",
    "    probs = np.exp(probs)\n",
    "    probs /= np.sum(probs) # normalise\n",
    "    \n",
    "    return -1 * np.dot(probs, np.log10(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the baseline on test set using sequence entropy query strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doCompute:\n",
    "    n = 1\n",
    "    seqs_annotate = [extract_seq(seqid, seq_all) for seqid in seqid_set_annotate]\n",
    "    # repeat until all sequences in annotation set have been queried \n",
    "    while len(seqid_set_annotate) > 0:\n",
    "        # choose sequence to query\n",
    "        seq_entropy_list = Parallel(n_jobs=-2)(delayed(calc_sequence_entropy)\\\n",
    "                           (seq[0], seq[-1], len(seq), poi_info, poi_cat_transmat_log, \\\n",
    "                            poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log) \\\n",
    "                           for seq in seqs_annotate)\n",
    "        assert(np.any(np.isnan(np.array(seq_entropy_list))) == False) # No NaN value\n",
    "        seq_idx = np.argmax(np.array(seq_entropy_list))\n",
    "\n",
    "        # add query result to training set, here just add the sequence id to training set\n",
    "        seq_order_se.append(seqid_set_annotate[seq_idx])\n",
    "        seqid_set_training.append(seqid_set_annotate[seq_idx])\n",
    "        print('Choose sequence %23s, SE: %.4f' % (str(seqs_annotate[seq_idx]), seq_entropy_list[seq_idx]))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # remove the selected example from annotation set\n",
    "        del seqid_set_annotate[seq_idx]\n",
    "        del seqs_annotate[seq_idx]\n",
    "\n",
    "        # estimate parameters using the updated training set\n",
    "        (poi_info, poi_cat_transmat_log, poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log) = \\\n",
    "        estimate_parameter(seqid_set_training, poi_all, seq_all)\n",
    "\n",
    "        # evaluate on test set\n",
    "        F1scores = evaluate_parallel(seqid_set_evaluate, poi_info, seq_all, poi_cat_transmat_log, \\\n",
    "                                     poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "        F1_scores_se.append(F1scores)\n",
    "        print('Evaluation %-3d/%d, F1 mean: %.2f, std: %.2f' % \\\n",
    "              (n, len(seqid_set_annotate0), np.mean(F1_scores_se[-1]), np.std(F1_scores_se[-1])))\n",
    "        sys.stdout.flush()\n",
    "        n += 1\n",
    "        \n",
    "    pickle.dump(F1_scores_se, open(fse, 'wb'))\n",
    "    pickle.dump(seq_order_se, open(fose, 'wb'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 10])\n",
    "plt.boxplot(F1_scores_se)\n",
    "plt.plot(np.arange(len(F1_scores_se)), [np.mean(x) for x in F1_scores_se], color='g', marker='o')\n",
    "xticks = [10*x for x in range(int(round(len(F1_scores_se), -1)/10))]\n",
    "plt.xticks(xticks, xticks) # xticks starts from 1\n",
    "plt.ylim([0.3, 1.1])\n",
    "plt.xlabel('#query')\n",
    "plt.ylabel('F1')\n",
    "#plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Maximum Information Density Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\phi^{ID}(\\textbf{x}) = \\phi^{SE}(\\textbf{x}) \\times \n",
    "\\left(\n",
    "\\frac{1}{U} \\sum_{u=1}^U \\text{sim}(\\textbf{x}, \\textbf{x}^u)\n",
    "\\right)^\\beta\n",
    "\\end{equation}\n",
    "That is, the informativeness of example $\\textbf{x}$ is weighted by its average similarity to all other unlabelled examples (denoted by $\\mathcal{U})$ in the pool, subject to parameter $\\beta$ which was set to $1$ in [Settles08](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.187.7401&rep=rep1&type=pdf) giving no reason, \n",
    "sequence entropy $\\phi^{SE}$ measures the \"base\" informativeness and could be replaced by $\\phi^{NSE}$ defined above.  \n",
    "This query strategy select the example $\\textbf{x}$ of maximum $\\phi^{ID}$ from all unlabelled examples in a pool to query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to compute the simularity between two trajectories, [Settles08](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.187.7401&rep=rep1&type=pdf) uses cosine simularity after transforming a sequence into a fixed length feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let sequence $i$ $[p_{i_1}, p_{i_2}, \\dots, p_{i_{N1}}]$ and sequence $j$ $[p_{j_1}, p_{j_2}, \\dots, p_{j_{N1}}]$,\n",
    "define the similarity between sequence $i$ and $j$ as follows:\n",
    "\\begin{equation}\n",
    "\\text{sim}([p_{i_1}, p_{i_2}, \\dots, p_{i_{N1}}], [p_{j_1}, p_{j_2}, \\dots, p_{j_{N1}}]) = \n",
    "\\frac{\\lvert [p_{i_1}, p_{i_2}, \\dots, p_{i_{N1}}] \\cap [p_{j_1}, p_{j_2}, \\dots, p_{j_{N1}}] \\rvert}\n",
    "     {\\lvert [p_{i_1}, p_{i_2}, \\dots, p_{i_{N1}}] \\cup [p_{j_1}, p_{j_2}, \\dots, p_{j_{N1}}] \\rvert}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_sequence_similarity(seq1, seq2):\n",
    "    assert(len(seq1) > 0 and len(seq2) > 0)\n",
    "    return len(set(seq1) & set(seq2)) / len(set(seq1) | set(seq2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of training set and annotation set and use the copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_training = list(seqid_set_training0)\n",
    "seqid_set_annotate = list(seqid_set_annotate0)\n",
    "seqid_set_evaluate = list(seqid_set_evaluate0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the simularity between all pair of unlabelled exmaples.  \n",
    "**NOTE** that there are duplicate sequences exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distinct_seqstr_list = {str(extract_seq(seqid, seq_all)) for seqid in seqid_set_annotate0}\n",
    "seqs_annotate0 = [parse_seqstr(seqstr) for seqstr in sorted(distinct_seqstr_list)] # distinct sequences\n",
    "seq_sim_mat = pd.DataFrame(data=np.zeros((len(seqs_annotate0), len(seqs_annotate0)), dtype=np.float), \\\n",
    "                           index=[str(x) for x in seqs_annotate0], columns=[str(x) for x in seqs_annotate0])\n",
    "for i in range(len(seqs_annotate0)):\n",
    "    for j in range(i, len(seqs_annotate0)):\n",
    "        sim = calc_sequence_similarity(seqs_annotate0[i], seqs_annotate0[j])\n",
    "        assert(not np.isnan(sim))\n",
    "        seq_sim_mat.iloc[i, j] = sim\n",
    "        seq_sim_mat.iloc[j, i] = sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#seq_sim_mat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#k = 5\n",
    "#sumk = seq_sim_mat.iloc[k].sum()\n",
    "#sumk /= len(seqs_annotate0)\n",
    "#sumk = sumk**1\n",
    "#print(sumk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sums=[(np.sum([seq_sim_mat.loc[str(x), str(y)] for y in seqs_annotate0])/len(seqs_annotate0))**1 for x in seqs_annotate0]\n",
    "#sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1_scores_id = []\n",
    "seq_order_id = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doCompute = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists(fid) and os.path.exists(foid):\n",
    "    doCompute = False\n",
    "    F1_scores_id = pickle.load(open(fid, 'rb'))\n",
    "    seq_order_id = pickle.load(open(foid, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend trajectories using the initial training set and evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doCompute:\n",
    "    (poi_info, poi_cat_transmat_log, poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log) = \\\n",
    "    estimate_parameter(seqid_set_training, poi_all, seq_all)\n",
    "    F1scores = evaluate_parallel(seqid_set_evaluate, poi_info, seq_all, poi_cat_transmat_log, \\\n",
    "                                 poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "    F1_scores_id.append(F1scores)\n",
    "    print('Evaluation 0, F1 mean: %.2f, std: %.2f' % (np.mean(F1_scores_id[-1]), np.std(F1_scores_id[-1])))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the baseline on test set using information density query strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doCompute:\n",
    "    n = 1\n",
    "    beta = 1\n",
    "    seqs_annotate = [extract_seq(seqid, seq_all) for seqid in seqid_set_annotate]\n",
    "    # repeat until all sequences in annotation set have been queried \n",
    "    while len(seqid_set_annotate) > 0:\n",
    "        # choose sequence to query\n",
    "        seq_entropy_list = Parallel(n_jobs=-2)(delayed(calc_sequence_entropy)\\\n",
    "                           (seq[0], seq[-1], len(seq), poi_info, poi_cat_transmat_log, \\\n",
    "                            poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log) \\\n",
    "                           for seq in seqs_annotate)\n",
    "        assert(np.any(np.isnan(np.array(seq_entropy_list))) == False) # No NaN value\n",
    "        seq_sim_list = [(np.sum([seq_sim_mat.loc[str(x), str(y)] for y in seqs_annotate])/len(seqs_annotate))**beta \\\n",
    "                        for x in seqs_annotate]\n",
    "        assert(len(seq_entropy_list) == len(seq_sim_list))\n",
    "        seq_ids = np.multiply(np.array(seq_entropy_list), np.array(seq_sim_list))\n",
    "        seq_idx = np.argmax(seq_ids)\n",
    "\n",
    "        # add query result to training set, here just add the sequence id to training set\n",
    "        seq_order_id.append(seqid_set_annotate[seq_idx])\n",
    "        seqid_set_training.append(seqid_set_annotate[seq_idx])\n",
    "        print('Choose sequence %23s, ID:%.4f' % (str(seqs_annotate[seq_idx]), seq_ids[seq_idx]))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # remove the selected example from annotation set\n",
    "        del seqid_set_annotate[seq_idx]\n",
    "        del seqs_annotate[seq_idx]\n",
    "\n",
    "        # estimate parameters using the updated training set\n",
    "        (poi_info, poi_cat_transmat_log, poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log) = \\\n",
    "        estimate_parameter(seqid_set_training, poi_all, seq_all)\n",
    "\n",
    "        # evaluate on test set\n",
    "        F1scores = evaluate_parallel(seqid_set_evaluate, poi_info, seq_all, poi_cat_transmat_log, \\\n",
    "                                     poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "        F1_scores_id.append(F1scores)\n",
    "        print('Evaluation %-3d/%d, F1 mean: %.2f, std: %.2f' % \\\n",
    "              (n, len(seqid_set_annotate0), np.mean(F1_scores_id[-1]), np.std(F1_scores_id[-1])))\n",
    "        sys.stdout.flush()\n",
    "        n += 1\n",
    "        \n",
    "    pickle.dump(F1_scores_id, open(fse, 'wb'))\n",
    "    pickle.dump(seq_order_id, open(fose, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20, 15])\n",
    "plt.boxplot(F1_scores_id)\n",
    "plt.plot(np.arange(len(F1_scores_id)), [np.mean(x) for x in F1_scores_id], color='g', marker='s')\n",
    "xticks = [10*x for x in range(int(round(len(F1_scores_id), -1)/10))]\n",
    "plt.xticks(xticks, xticks) # xticks starts from 1\n",
    "plt.ylim([0.3, 1.1])\n",
    "plt.xlabel('#query')\n",
    "plt.ylabel('F1')\n",
    "#plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random guessing probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n3 = seq_stats[seq_stats['seqLen'] == 3].shape[0]\n",
    "n4 = seq_stats[seq_stats['seqLen'] == 4].shape[0]\n",
    "n5 = seq_stats[seq_stats['seqLen'] == 5].shape[0]\n",
    "nseq = n3 + n4 + n5\n",
    "npoi = seq_all[seq_all['seqID'].isin(seqid_set_exp)]['poiID'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import comb\n",
    "from math import factorial\n",
    "prob3 = (1*comb(1, 1) + (2/3)*comb(npoi-3, 1)) / (npoi-2)\n",
    "prob4 = (1*comb(2, 2) + (3/4)*comb(2, 1)*comb(npoi-4, 1) + (2/4)*comb(npoi-4, 2)) * factorial(2) / ((npoi-2) * (npoi-3))\n",
    "prob5 = (1*comb(3, 3) + (4/5)*comb(3, 2)*comb(npoi-5, 1) + (3/5)*comb(3, 1)*comb(npoi-5, 2) + (2/5)*comb(npoi-5, 3)) * \\\n",
    "        factorial(3) / ((npoi-2) * (npoi-3) * (npoi-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randguess = prob3 * (n3 / nseq) + prob4 * (n4 / nseq) + prob5 * (n5 / nseq)\n",
    "print(randguess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare learning curves using the mean of F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20, 12])\n",
    "plt.plot(np.arange(len(F1_scores_rand)), [np.mean(x) for x in F1_scores_rand], marker='o', label='random baseline')\n",
    "plt.plot(np.arange(len(F1_scores_lc)),   [np.mean(x) for x in F1_scores_lc],   marker='d', label='least confident')\n",
    "plt.plot(np.arange(len(F1_scores_se)),   [np.mean(x) for x in F1_scores_se],   marker='s', label='sequence entropy')\n",
    "plt.plot(np.arange(len(F1_scores_id)),   [np.mean(x) for x in F1_scores_id],   marker='^', label='information density')\n",
    "plt.plot([0, len(F1_scores_rand)+1], [randguess, randguess], linestyle='--', label='random guessing')\n",
    "plt.xlim([0, len(F1_scores_rand)+1])\n",
    "plt.ylim([0.6, 0.8])\n",
    "plt.xlabel('#query')\n",
    "plt.ylabel('mean F1')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare learning curves using the median of F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20, 12])\n",
    "plt.plot(np.arange(len(F1_scores_rand)), [np.median(x) for x in F1_scores_rand], marker='o', label='random baseline')\n",
    "plt.plot(np.arange(len(F1_scores_lc)),   [np.median(x) for x in F1_scores_lc],   marker='d', label='least confident')\n",
    "plt.plot(np.arange(len(F1_scores_se)),   [np.median(x) for x in F1_scores_se],   marker='s', label='sequence entropy')\n",
    "plt.plot(np.arange(len(F1_scores_id)),   [np.median(x) for x in F1_scores_id],   marker='^', label='information density')\n",
    "plt.plot([0, len(F1_scores_rand)+1], [randguess, randguess], linestyle='--', label='random guessing')\n",
    "plt.xlim([0, len(F1_scores_rand)+1])\n",
    "plt.ylim([0.6, 0.8])\n",
    "plt.xlabel('#query')\n",
    "plt.ylabel('F1')\n",
    "plt.legend(loc='lower right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
