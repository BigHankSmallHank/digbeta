{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trajectory Recommendation using Markov Chain\n",
    "====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "Transition Features\n",
    "1. POI category (a transition matrix between different categories)\n",
    "1. POI popularity (a transition matrix between different class of popularity)\n",
    "1. POI pair distance (a transition matrix between different class of distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommend Trajectories\n",
    "Given (start, end) and trajectory length $l$ for a specific user $u$, we'll recommend a trajectory to user $u$ as follows:\n",
    "- Compute/enumerate all trajectories of length $l$ with (start, end) as candidates\n",
    "- Use a uniform prior for all candidates\n",
    "- Compute the likelihood of candidates using the above transition features estimated from travelling sequences in the training set\n",
    "- Sort candidates by their posterior probabilities (i.e. $\\text{prior} \\times \\text{likelihood}$) in descending order\n",
    "- Recommend a trajectory from the top $K$ (e.g. 5) candidates with probability proportional to its posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** that we use the same leave-one-out cross validation approach described in the [ijcai15 paper](#https://sites.google.com/site/limkwanhui/publications/2015-IJCAI-personalTour.pdf?attredirects=0), concretely, for each user $u$ in the dataset, choose one trajectory (length >= 3) from all trajectories of $u$ uniformly at random, this trajectory is used as the ground truth to measure the performance of the recommendation (i.e. compute the [precision, recall and F1-score](./ijcai15.ipynb#sec2.1)), all other trajectories are used to train/estimate parameters. So the training set will change when iterating through all users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "#from numba import jit\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nfeatures = 8 # number of features\n",
    "EPS = 1e-12 # smooth, deal with 0 probability\n",
    "random.seed(987654321) # control random choice when splitting training/testing set\n",
    "np.random.seed(987654321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/data-ijcai15'\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Osak.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Osak.csv')\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Glas.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Glas.csv')\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Edin.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Edin.csv')\n",
    "fvisit = os.path.join(data_dir, 'userVisits-Toro.csv')\n",
    "fcoord = os.path.join(data_dir, 'photoCoords-Toro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "suffix = fvisit.split('-')[-1].split('.')[0]\n",
    "fseqpart = os.path.join(data_dir, 'seqPart-' + suffix + '.pkl')\n",
    "frand = os.path.join(data_dir, 'F1-rand-' + suffix + '.pkl')\n",
    "flc = os.path.join(data_dir, 'F1-lc-' + suffix + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visits = pd.read_csv(fvisit, sep=';')\n",
    "coords = pd.read_csv(fcoord, sep=';')\n",
    "# merge data frames according to column 'photoID'\n",
    "assert(visits.shape[0] == coords.shape[0])\n",
    "traj = pd.merge(visits, coords, on='photoID')\n",
    "#traj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_photo = traj['photoID'].unique().shape[0]\n",
    "num_user = traj['userID'].unique().shape[0]\n",
    "num_poi = traj['poiID'].unique().shape[0]\n",
    "num_seq = traj['seqID'].unique().shape[0]\n",
    "pd.DataFrame({'#photo': num_photo, '#user': num_user, '#poi': num_poi, '#seq': num_seq, \\\n",
    "              '#photo/user': num_photo/num_user, '#seq/user': num_seq/num_user}, index=[str(suffix)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Compute POI Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute POI (Longitude, Latitude) as the average coordinates of the assigned photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_coords = traj[['poiID', 'photoLon', 'photoLat']].groupby('poiID').mean()\n",
    "poi_coords.reset_index(inplace=True)\n",
    "poi_coords.rename(columns={'photoLon':'poiLon', 'photoLat':'poiLat'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract POI category and visiting frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_catfreq = traj[['poiID', 'poiTheme', 'poiFreq']].groupby('poiID').first()\n",
    "poi_catfreq.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_all = pd.merge(poi_catfreq, poi_coords, on='poiID')\n",
    "poi_all.set_index('poiID', inplace=True)\n",
    "#poi_all.to_csv(fpoi, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Construct Travelling Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_all = traj[['userID', 'seqID', 'poiID', 'dateTaken']].copy().groupby(['userID', 'seqID', 'poiID'])\\\n",
    "          .agg([np.min, np.max, np.size])\n",
    "seq_all.columns = seq_all.columns.droplevel()\n",
    "seq_all.reset_index(inplace=True)\n",
    "seq_all.rename(columns={'amin':'arrivalTime', 'amax':'departureTime', 'size':'#photo'}, inplace=True)\n",
    "seq_all['poiDuration(sec)'] = seq_all['departureTime'] - seq_all['arrivalTime']\n",
    "seq_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_user = seq_all[['userID', 'seqID', 'poiID']].copy().groupby(['userID', 'seqID']).agg(np.size)\n",
    "seq_user.reset_index(inplace=True)\n",
    "seq_user.rename(columns={'poiID':'seqLen'}, inplace=True)\n",
    "seq_user.set_index('seqID', inplace=True)\n",
    "#seq_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_seq(seqid, seq_all):\n",
    "    seqi = seq_all[seq_all['seqID'] == seqid].copy()\n",
    "    seqi.sort(columns=['arrivalTime'], ascending=True, inplace=True)\n",
    "    return seqi['poiID'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compute Transition Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Basic Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\text{Pr}(\\text{Cat}_i \\to \\text{Cat}_j)$:\n",
    "   the transition probability from a POI of category $\\text{Cat}_i$ to a POI of category $\\text{Cat}_j$\n",
    "- $\\text{Pr}(\\text{Pop}_i \\to \\text{Pop}_j)$:\n",
    "   the transition probability from a POI of Popularity class $\\text{Pop}_i$ to a POI of Popularity class $\\text{Pop}_j$\n",
    "- $\\text{Pr}(\\text{Dist}_i \\to \\text{Dist}_j)$:\n",
    "   the transition probability from a POI-POI pair with distance (between the two) class $\\text{Dist}_i$ to a POI-POI pair with distance (between the two) class $\\text{Dist}_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Transition Probabilities between POI Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model transition probabilities between POI categories, i.e.\n",
    "$\\text{Pr}(\\text{Cat}_{\\text{POI}_i} \\to \\text{Cat}_{\\text{POI}_j})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the number of transition first, then normalise each row while taking care of zero by adding each cell a small number (i.e. $0.1$ times the minimum value of that row) if there exists a zero cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poi_cat_transmat(seqid_set, poi_all, seq_all):\n",
    "    poi_cats = poi_all['poiTheme'].unique().tolist()\n",
    "    poi_cats.sort()\n",
    "    poi_cat_transmat = pd.DataFrame(data=np.zeros((len(poi_cats), len(poi_cats)), dtype=np.float), \\\n",
    "                                    index=poi_cats, columns=poi_cats)\n",
    "    for seqid in seqid_set:\n",
    "        seq = extract_seq(seqid, seq_all)\n",
    "        for j in range(len(seq)-1):\n",
    "            poi1 = seq[j]\n",
    "            poi2 = seq[j+1]\n",
    "            cat1 = poi_all.loc[poi1, 'poiTheme']\n",
    "            cat2 = poi_all.loc[poi2, 'poiTheme']\n",
    "            poi_cat_transmat.loc[cat1, cat2] += 1\n",
    "    return poi_cat_transmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalise_transmat(transmat):\n",
    "    assert(isinstance(transmat, pd.DataFrame))\n",
    "    for row in range(transmat.index.shape[0]):\n",
    "        nonzeroidx = np.nonzero(transmat.iloc[row])[0].tolist()\n",
    "        if len(nonzeroidx) < transmat.columns.shape[0]:\n",
    "            minv = np.min(transmat.iloc[row, nonzeroidx])\n",
    "            EPS = 0.1 * minv  # row-specific smooth factor\n",
    "            transmat.iloc[row] += EPS\n",
    "        else: # all zero in this row\n",
    "            transmat.iloc[row] += 1  # uniform prior            \n",
    "        rowsum = np.sum(transmat.iloc[row])\n",
    "        #assert(rowsum > 0)\n",
    "        transmat.iloc[row] /= rowsum\n",
    "    return transmat        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Transition Probabilities between POI Popularity Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model transition probabilities between POI popularities, i.e.\n",
    "$\\text{Pr}(\\text{Pop}_{\\text{POI}_i} \\to \\text{Pop}_{\\text{POI}_j})$\n",
    "after discretizing POI popularities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Discretize POI Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be seen from the above plot that discretization based on equal frequency (quantiles) performs better than that based on equal width, to balance the complexity and accuracy, we choose `\"quantile, nbins=9\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poi_info(seqid_set, seq_all, poi_all):\n",
    "    poi_info = seq_all[seq_all['seqID'].isin(seqid_set)][['poiID', 'seqID']].copy()\n",
    "    poi_info = poi_info.groupby('poiID').agg(np.size)\n",
    "    poi_info.rename(columns={'seqID':'popularity'}, inplace=True)\n",
    "    #poi_info.set_index('poiID', inplace=True)\n",
    "    poi_info['poiTheme'] = poi_all.loc[poi_info.index, 'poiTheme']\n",
    "    poi_info['poiLon'] = poi_all.loc[poi_info.index, 'poiLon']\n",
    "    poi_info['poiLat'] = poi_all.loc[poi_info.index, 'poiLat']\n",
    "    # POI popularity discretization: quantile based bins\n",
    "    nbins = 9\n",
    "    quantiles = np.round(np.linspace(0, 1, nbins+1), 2)[1:-1]\n",
    "    bins_qt = [0]\n",
    "    bins_qt.extend(poi_info['popularity'].quantile(quantiles))\n",
    "    bins_qt.append(poi_info['popularity'].max() + 1)\n",
    "    poi_info['popClass'] = np.digitize(poi_info['popularity'].get_values(), bins_qt)\n",
    "    return poi_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Compute Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poi_pop_transmat(seqid_set, poi_all, seq_all):\n",
    "    pop_class = poi_all['popClass'].unique().tolist()\n",
    "    pop_class.sort()\n",
    "    poi_pop_transmat = pd.DataFrame(data=np.zeros((len(pop_class), len(pop_class)), dtype=np.float), \\\n",
    "                                    index=pop_class, columns=pop_class)\n",
    "    for seqid in seqid_set:\n",
    "        seq = extract_seq(seqid, seq_all)\n",
    "        for j in range(len(seq)-1):\n",
    "            poi1 = seq[j]\n",
    "            poi2 = seq[j+1]\n",
    "            pc1 = poi_all.loc[poi1, 'popClass']\n",
    "            pc2 = poi_all.loc[poi2, 'popClass']\n",
    "            poi_pop_transmat.loc[pc1, pc2] += 1\n",
    "    return poi_pop_transmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3.4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Transition Probabilities between POI Pair Distance Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model transition probabilities between different POI pair distances, i.e.\n",
    "$\\text{Pr}(\\text{Dist}_{\\text{POI}_{i-1} \\to \\text{POI}_i} \\to \\text{Dist}_{\\text{POI}_{i} \\to \\text{POI}_j})$\n",
    "after discretize POI pair distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Improve the distance calculation using Google maps [distance API](https://developers.google.com/maps/documentation/distance-matrix/intro) with different [travel modes](https://developers.google.com/maps/documentation/distance-matrix/intro#travel_modes) demonstrated [here](https://jakevdp.github.io/blog/2015/10/17/analyzing-pronto-cycleshare-data-with-python-and-pandas/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Discretize POI Pair Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_dist(longitude1, latitude1, longitude2, latitude2):\n",
    "    \"\"\"Calculate the distance (unit: km) between two places on earth\"\"\"\n",
    "    # convert degrees to radians\n",
    "    lon1 = math.radians(longitude1)\n",
    "    lat1 = math.radians(latitude1)\n",
    "    lon2 = math.radians(longitude2)\n",
    "    lat2 = math.radians(latitude2)\n",
    "    radius = 6371.009 # mean earth radius is 6371.009km, en.wikipedia.org/wiki/Earth_radius#Mean_radius\n",
    "    # The haversine formula, en.wikipedia.org/wiki/Great-circle_distance\n",
    "    dlon = math.fabs(lon1 - lon2)\n",
    "    dlat = math.fabs(lat1 - lat2)\n",
    "    return 2 * radius * math.asin( math.sqrt( \\\n",
    "               (math.sin(0.5*dlat))**2 + math.cos(lat1) * math.cos(lat2) * (math.sin(0.5*dlon))**2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we choose `quantile, nbins=10` to balance the complexity and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poipair_distclass_mat(poi_all):\n",
    "    poi_distmat = pd.DataFrame(data=np.zeros((poi_all.shape[0], poi_all.shape[0]), dtype=np.float), \\\n",
    "                               index=poi_all.index, columns=poi_all.index)\n",
    "    distdata = []\n",
    "    for i in range(poi_all.index.shape[0]):\n",
    "        poi1 = poi_all.index[i]\n",
    "        for j in range(i+1, poi_all.index.shape[0]):\n",
    "            poi2 = poi_all.index[j]\n",
    "            dist = calc_dist(poi_all.loc[poi1, 'poiLon'], poi_all.loc[poi1, 'poiLat'], \\\n",
    "                             poi_all.loc[poi2, 'poiLon'], poi_all.loc[poi2, 'poiLat'])\n",
    "            poi_distmat.loc[poi1, poi2] = dist\n",
    "            poi_distmat.loc[poi2, poi1] = dist\n",
    "            distdata.append(dist)\n",
    "            \n",
    "    # discretize POI pair distance: quantile based bins\n",
    "    distdata = pd.Series(distdata)\n",
    "    nbins = 10\n",
    "    quantiles = np.round(np.linspace(0, 1, nbins+1), 2)[1:-1]\n",
    "    bins_qt = [0]\n",
    "    bins_qt.extend(distdata.quantile(quantiles))\n",
    "    bins_qt.append(10*round(distdata.max()))\n",
    "    \n",
    "    poipair_distclass_mat = pd.DataFrame(data=np.zeros((poi_all.shape[0], poi_all.shape[0]), dtype=np.int), \\\n",
    "                                         index=poi_all.index, columns=poi_all.index)\n",
    "    for i in range(poi_all.index.shape[0]):\n",
    "        poi1 = poi_all.index[i]\n",
    "        for j in range(i+1, poi_all.index.shape[0]):\n",
    "            poi2 = poi_all.index[j]\n",
    "            dc = np.digitize([poi_distmat.loc[poi1, poi2]], bins_qt)[0]\n",
    "            poipair_distclass_mat.loc[poi1, poi2] = dc\n",
    "            poipair_distclass_mat.loc[poi2, poi1] = dc\n",
    "    return poipair_distclass_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Compute Transition Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use POI pair that is **observed** in dataset to compute the transition matrix between different \"class\" of distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_poipair_dist_transmat(seqid_set, seq_all, poipair_distclass_mat):\n",
    "    dist_class = [poipair_distclass_mat.iloc[x, y] \\\n",
    "                  for x in range(poipair_distclass_mat.index.shape[0]) \\\n",
    "                  for y in range(x+1, poipair_distclass_mat.index.shape[0])]\n",
    "    dist_class = np.unique(np.array(dist_class))\n",
    "    poipair_dist_transmat = pd.DataFrame(data=np.zeros((len(dist_class), len(dist_class)), dtype=np.float), \\\n",
    "                                         index=dist_class, columns=dist_class)\n",
    "    for seqid in seqid_set:\n",
    "        seq = extract_seq(seqid, seq_all)\n",
    "        if len(seq) < 3: continue\n",
    "        for j in range(1, len(seq)-1):\n",
    "            poi1 = seq[j-1]\n",
    "            poi2 = seq[j]\n",
    "            poi3 = seq[j+1]\n",
    "            dc1 = poipair_distclass_mat.loc[poi1, poi2]\n",
    "            dc2 = poipair_distclass_mat.loc[poi2, poi3]\n",
    "            poipair_dist_transmat.loc[dc1, dc2] += 1\n",
    "    return poipair_dist_transmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compute Trajectory Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log likelihood of trajectory $[\\text{POI}_1, \\text{POI}_2, \\dots, \\text{POI}_i, ..., \\text{POI}_N]$ is defined as\n",
    "\\begin{align}\n",
    "\\text{logl} =& \n",
    "\\sum_{i=1}^{N-1} \\log(\\text{Pr}(\\text{Cat}_{\\text{POI}_i} \\to \\text{Cat}_{\\text{POI}_{i+1}})) + \n",
    "\\sum_{i=1}^{N-1} \\log(\\text{Pr}(\\text{Pop}_{\\text{POI}_i} \\to \\text{Pop}_{\\text{POI}_{i+1}})) + \n",
    "\\sum_{i=2}^{N-1} \\log(\\text{Pr}(\\text{Dist}_{\\text{POI}_{i-1} \\to \\text{POI}_i} \\to \n",
    "\\text{Dist}_{\\text{POI}_{i} \\to \\text{POI}_{i+1}})) \\\\\n",
    "& + \\log(\\text{Pr}(\\text{POI}_1)) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\text{Pr}(\\text{POI}_1)$ is the prior of $\\text{POI}_1$ and we assume $\\text{Pr}(\\text{POI}_1)=1.0$, 10-based logarithm is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_seq_loglikelihood(seq, poi_all, poi_cat_transmat_log, poi_pop_transmat_log, \\\n",
    "                           poi_distclass_mat, poipair_dist_transmat_log):\n",
    "    assert(len(seq) > 1)\n",
    "    cat1 = poi_all.loc[seq[0], 'poiTheme']\n",
    "    cat2 = poi_all.loc[seq[1], 'poiTheme']\n",
    "    pc1  = poi_all.loc[seq[0], 'popClass']\n",
    "    pc2  = poi_all.loc[seq[1], 'popClass']\n",
    "    logL = poi_cat_transmat_log.loc[cat1, cat2] + poi_pop_transmat_log.loc[pc1, pc2]\n",
    "    for j in range(1, len(seq)-1):\n",
    "        poi1 = seq[j-1]\n",
    "        poi2 = seq[j]\n",
    "        poi3 = seq[j+1]\n",
    "        cat2 = poi_all.loc[poi2, 'poiTheme']\n",
    "        cat3 = poi_all.loc[poi3, 'poiTheme']\n",
    "        pc2  = poi_all.loc[poi2, 'popClass']\n",
    "        pc3  = poi_all.loc[poi3, 'popClass']\n",
    "        dc12 = poi_distclass_mat.loc[poi1, poi2]\n",
    "        dc23 = poi_distclass_mat.loc[poi2, poi3]\n",
    "        logL += poi_cat_transmat_log.loc[cat2, cat3] + poi_pop_transmat_log.loc[pc2, pc3]\n",
    "        #print(seq, dc12, dc23)\n",
    "        logL += poipair_dist_transmat_log.loc[dc12, dc23]\n",
    "    return logL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Trajectory Recommendation & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter estimation of the Markov Chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_parameter(seqid_set_training, poi_all, seq_all):\n",
    "    # compute POI info using training set\n",
    "    poi_info = calc_poi_info(seqid_set_training, seq_all, poi_all)\n",
    "    \n",
    "    # compute POI category transition matrix\n",
    "    poi_cat_transmat = calc_poi_cat_transmat(seqid_set_training, poi_info, seq_all)\n",
    "    poi_cat_transmat = normalise_transmat(poi_cat_transmat)\n",
    "    poi_cat_transmat_log = np.log10(poi_cat_transmat)\n",
    "    \n",
    "    # compute POI popularity transition matrix    \n",
    "    poi_pop_transmat = calc_poi_pop_transmat(seqid_set_training, poi_info, seq_all)\n",
    "    poi_pop_transmat = normalise_transmat(poi_pop_transmat)\n",
    "    poi_pop_transmat_log = np.log10(poi_pop_transmat)\n",
    "    \n",
    "    # compute POI pair distance transition matrix\n",
    "    poipair_distclass_mat = calc_poipair_distclass_mat(poi_info)\n",
    "    poipair_dist_transmat = calc_poipair_dist_transmat(seqid_set_training, seq_all, poipair_distclass_mat)\n",
    "    poipair_dist_transmat = normalise_transmat(poipair_dist_transmat)\n",
    "    poipair_dist_transmat_log = np.log10(poipair_dist_transmat)\n",
    "    \n",
    "    return (poi_info, poi_cat_transmat_log, poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enumerate trajectories of the same (start, end) and length (3, 4 or 5) with respect to an actual sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enum_seq345(start, end, length, poi_list): \n",
    "    assert(length in {3, 4, 5})\n",
    "    p0 = start\n",
    "    pN = end\n",
    "    \n",
    "    # enumerate sequences with length 3\n",
    "    if length == 3:\n",
    "        return [[p0, p, pN] \\\n",
    "                for p in poi_list if p not in {p0, pN}]\n",
    "    \n",
    "    # enumerate sequences with length 4\n",
    "    if length == 4:\n",
    "        return [[p0, p1, p2, pN] \\\n",
    "                for p1 in poi_list if p1 not in {p0, pN} \\\n",
    "                for p2 in poi_list if p2 not in {p0, p1, pN}]\n",
    "    \n",
    "    # enumerate sequences with length 5\n",
    "    if length == 5:\n",
    "        return [[p0, p1, p2, p3, pN] \\\n",
    "                for p1 in poi_list if p1 not in {p0, pN} \\\n",
    "                for p2 in poi_list if p2 not in {p0, p1, pN} \\\n",
    "                for p3 in poi_list if p3 not in {p0, p1, p2, pN}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_seqstr(seqstr):\n",
    "    term = re.sub('[ \\[\\]]', '', seqstr).split(',')\n",
    "    return [int(x) for x in term]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With uniform prior, the posterior probability of trajectory is proportional to its likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend_trajectory(start, end, length, poi_info, poi_cat_transmat_log, poi_pop_transmat_log, \\\n",
    "                         poipair_distclass_mat, poipair_dist_transmat_log):    \n",
    "    # enumerate trajectory\n",
    "    poi_list = poi_info.index.tolist()\n",
    "    enum_seqs = enum_seq345(start, end, length, poi_list)\n",
    "    \n",
    "    # compute log likelihood\n",
    "    #t1 = datetime.now()\n",
    "    logL = Parallel(n_jobs=-2)(delayed(calc_seq_loglikelihood)\\\n",
    "           (seq, poi_info, poi_cat_transmat_log, poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log)\\\n",
    "           for seq in enum_seqs)\n",
    "    #print('%d seconds used' % (datetime.now()-t1).total_seconds())\n",
    "    \n",
    "    # sanity check: passed\n",
    "    #seq1 = [10, 21, 28, 22]\n",
    "    #dc12 = poipair_distclass_mat.loc[seq1[0], seq1[1]]\n",
    "    #dc23 = poipair_distclass_mat.loc[seq1[1], seq1[2]]\n",
    "    #dc34 = poipair_distclass_mat.loc[seq1[2], seq1[3]]\n",
    "    #s1 = poi_cat_transmat_log.loc[poi_info.loc[seq1[0], 'poiTheme'], poi_info.loc[seq1[1], 'poiTheme']] + \\\n",
    "    #     poi_cat_transmat_log.loc[poi_info.loc[seq1[1], 'poiTheme'], poi_info.loc[seq1[2], 'poiTheme']] + \\\n",
    "    #     poi_cat_transmat_log.loc[poi_info.loc[seq1[2], 'poiTheme'], poi_info.loc[seq1[3], 'poiTheme']] + \\\n",
    "    #     poi_pop_transmat_log.loc[poi_info.loc[seq1[0], 'popClass'], poi_info.loc[seq1[1], 'popClass']] + \\\n",
    "    #     poi_pop_transmat_log.loc[poi_info.loc[seq1[1], 'popClass'], poi_info.loc[seq1[2], 'popClass']] + \\\n",
    "    #     poi_pop_transmat_log.loc[poi_info.loc[seq1[2], 'popClass'], poi_info.loc[seq1[3], 'popClass']]\n",
    "    #s2 = poipair_dist_transmat_log.loc[dc12, dc23] + poipair_dist_transmat_log.loc[dc23, dc34]\n",
    "    #logL1 = calc_seq_loglikelihood(seq1, poi_info, poi_cat_transmat_log, poi_pop_transmat_log, \\\n",
    "    #                               poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "    #print(s1+s2, '==', logL1, '?')\n",
    "    \n",
    "    # dataframe of enumerated trajectories\n",
    "    enum_seq_df = pd.DataFrame(data=np.array(logL), index=[str(x) for x in enum_seqs], columns=['logLikelihood'])\n",
    "    \n",
    "    # sort by loglikeilihood in descending order\n",
    "    enum_seq_df.sort(columns=['logLikelihood'], ascending=False, inplace=True)\n",
    "    \n",
    "    # recommend trajectory\n",
    "    probs = enum_seq_df['logLikelihood'].get_values()\n",
    "    probs = np.exp(probs)\n",
    "    probs /= np.sum(probs)\n",
    "    #enum_seq_df['probability'] = probs\n",
    "    \n",
    "    # recommend a trajectory with probability proportional to its posterior (i.e. likelihood when prior is uniform)\n",
    "    #sample = np.random.multinomial(1, probs) # catgorical/multinoulli distribution, multinomial distribution (n=1)\n",
    "    #idx = sample.nonzero()[0][0]\n",
    "    #return (parse_seqstr(enum_seq_df.index[idx]), probs[idx]) # return the recommended sequence and its probability\n",
    "    \n",
    "    # recommend the topk trajectories with their probabilities\n",
    "    k = 10\n",
    "    if enum_seq_df.shape[0] < k: \n",
    "        k = enum_seq_df.shape[0]\n",
    "    result_df = enum_seq_df.iloc[0:k].copy()\n",
    "    result_df.reset_index(inplace=True)\n",
    "    result_df.rename(columns={'index':'sequence'}, inplace=True)\n",
    "    result_df.drop('logLikelihood', axis=1, inplace=True)\n",
    "    result_df['probability'] = probs[0:k]\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use F1 measure defined [here](./ijcai15.ipynb#sec2.1) to evaluate the performance of recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_F1score(seq_act, seq_rec):\n",
    "    assert(len(seq_act) > 0)\n",
    "    assert(len(seq_rec) > 0)\n",
    "    actset = set(seq_act)\n",
    "    recset = set(seq_rec)\n",
    "    intersect = actset & recset\n",
    "    recall = len(intersect) / len(seq_act)\n",
    "    precision = len(intersect) / len(seq_rec)\n",
    "    F1score = 2. * precision * recall / (precision + recall)\n",
    "    return F1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(seqid_set, poi_all, seq_all, poi_cat_transmat_log, poi_pop_transmat_log, \\\n",
    "             poipair_distclass_mat, poipair_dist_transmat_log):\n",
    "    F1scores = []\n",
    "    for seqid in seqid_set:\n",
    "        seq_act = extract_seq(seqid, seq_all)\n",
    "        seq_rec_df = recommend_trajectory(seq_act[0], seq_act[-1], len(seq_act), poi_all, poi_cat_transmat_log, \\\n",
    "                                          poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "        seq_rec = parse_seqstr(seq_rec_df.iloc[0]['sequence']) # choose the one with max posterior\n",
    "        F1 = calc_F1score(seq_act, seq_rec)\n",
    "        F1scores.append(F1)\n",
    "        #print('Actual: %-23s  Recommended: %-23s F1: %.2f' % (str(seq_act), str(seq_rec), F1)); sys.stdout.flush()\n",
    "    return F1scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Active Learning without Personalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $Traj$: a trajectory\n",
    "- $|Traj|$: the number of POIs in trajectory $Traj$\n",
    "- $P_s$: the `start` (first) POI of a trajectory\n",
    "- $P_e$: the `end` (last) POI of a trajectory\n",
    "- $T_s$: the earlist start time of a trajectory\n",
    "- $T_e$: the latest end time of a trajectory\n",
    "- $u$: a specific user\n",
    "- $\\textbf{x}$: an example\n",
    "- $\\textbf{y}$: label of an example $\\textbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example: $\\textbf{x} = (P_s, P_e, T_s, T_e, |Traj|)$\n",
    "- Label of Example: $\\textbf{y} = Traj$\n",
    "- Assume that all examples are i.i.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec5.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Travelling Sequences Dataset Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset (travelling sequences with length 3/4/5) into \n",
    "- training set (i.e. initial training) ~~(10%)~~ (40%), \n",
    "- annotation set (i.e. simulate user annotation)~~(50%)~~ (40%), \n",
    "- evaluation set (a.k.a. test set) ~~(40%)~~ (20%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**:\n",
    "- when the initial training set is too small such that the travelling sequences in training set can not span all POIs in annotation/evaluation set, then the annotation/evaluation will fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_stats = seq_all[['userID', 'seqID', 'poiID']].copy().groupby(['userID', 'seqID']).agg(np.size)\n",
    "seq_stats.reset_index(inplace=True)\n",
    "seq_stats.rename(columns={'poiID':'seqLen'}, inplace=True)\n",
    "seq_stats.set_index('seqID', inplace=True)\n",
    "print(seq_stats.index.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seqid_set_exp = seq_stats[seq_stats['seqLen'].isin({3, 4, 5})].index\n",
    "print(len(seqid_set_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a random portion of sequences as training data that span all POIs in annotation/evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_training0 = []\n",
    "seqid_set_annotate0 = []\n",
    "seqid_set_evaluate0 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doCompute = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(fseqpart):\n",
    "    doCompute = False\n",
    "    (seqid_set_training0, seqid_set_annotate0, seqid_set_evaluate0) = pickle.load(open(fseqpart, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doCompute:\n",
    "    idx_range = np.arange(len(seqid_set_exp))\n",
    "    num_train = round(len(seqid_set_exp) * 0.3)\n",
    "    num_annotate = round(len(seqid_set_exp) * 0.5)\n",
    "    poi_coverage_ok = False\n",
    "    for i in range(200):\n",
    "        np.random.shuffle(idx_range) \n",
    "        seqid_set_training0 = list(seqid_set_exp[idx_range[0:num_train]])\n",
    "        seqid_set_annotate0 = list(seqid_set_exp[idx_range[num_train:num_train+num_annotate]])\n",
    "        seqid_set_evaluate0 = list(seqid_set_exp[idx_range[num_train+num_annotate:]])\n",
    "        poi_set_training = {poi for seqid in seqid_set_training0 for poi in extract_seq(seqid, seq_all)}\n",
    "        poi_set_annotate = {poi for seqid in seqid_set_annotate0 for poi in extract_seq(seqid, seq_all)}\n",
    "        poi_set_evaluate = {poi for seqid in seqid_set_evaluate0 for poi in extract_seq(seqid, seq_all)}\n",
    "        #print(len(poi_set_training), len(poi_set_annotate), len(poi_set_evaluate))\n",
    "        #for seqid in seqid_set_training0: poi_set_training = poi_set_training | set(extract_seq(seqid, seq_all))\n",
    "        if len(poi_set_training & poi_set_annotate) == len(poi_set_annotate) and \\\n",
    "           len(poi_set_training & poi_set_evaluate) == len(poi_set_evaluate):\n",
    "            poi_coverage_ok = True\n",
    "            pickle.dump((seqid_set_training0, seqid_set_annotate0, seqid_set_evaluate0), open(fseqpart, 'wb'))\n",
    "            break\n",
    "    if poi_coverage_ok == False:\n",
    "        print('Failed to find a training set of sequences that span all POIs in annotation/evaluation set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(seqid_set_exp))\n",
    "print(len(seqid_set_training0))\n",
    "print(len(seqid_set_annotate0))\n",
    "print(len(seqid_set_evaluate0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Random Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query strategy: choose a random example in annotation set to query (passive learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of training set and annotation set and use the copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_training = list(seqid_set_training0)\n",
    "seqid_set_annotate = list(seqid_set_annotate0)\n",
    "seqid_set_evaluate = list(seqid_set_evaluate0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_order = np.arange(len(seqid_set_annotate))\n",
    "np.random.shuffle(query_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1_mean_rand = []\n",
    "F1_std_rand = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doCompute = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(frand):\n",
    "    doCompute = False\n",
    "    F1_rand = pickle.load(open(frand, 'rb'))\n",
    "    assert(F1_rand.shape[0] == 2)\n",
    "    F1_mean_rand = F1_rand[0]\n",
    "    F1_std_rand = F1_rand[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate parameters using the initial training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if doCompute:\n",
    "    (poi_info, poi_cat_transmat_log, poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log) = \\\n",
    "    estimate_parameter(seqid_set_training, poi_all, seq_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: evaluate using the first example in test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doCompute:\n",
    "    seqid = seqid_set_evaluate[0]\n",
    "    seq_act = extract_seq(seqid, seq_all)\n",
    "    seq_rec_df = recommend_trajectory(seq_act[0], seq_act[-1], len(seq_act), poi_info, poi_cat_transmat_log, \\\n",
    "                                      poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "    seq_rec = parse_seqstr(seq_rec_df.iloc[0]['sequence']) # choose the one with max posterior\n",
    "    F1 = calc_F1score(seq_act, seq_rec) \n",
    "    print('Actual: %-23s  Recommended: %-23s F1: %.2f' % (str(seq_act), str(seq_rec), F1))\n",
    "    seq_rec_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend trajectories using the initial training set and evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doCompute:\n",
    "    F1scores = evaluate(seqid_set_evaluate, poi_info, seq_all, poi_cat_transmat_log, poi_pop_transmat_log, \\\n",
    "                        poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "    F1_mean_rand.append(np.mean(F1scores))\n",
    "    F1_std_rand.append(np.std(F1scores))\n",
    "    print('Evaluation 0, F1 mean: %.2f, std: %.2f' % (F1_mean_rand[-1], F1_std_rand[-1])); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the baseline on test set using random query strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doCompute:\n",
    "    n = 1\n",
    "    for idx in query_order:\n",
    "        # choose sequence to query\n",
    "        seqid = seqid_set_annotate[idx]\n",
    "        \n",
    "        # add query result to training set, here just add the sequence id to training set\n",
    "        seqid_set_training.append(seqid)\n",
    "        \n",
    "        # estimate parameters using current training set\n",
    "        (poi_info, poi_cat_transmat_log, poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log) = \\\n",
    "        estimate_parameter(seqid_set_training, poi_all, seq_all)\n",
    "        \n",
    "        # evaluate on test set\n",
    "        F1scores = evaluate(seqid_set_evaluate, poi_info, seq_all, poi_cat_transmat_log, poi_pop_transmat_log, \\\n",
    "                            poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "        F1_mean_rand.append(np.mean(F1scores))\n",
    "        F1_std_rand.append(np.std(F1scores))\n",
    "        print('Evaluation %-2d/%d, F1 mean: %.2f, std: %.2f' % \\\n",
    "              (n, len(seqid_set_annotate0), F1_mean_rand[-1], F1_std_rand[-1])); sys.stdout.flush();\n",
    "        \n",
    "        n += 1\n",
    "        \n",
    "    pickle.dump(np.array([F1_mean_rand, F1_std_rand]), open(frand, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 10])\n",
    "#plt.plot(np.arange(len(seqid_set_annotate)+1), F1_random, marker='s', color='g', linestyle='--', label='random baseline')\n",
    "plt.errorbar(np.arange(len(seqid_set_annotate0)+1), F1_mean_rand, yerr=F1_std_rand, color='g', \\\n",
    "             linestyle='--', marker='s', label='random baseline')\n",
    "plt.xlabel('#query')\n",
    "plt.ylabel('F1')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Least Confident Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Strategy:\n",
    "\\begin{equation}\n",
    "\\phi^{LC}(\\textbf{x}) = 1 - P(\\textbf{y}^* | \\textbf{x}; \\Theta)\n",
    "\\end{equation}\n",
    "where $\\textbf{y}^*$ is the most likely label of example $\\textbf{x}$ with respect to a probabilistic model of which the parameters are denoted by $\\Theta$.  \n",
    "This query strategy select the example $\\textbf{x}$ of maximum $\\phi^{LC}$ from all unlabelled examples in a pool to query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**:\n",
    "- This query strategy biases to sequences with more POIs, as the number of candidate trajectories grows exponentially as the number of POIs specified, the probability of the most likely candidate trajectory becomes smaller and smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of training set and annotation set and use the copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_training = list(seqid_set_training0)\n",
    "seqid_set_annotate = list(seqid_set_annotate0)\n",
    "seqid_set_evaluate = list(seqid_set_evaluate0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1_mean_lc = []\n",
    "F1_std_lc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doCompute = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(flc):\n",
    "    doCompute = False\n",
    "    F1_lc = pickle.load(open(flc, 'rb'))\n",
    "    assert(F1_lc.shape[0] == 2)\n",
    "    F1_mean_lc = F1_lc[0]\n",
    "    F1_std_lc = F1_lc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend trajectories using the initial training set and evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doCompute:\n",
    "    (poi_info, poi_cat_transmat_log, poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log) = \\\n",
    "    estimate_parameter(seqid_set_training, poi_all, seq_all)\n",
    "    F1scores = evaluate(seqid_set_evaluate, poi_info, seq_all, poi_cat_transmat_log, poi_pop_transmat_log, \\\n",
    "                        poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "    F1_mean_lc.append(np.mean(F1scores))\n",
    "    F1_std_lc.append(np.std(F1scores))\n",
    "    print('Evaluation 0, F1 mean: %.2f, std: %.2f' % (F1_mean_lc[-1], F1_std_lc[-1])); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the baseline on test set using least confident query strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doCompute:\n",
    "    n = 1\n",
    "    seqs_annotate = [extract_seq(seqid, seq_all) for seqid in seqid_set_annotate]\n",
    "    # repeat until all sequences in annotation set have been queried \n",
    "    while len(seqid_set_annotate) > 0:\n",
    "        # choose sequence to query\n",
    "        seq_idx = 0\n",
    "        max_lc = 0\n",
    "        for idx in range(len(seqid_set_annotate)):\n",
    "            seq_act = seqs_annotate[idx]\n",
    "            seq_rec_df = recommend_trajectory(seq_act[0], seq_act[-1], len(seq_act), poi_info, poi_cat_transmat_log, \\\n",
    "                                              poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "            lc = 1 - seq_rec_df.iloc[0]['probability']\n",
    "            if lc > max_lc:\n",
    "                max_lc = lc\n",
    "                seq_idx = idx\n",
    "\n",
    "        # add query result to training set, here just add the sequence id to training set\n",
    "        seqid_set_training.append(seqid_set_annotate[seq_idx])\n",
    "        print('Choose sequence', seqs_annotate[seq_idx])\n",
    "\n",
    "        # remove the selected example from annotation set\n",
    "        del seqid_set_annotate[seq_idx]\n",
    "        del seqs_annotate[seq_idx]\n",
    "\n",
    "        # estimate parameters using the updated training set\n",
    "        (poi_info, poi_cat_transmat_log, poi_pop_transmat_log, poipair_distclass_mat, poipair_dist_transmat_log) = \\\n",
    "        estimate_parameter(seqid_set_training, poi_all, seq_all)\n",
    "\n",
    "        # evaluate on test set\n",
    "        F1scores = evaluate(seqid_set_evaluate, poi_info, seq_all, poi_cat_transmat_log, poi_pop_transmat_log, \\\n",
    "                            poipair_distclass_mat, poipair_dist_transmat_log)\n",
    "        F1_mean_lc.append(np.mean(F1scores))\n",
    "        F1_std_lc.append(np.std(F1scores))\n",
    "        print('Evaluation %-2d/%d, F1 mean: %.2f, std: %.2f' % \\\n",
    "              (n, len(seqid_set_annotate0), F1_mean_lc[-1], F1_std_lc[-1])); sys.stdout.flush();\n",
    "\n",
    "        n += 1\n",
    "    pickle.dump(np.array([F1_mean_lc, F1_std_lc]), open(flc, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 10])\n",
    "plt.errorbar(np.arange(len(seqid_set_annotate0)+1), F1_mean_rand, yerr=F1_std_rand, \\\n",
    "             linestyle='--', marker='s', label='random baseline')\n",
    "plt.errorbar(np.arange(len(seqid_set_annotate0)+1), F1_mean_lc, yerr=F1_std_lc, \\\n",
    "             linestyle='--', marker='s', label='least confident')\n",
    "plt.xlabel('#query')\n",
    "plt.ylabel('F1')\n",
    "plt.legend(loc='lower right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
