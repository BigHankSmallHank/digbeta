{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trajectory Recommendation using RankSVM and Structured Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Contents](#toc)\n",
    "1. [Preprocess Dataset](#sec1)\n",
    "  1. [Load Data](#sec1.1)\n",
    "  1. [Utility function](#sec1.2)\n",
    "1. [POI Ranking](#sec2)\n",
    "  1. [POI Features for Ranking](#sec2.1)\n",
    "  1. [Training DataFrame](#sec2.2)\n",
    "  1. [Test DataFrame](#sec2.3)\n",
    "  1. [Ranking POIs using rankSVM](#sec2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import os, sys, time, pickle, tempfile\n",
    "import math, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import seaborn as sns\n",
    "\n",
    "from pystruct.models import ChainCRF\n",
    "from pystruct.learners import OneSlackSSVM\n",
    "from pystruct.learners import FrankWolfeSSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(987654321) # control random choice when splitting training/testing set\n",
    "np.random.seed(987654321)\n",
    "ranksvm_dir = '$HOME/work/ranksvm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/data-recsys16'\n",
    "suffix = ['Osak', 'Glas', 'Edin', 'Toro', 'Melb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_ix = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasettypes = ['all', 'noshort', 'nofew']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dstype = datasettypes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uspecific = False\n",
    "KX = 100  # KX folds in user specific setting [100, 50, 20, 10, 8, 4, 2, 1]\n",
    "kxstr = str(KX) + 'X-'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method switches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_crf = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noloop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpoi = os.path.join(data_dir, 'poi-' + suffix[dat_ix] + '.csv')\n",
    "if noloop == True:\n",
    "    ftraj = os.path.join(data_dir, 'traj-noloop-' + dstype + '-' + suffix[dat_ix] + '.csv')\n",
    "else:\n",
    "    ftraj = os.path.join(data_dir, 'traj-' + dstype + '-' + suffix[dat_ix] + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>trajID</th>\n",
       "      <th>poiID</th>\n",
       "      <th>startTime</th>\n",
       "      <th>endTime</th>\n",
       "      <th>#photo</th>\n",
       "      <th>trajLen</th>\n",
       "      <th>poiDuration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10063645@N00</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1160716026</td>\n",
       "      <td>1160716026</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10063645@N00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1162247008</td>\n",
       "      <td>1162247008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10063645@N00</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1162333588</td>\n",
       "      <td>1162333588</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10063645@N00</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>1162693595</td>\n",
       "      <td>1162693595</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10063645@N00</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>1162693817</td>\n",
       "      <td>1162693829</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         userID  trajID  poiID   startTime     endTime  #photo  trajLen  \\\n",
       "0  10063645@N00       1     11  1160716026  1160716026       1        1   \n",
       "1  10063645@N00       2      2  1162247008  1162247008       1        1   \n",
       "2  10063645@N00       3     11  1162333588  1162333588       1        1   \n",
       "3  10063645@N00       4     16  1162693595  1162693595       1        2   \n",
       "4  10063645@N00       4     21  1162693817  1162693829       2        2   \n",
       "\n",
       "   poiDuration  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4           12  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_all = pd.read_csv(ftraj)\n",
    "traj_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poiCat</th>\n",
       "      <th>poiLon</th>\n",
       "      <th>poiLat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poiID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transport</td>\n",
       "      <td>-4.258101</td>\n",
       "      <td>55.857920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transport</td>\n",
       "      <td>-4.250728</td>\n",
       "      <td>55.861496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transport</td>\n",
       "      <td>-4.595632</td>\n",
       "      <td>55.509388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transport</td>\n",
       "      <td>-4.331517</td>\n",
       "      <td>55.868897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transport</td>\n",
       "      <td>-4.269905</td>\n",
       "      <td>55.856135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          poiCat    poiLon     poiLat\n",
       "poiID                                \n",
       "1      Transport -4.258101  55.857920\n",
       "2      Transport -4.250728  55.861496\n",
       "4      Transport -4.595632  55.509388\n",
       "5      Transport -4.331517  55.868897\n",
       "6      Transport -4.269905  55.856135"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_all = pd.read_csv(fpoi)\n",
    "poi_all.set_index('poiID', inplace=True)\n",
    "poi_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#poi</th>\n",
       "      <th>#traj</th>\n",
       "      <th>#traj/user</th>\n",
       "      <th>#user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Glas</th>\n",
       "      <td>27</td>\n",
       "      <td>2227</td>\n",
       "      <td>3.705491</td>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      #poi  #traj  #traj/user  #user\n",
       "Glas    27   2227    3.705491    601"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_user = traj_all['userID'].unique().shape[0]\n",
    "num_poi = traj_all['poiID'].unique().shape[0]\n",
    "num_traj = traj_all['trajID'].unique().shape[0]\n",
    "#assert(num_poi == poi_all.shape[0])\n",
    "pd.DataFrame({'#user': num_user, '#poi': num_poi, '#traj': num_traj, '#traj/user': num_traj/num_user}, \\\n",
    "             index=[str(suffix[dat_ix])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFYCAYAAABpkTT0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff6de6f7b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = traj_all['trajLen'].hist(bins=20)\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping trajectory to user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trajID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10063645@N00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10063645@N00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10063645@N00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10063645@N00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10063645@N00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              userID\n",
       "trajID              \n",
       "1       10063645@N00\n",
       "2       10063645@N00\n",
       "3       10063645@N00\n",
       "4       10063645@N00\n",
       "5       10063645@N00"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_user = traj_all[['trajID', 'userID']].copy().groupby('trajID').first()\n",
    "traj_user.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Utility function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print computing progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_progress(cnt, total):\n",
    "    \"\"\"Display a progress bar\"\"\"\n",
    "    assert(cnt > 0 and total > 0 and cnt <= total)\n",
    "    length = 80\n",
    "    ratio = cnt / total\n",
    "    n = int(length * ratio)\n",
    "    sys.stdout.write('\\r[%-80s] %d%%' % ('-'*n, int(ratio*100)))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract trajectory, i.e., a list of POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_traj(tid, traj_all):\n",
    "    traj = traj_all[traj_all['trajID'] == tid].copy()\n",
    "    traj.sort_values(by=['startTime'], ascending=True, inplace=True)\n",
    "    return traj['poiID'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute POI properties, e.g., popularity, total number of visit, average visit duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poi_info(trajid_list, traj_all, poi_all):\n",
    "    assert(len(trajid_list) > 0)\n",
    "    # to allow duplicated trajid\n",
    "    poi_info = traj_all[traj_all['trajID'] == trajid_list[0]][['poiID', 'poiDuration']].copy() \n",
    "    for i in range(1, len(trajid_list)):\n",
    "        traj = traj_all[traj_all['trajID'] == trajid_list[i]][['poiID', 'poiDuration']]\n",
    "        poi_info = poi_info.append(traj, ignore_index=True)\n",
    "    \n",
    "    poi_info = poi_info.groupby('poiID').agg([np.mean, np.size])\n",
    "    poi_info.columns = poi_info.columns.droplevel()\n",
    "    poi_info.reset_index(inplace=True)\n",
    "    poi_info.rename(columns={'mean':'avgDuration', 'size':'nVisit'}, inplace=True)\n",
    "    poi_info.set_index('poiID', inplace=True) \n",
    "    poi_info['poiCat'] = poi_all.loc[poi_info.index, 'poiCat']\n",
    "    poi_info['poiLon'] = poi_all.loc[poi_info.index, 'poiLon']\n",
    "    poi_info['poiLat'] = poi_all.loc[poi_info.index, 'poiLat']\n",
    "    \n",
    "    # POI popularity: the number of distinct users that visited the POI\n",
    "    pop_df = traj_all[traj_all['trajID'].isin(trajid_list)][['poiID', 'userID']].copy()\n",
    "    pop_df = pop_df.groupby('poiID').agg(pd.Series.nunique)\n",
    "    pop_df.rename(columns={'userID':'nunique'}, inplace=True)\n",
    "    poi_info['popularity'] = pop_df.loc[poi_info.index, 'nunique']\n",
    "    \n",
    "    return poi_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the F1 score for recommended trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_F1(seq_act, seq_rec):\n",
    "    '''Compute recall, precision and F1 when trajectories contain sub-tours'''\n",
    "    assert(len(seq_act) > 0)\n",
    "    assert(len(seq_rec) > 0)\n",
    "    #actset = set(seq_act)\n",
    "    #recset = set(seq_rec)\n",
    "    #intersect = actset & recset\n",
    "    \n",
    "    match_tags = np.zeros(len(seq_act), dtype=np.bool)\n",
    "    for poi in seq_rec:\n",
    "        for j in range(len(seq_act)):\n",
    "            if match_tags[j] == False and poi == seq_act[j]:\n",
    "                match_tags[j] = True\n",
    "                break\n",
    "    intersize = np.nonzero(match_tags)[0].shape[0]\n",
    "    recall = intersize / len(seq_act)\n",
    "    precision = intersize / len(seq_rec)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute distance between two POIs using [Haversine formula](http://en.wikipedia.org/wiki/Great-circle_distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_dist_vec(longitudes1, latitudes1, longitudes2, latitudes2):\n",
    "    \"\"\"Calculate the distance (unit: km) between two places on earth, vectorised\"\"\"\n",
    "    # convert degrees to radians\n",
    "    lng1 = np.radians(longitudes1)\n",
    "    lat1 = np.radians(latitudes1)\n",
    "    lng2 = np.radians(longitudes2)\n",
    "    lat2 = np.radians(latitudes2)\n",
    "    radius = 6371.0088 # mean earth radius, en.wikipedia.org/wiki/Earth_radius#Mean_radius\n",
    "\n",
    "    # The haversine formula, en.wikipedia.org/wiki/Great-circle_distance\n",
    "    dlng = np.fabs(lng1 - lng2)\n",
    "    dlat = np.fabs(lat1 - lat2)\n",
    "    dist =  2 * radius * np.arcsin( np.sqrt( \n",
    "                (np.sin(0.5*dlat))**2 + np.cos(lat1) * np.cos(lat2) * (np.sin(0.5*dlng))**2 ))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance between POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_distmat = pd.DataFrame(data=np.zeros((poi_all.shape[0], poi_all.shape[0]), dtype=np.float), \\\n",
    "                           index=poi_all.index, columns=poi_all.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ix in poi_all.index:\n",
    "    poi_distmat.loc[ix] = calc_dist_vec(poi_all.loc[ix, 'poiLon'], \\\n",
    "                                        poi_all.loc[ix, 'poiLat'], \\\n",
    "                                        poi_all['poiLon'], \\\n",
    "                                        poi_all['poiLat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a *query* (in IR terminology) using tuple (start POI, end POI, #POI) ~~user ID.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trajid_set_all = sorted(traj_all['trajID'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_id_dict = dict()  # (start, end, length) --> qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trajs = [extract_traj(tid, traj_all) for tid in trajid_set_all]\n",
    "keys = [(t[0], t[-1], len(t)) for t in trajs if len(t) > 2]\n",
    "cnt = 0\n",
    "for key in keys:\n",
    "    if key not in query_id_dict:   # (start, end, length) --> qid\n",
    "        query_id_dict[key] = cnt\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avgDuration</th>\n",
       "      <th>nVisit</th>\n",
       "      <th>poiCat</th>\n",
       "      <th>poiLon</th>\n",
       "      <th>poiLat</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poiID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1992.073770</td>\n",
       "      <td>244</td>\n",
       "      <td>Transport</td>\n",
       "      <td>-4.258101</td>\n",
       "      <td>55.857920</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1222.623656</td>\n",
       "      <td>279</td>\n",
       "      <td>Transport</td>\n",
       "      <td>-4.250728</td>\n",
       "      <td>55.861496</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Transport</td>\n",
       "      <td>-4.595632</td>\n",
       "      <td>55.509388</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>Transport</td>\n",
       "      <td>-4.331517</td>\n",
       "      <td>55.868897</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>499.500000</td>\n",
       "      <td>20</td>\n",
       "      <td>Transport</td>\n",
       "      <td>-4.269905</td>\n",
       "      <td>55.856135</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       avgDuration  nVisit     poiCat    poiLon     poiLat  popularity\n",
       "poiID                                                                 \n",
       "1      1992.073770     244  Transport -4.258101  55.857920         127\n",
       "2      1222.623656     279  Transport -4.250728  55.861496         181\n",
       "4         0.000000       1  Transport -4.595632  55.509388           1\n",
       "5         0.000000       2  Transport -4.331517  55.868897           2\n",
       "6       499.500000      20  Transport -4.269905  55.856135          16"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_info_ = calc_poi_info(trajid_set_all, traj_all, poi_all)\n",
    "poi_info_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#traj in total: 2227\n",
      "#traj (length > 2): 112\n",
      "#query tuple: 97\n"
     ]
    }
   ],
   "source": [
    "print('#traj in total:', len(trajid_set_all))\n",
    "print('#traj (length > 2):', traj_all[traj_all['trajLen'] > 2]['trajID'].unique().shape[0])\n",
    "print('#query tuple:', len(query_id_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. POI Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 POI Features for Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI Features used for ranking:\n",
    "1. `popularity`: POI popularity, i.e., the number of distinct users that visited the POI\n",
    "1. `nVisit`: the total number of visit by all users\n",
    "1. `avgDuration`: average POI visit duration\n",
    "1. `sameCatStart`: 1 if POI category is the same as that of `startPOI`, -1 otherwise\n",
    "1. `sameCatEnd`: 1 if POI category is the same as that of `endPOI`, -1 otherwise\n",
    "1. `distStart`: distance (haversine formula) from `startPOI`\n",
    "1. `distEnd`: distance from `endPOI`\n",
    "1. `seqLen`: trajectory length (copy from query)\n",
    "1. `diffPopStart`: difference in POI popularity from `startPOI`\n",
    "1. `diffPopEnd`: difference in POI popularity from `endPOI`\n",
    "1. `diffNVisitStart`: difference in the total number of visit from `startPOI`\n",
    "1. `diffNVisitEnd`: difference in the total number of visit from `endPOI`\n",
    "1. `diffDurationStart`: difference in average POI visit duration from the actual duration spent at `startPOI`\n",
    "1. `diffDurationEnd`: difference in average POI visit duration from the actual duration spent at `endPOI`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_columns = ['poiID', 'label', 'queryID', 'popularity', 'nVisit', 'avgDuration', \\\n",
    "              'sameCatStart', 'sameCatEnd', 'distStart', 'distEnd', 'trajLen', 'diffPopStart', \\\n",
    "              'diffPopEnd', 'diffNVisitStart', 'diffNVisitEnd', 'diffDurationStart', 'diffDurationEnd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Features aggregated from a number of trajectories:~~\n",
    "~~1. Compute POI `popularity` and average visit `duration` using all trajectories from training and querying set,~~\n",
    "~~1. Use the same features that computed above for the test set, except the distance based features.~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data are generated as follows:\n",
    "1. each input tuple $(\\text{startPOI}, \\text{endPOI}, \\text{#POI})$ form a `query` (in IR terminology).\n",
    "1. the label of a specific POI is the number of presence of that POI in a specific `query`, excluding the presence as $\\text{startPOI}$ or $\\text{endPOI}$.\n",
    "1. for each `query`, the label of all absence POIs from trajectories of that `query` in training set got a label 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of training data matrix is `#(qid, poi)` by `#feature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_train_subdf(poi_id, query_id_set, columns, poi_info, poi_distmat, query_id_rdict):\n",
    "    df_ = pd.DataFrame(data=np.zeros((len(query_id_set), len(columns)), dtype=np.float), columns=columns)\n",
    "    \n",
    "    pop = poi_info.loc[poi_id, 'popularity']; nvisit = poi_info.loc[poi_id, 'nVisit']\n",
    "    cat = poi_info.loc[poi_id, 'poiCat']; duration = poi_info.loc[poi_id, 'avgDuration']\n",
    "    \n",
    "    for j in range(len(query_id_set)):\n",
    "        qid = query_id_set[j]\n",
    "        assert(qid in query_id_rdict) # qid --> (start, end, length)\n",
    "        (p0, pN, trajLen) = query_id_rdict[qid]\n",
    "        idx = df_.index[j]\n",
    "        df_.loc[idx, 'poiID'] = poi_id\n",
    "        df_.loc[idx, 'queryID'] = qid\n",
    "        df_.loc[idx, 'popularity'] = pop\n",
    "        df_.loc[idx, 'nVisit'] = nvisit\n",
    "        df_.loc[idx, 'avgDuration'] = duration\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_info.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'sameCatEnd']   = 1 if cat == poi_info.loc[pN, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi_id, p0]\n",
    "        df_.loc[idx, 'distEnd']   = poi_distmat.loc[poi_id, pN]\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffPopEnd']   = pop - poi_info.loc[pN, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffNVisitEnd']   = nvisit - poi_info.loc[pN, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'diffDurationEnd']   = duration - poi_info.loc[pN, 'avgDuration']\n",
    "        \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_train_df(trajid_list, columns, traj_all, poi_info, poi_distmat, query_id_dict):\n",
    "    columns = columns.copy()\n",
    "    train_trajs = [extract_traj(tid, traj_all) for tid in trajid_list]\n",
    "    \n",
    "    qid_set = sorted(set([query_id_dict[(t[0], t[-1], len(t))] for t in train_trajs if len(t) > 2]))\n",
    "    poi_set = sorted(set(poi_info.index.tolist()))\n",
    "    \n",
    "    #qid_poi_pair = list(itertools.product(qid_set, poi_set)) # Cartesian product of qid_set and poi_set\n",
    "    #df_ = pd.DataFrame(data=np.zeros((len(qid_poi_pair), len(columns)), dtype= np.float), columns=columns)\n",
    "    \n",
    "    query_id_rdict = dict()\n",
    "    for k, v in query_id_dict.items(): \n",
    "        query_id_rdict[v] = k  # qid --> (start, end, length)\n",
    "    \n",
    "    train_df_list = Parallel(n_jobs=4)\\\n",
    "                            (delayed(gen_train_subdf)(poi, qid_set, columns, poi_info, poi_distmat, query_id_rdict) \\\n",
    "                             for poi in poi_set)\n",
    "                        \n",
    "    assert(len(train_df_list) > 0)\n",
    "    df_ = train_df_list[0]\n",
    "    for j in range(1, len(train_df_list)):\n",
    "        df_ = df_.append(train_df_list[j], ignore_index=True)            \n",
    "        \n",
    "    # set label\n",
    "    df_.set_index(['queryID', 'poiID'], inplace=True)\n",
    "    for t in train_trajs:\n",
    "        if len(t) > 2:\n",
    "            qid = query_id_dict[(t[0], t[-1], len(t))]\n",
    "            for poi in t[1:-1]:  # do NOT count if the POI is startPOI/endPOI\n",
    "                df_.loc[(qid, poi), 'label'] += 1\n",
    "    \n",
    "    df_.reset_index(inplace=True)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: \n",
    "- different POIs have different features for the same query trajectory\n",
    "- the same POI get different features for different query-id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Test DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data are generated the same way as training data, except that the labels of testing data (unknown) could be arbitrary values as suggested in [libsvm FAQ](http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#f431).\n",
    "The reported accuracy (by `svm-predict` command) is meaningless as it is calculated based on these labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of training data matrix is `#poi` by `#feature` with one specific `query`, i.e. tuple $(\\text{startPOI}, \\text{endPOI}, \\text{#POI})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_test_df(startPOI, endPOI, nPOI, columns, poi_info, poi_distmat, query_id_dict):\n",
    "    columns = columns.copy()\n",
    "    key = (p0, pN, trajLen) = (startPOI, endPOI, nPOI)\n",
    "    assert(key in query_id_dict)\n",
    "    assert(p0 in poi_info.index)\n",
    "    assert(pN in poi_info.index)\n",
    "    \n",
    "    poi_set = sorted(set(poi_info.index.tolist()))\n",
    "    df_ = pd.DataFrame(data=np.zeros((len(poi_set), len(columns)), dtype= np.float), columns=columns)\n",
    "    \n",
    "    qid = query_id_dict[key]\n",
    "    df_['queryID'] = qid\n",
    "    df_['label'] = np.random.rand(df_.shape[0]) # label for test data is arbitrary according to libsvm FAQ\n",
    "\n",
    "    for i in range(df_.index.shape[0]):\n",
    "        poi = poi_set[i]\n",
    "        lon = poi_info.loc[poi, 'poiLon']; lat = poi_info.loc[poi, 'poiLat']\n",
    "        pop = poi_info.loc[poi, 'popularity']; nvisit = poi_info.loc[poi, 'nVisit']\n",
    "        cat = poi_info.loc[poi, 'poiCat']; duration = poi_info.loc[poi, 'avgDuration']\n",
    "        idx = df_.index[i]\n",
    "        df_.loc[idx, 'poiID'] = poi \n",
    "        df_.loc[idx, 'popularity'] = pop\n",
    "        df_.loc[idx, 'nVisit'] = nvisit\n",
    "        df_.loc[idx, 'avgDuration'] = duration\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_all.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'sameCatEnd']   = 1 if cat == poi_all.loc[pN, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi, p0]\n",
    "        df_.loc[idx, 'distEnd']   = poi_distmat.loc[poi, pN]\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffPopEnd']   = pop - poi_info.loc[pN, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffNVisitEnd']   = nvisit - poi_info.loc[pN, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'diffDurationEnd']   = duration - poi_info.loc[pN, 'avgDuration']\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: \n",
    "- different POIs have different features for the same query trajectory\n",
    "- the same POI get different features for different query-id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a string for a training/test data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data_str(df_, df_columns=df_columns):\n",
    "    columns = df_columns[1:].copy()  # get rid of 'poiID'\n",
    "    for col in columns:\n",
    "        assert(col in df_.columns)\n",
    "        \n",
    "    lines = []\n",
    "    for idx in df_.index:\n",
    "        slist = [str(df_.loc[idx, 'label'])]\n",
    "        slist.append(' qid:')\n",
    "        slist.append(str(int(df_.loc[idx, 'queryID'])))\n",
    "        for j in range(2, len(columns)):\n",
    "            slist.append(' ')\n",
    "            slist.append(str(j-1))\n",
    "            slist.append(':')\n",
    "            slist.append(str(df_.loc[idx, columns[j]]))\n",
    "        slist.append('\\n')\n",
    "        lines.append(''.join(slist))\n",
    "    return ''.join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2.4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Ranking POIs using rankSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RankSVM implementation in [libsvm.zip](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ranksvm/libsvm-ranksvm-3.20.zip) or [liblinear.zip](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ranksvm/liblinear-ranksvm-1.95.zip), please read `README.ranksvm` in the zip file for installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [softmax function](https://en.wikipedia.org/wiki/Softmax_function) to convert ranking scores to a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    expx = np.exp(x)\n",
    "    return expx / np.sum(expx, axis=0) # column-wise sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a python wrapper of the `svm-train` or `train` and `svm-predict` or `predict` commands of rankSVM with ranking probabilities $P(p_i \\lvert (p_s, p_e, len))$ computed using [softmax function](https://en.wikipedia.org/wiki/Softmax_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python wrapper of rankSVM\n",
    "class RankSVM:\n",
    "    def __init__(self, bin_dir, useLinear=True, debug=False):\n",
    "        dir_ = !echo $bin_dir  # deal with environmental variables in path\n",
    "        assert(os.path.exists(dir_[0]))\n",
    "        self.bin_dir = dir_[0]\n",
    "        \n",
    "        self.bin_train = 'svm-train'\n",
    "        self.bin_predict = 'svm-predict'\n",
    "        if useLinear:\n",
    "            self.bin_train = 'train'\n",
    "            self.bin_predict = 'predict'\n",
    "        \n",
    "        assert(isinstance(debug, bool))\n",
    "        self.debug = debug\n",
    "        \n",
    "        # create named tmp files for model and feature scaling parameters\n",
    "        self.fmodel = None\n",
    "        self.fscale = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fmodel = fd.name\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fscale = fd.name\n",
    "        \n",
    "        if self.debug:\n",
    "            print('model file:', self.fmodel)\n",
    "            print('feature scaling parameter file:', self.fscale)\n",
    "    \n",
    "    \n",
    "    def __del__(self):\n",
    "        # remove tmp files\n",
    "        if self.fmodel is not None and os.path.exists(self.fmodel):\n",
    "            os.unlink(self.fmodel)\n",
    "        if self.fscale is not None and os.path.exists(self.fscale):\n",
    "            os.unlink(self.fscale)\n",
    "    \n",
    "    \n",
    "    def train(self, train_df, cost=1):\n",
    "        # cost is parameter C in SVM\n",
    "        # write train data to file\n",
    "        ftrain = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain = fd.name\n",
    "            datastr = gen_data_str(train_df)\n",
    "            fd.write(datastr)\n",
    "        \n",
    "        # feature scaling\n",
    "        ftrain_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -s $self.fscale $ftrain > $ftrain_scaled\n",
    "        \n",
    "        if self.debug:\n",
    "            print('cost:', cost)\n",
    "            print('train data file:', ftrain)\n",
    "            print('feature scaled train data file:', ftrain_scaled)\n",
    "        \n",
    "        # train rank svm and generate model file, if the model file exists, rewrite it\n",
    "        #n_cv = 10  # parameter k for k-fold cross-validation, NO model file will be generated in CV mode\n",
    "        #result = !$self.bin_dir/svm-train -c $cost -v $n_cv $ftrain $self.fmodel\n",
    "        result = !$self.bin_dir/$self.bin_train -c $cost $ftrain_scaled $self.fmodel\n",
    "        if self.debug:\n",
    "            print('Training finished.')\n",
    "            for i in range(len(result)): print(result[i])\n",
    "\n",
    "        # remove train data file\n",
    "        os.unlink(ftrain)\n",
    "        os.unlink(ftrain_scaled)        \n",
    "    \n",
    "    \n",
    "    def predict(self, test_df):\n",
    "        # predict ranking scores for the given feature matrix\n",
    "        if self.fmodel is None or not os.path.exists(self.fmodel):\n",
    "            print('Model should be trained before predicting')\n",
    "            return\n",
    "        \n",
    "        # write test data to file\n",
    "        ftest = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftest = fd.name\n",
    "            datastr = gen_data_str(test_df)\n",
    "            fd.write(datastr)\n",
    "                \n",
    "        # feature scaling\n",
    "        ftest_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            ftest_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -r $self.fscale $ftest > $ftest_scaled\n",
    "            \n",
    "        # generate prediction file\n",
    "        fpredict = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            fpredict = fd.name\n",
    "            \n",
    "        if self.debug:\n",
    "            print('test data file:', ftest)\n",
    "            print('feature scaled test data file:', ftest_scaled)\n",
    "            print('predict result file:', fpredict)\n",
    "            \n",
    "        # predict using trained model and write prediction to file\n",
    "        result = !$self.bin_dir/$self.bin_predict $ftest_scaled $self.fmodel $fpredict\n",
    "        if self.debug:\n",
    "            print('Predict result: %-30s  %s' % (result[0], result[1]))\n",
    "        \n",
    "        # generate prediction DataFrame from prediction file\n",
    "        poi_rank_df = pd.read_csv(fpredict, header=None)\n",
    "        poi_rank_df.rename(columns={0:'rank'}, inplace=True)\n",
    "        poi_rank_df['poiID'] = test_df['poiID'].astype(np.int)\n",
    "        #poi_rank_df.set_index('poiID', inplace=True) # duplicated 'poiID' when evaluating training data\n",
    "        poi_rank_df['probability'] = softmax(poi_rank_df['rank'])  # softmax\n",
    "        \n",
    "        # remove test file and prediction file\n",
    "        os.unlink(ftest)\n",
    "        os.unlink(ftest_scaled)\n",
    "        os.unlink(fpredict)\n",
    "        \n",
    "        return poi_rank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Model Training and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Pystruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704 6173\n",
      "(9, 128) (9,)\n",
      "(7, 128) (7,)\n"
     ]
    }
   ],
   "source": [
    "from pystruct.datasets import load_letters\n",
    "letters = load_letters()\n",
    "X, y, folds = letters['data'], letters['labels'], letters['folds']\n",
    "X, y = np.array(X), np.array(y)\n",
    "X_train, X_test = X[folds == 1], X[folds != 1]\n",
    "y_train, y_test = y[folds == 1], y[folds != 1]\n",
    "print(len(X_train), len(X_test))\n",
    "print(X_train[0].shape, y_train[0].shape)\n",
    "print(X_train[10].shape, y_train[10].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([max(y) for y in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y_train[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features for Structured Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a trajectory `[start, ..., end]`, the features used to train/test are \n",
    "1. ~~feature for POI `start` is a binary vector of size `total_number_of_POIs`, \n",
    "   with `1` at the location corresponds to POI `start` and `0` anywhere else, e.g. one hot encoding.~~\n",
    "1. ~~feature for POI `end` is simliar, with `1` at the location corresponds to POI `end`.~~\n",
    "1. features for other POIs in the trajectory are the ranking probabilities \n",
    "   (produced by rankSVM, transformed by softmax) of all POIs,\n",
    "   i.e. features of different POIs in the middle (i.e. not the `start` or `end`) are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: [PyStruct](https://pystruct.github.io/) assumes that [label `y` is a discrete vector](https://pystruct.github.io/intro.html) and [pystruct.learners assume labels `y` are integers starting with `0`](https://github.com/pystruct/pystruct/issues/114), concretely,\n",
    "- values in label vector $y$ should satisfy $y_i \\in Y$, \n",
    "  where $Y$ is the **index** of a discrete value space, and the index starts at 0.\n",
    "- label vector $y$ will be [transformed to one hot encoding (see function `joint_feature()`)](https://github.com/pystruct/pystruct/blob/master/pystruct/models/graph_crf.py).\n",
    "\n",
    "For example, if labels in training set is `[[1, 2], [0, 4, 9]]`, \n",
    "then it will cause an index out of bounds error as pystruct did something like this,\n",
    "1. construct an discrete value space: \n",
    "   - `set([1, 2] + [0, 4, 9]) -> {0, 1, 2, 4, 9}`\n",
    "   - `size({0, 1, 2, 4, 9}) = 5`\n",
    "1. convert labels using one hot encoding: \n",
    "   - label vector `[1, 2]` will be converted to a matrix of shape $2 \\times 5$,\n",
    "     with cells at `(0, 1), (1, 2)` set to `1` and others set to `0`.\n",
    "   - label vector `[0, 4, 9]` will be converted to a matrix of shape $3 \\times 5$,\n",
    "     with cells at `(0, 0)`, `(1, 4)`, **`(2, 9)` INDEX_OUT_OF_BOUNDS** set to `1` and others set to `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a [ChainCRF](https://pystruct.github.io/generated/pystruct.models.ChainCRF.html#pystruct.models.ChainCRF) using [OneSlackSSVM](https://pystruct.github.io/generated/pystruct.learners.OneSlackSSVM.html#pystruct.learners.OneSlackSSVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_info_all = calc_poi_info(trajid_set_all, traj_all, poi_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a mapping for POIs: *POI_ID $\\to$ POI_INDEX* with POIs in trajectories of length $\\ge 3$, also a map of the reverse direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#poi_set = set()\n",
    "poi_id_dict  = dict()\n",
    "poi_id_rdict = dict()\n",
    "poi_cnt_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tid in trajid_set_all:\n",
    "    t = extract_traj(tid, traj_all)\n",
    "    if len(t) < 3: continue\n",
    "    #poi_set = poi_set | set(t)\n",
    "    #poi_set = poi_set | set(t[1:-1])  # exclude the start and end POI\n",
    "    for poi in t[1:-1]:\n",
    "        if poi in poi_cnt_dict: \n",
    "            poi_cnt_dict[poi] += 1\n",
    "        else:\n",
    "            poi_cnt_dict[poi] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for ix, poi in enumerate(sorted(poi_set)): \n",
    "for ix, poi in enumerate(sorted(poi_cnt_dict.keys())): \n",
    "    poi_id_dict[poi] = ix\n",
    "for k, v in poi_id_dict.items(): \n",
    "    poi_id_rdict[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(len(poi_id_dict) == len(poi_id_rdict) <= poi_info_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_ix = [poi_id_rdict[x] for x in sorted(poi_id_rdict.keys())]  # POIs in ascending order of mapped IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 16, 21, 2] #1 ->\n",
      "           [20, 21, 16, 2]\n",
      "[13, 21, 2] #2 ->\n",
      "           [13, 2, 2]\n",
      "[2, 21, 16] #3 ->\n",
      "           [2, 21, 16]\n",
      "[2, 10, 21, 26] #4 ->\n",
      "           [2, 10, 26, 26]\n",
      "[28, 2, 13] #5 ->\n",
      "           [28, 12, 13]\n",
      "[11, 26, 25, 20, 28] #6 ->\n",
      "           [11, 15, 11, 12, 28]\n",
      "[2, 16, 1, 12, 21, 14] #7 ->\n",
      "           [2, 16, 21, 1, 26, 14]\n",
      "[9, 10, 15] #8 ->\n",
      "           [9, 26, 15]\n",
      "[12, 20, 16, 21, 1] #9 ->\n",
      "           [12, 28, 12, 20, 1]\n",
      "[19, 11, 28] #10 ->\n",
      "           [19, 16, 28]\n",
      "[12, 1, 16, 21, 2] #11 ->\n",
      "           [12, 28, 12, 20, 2]\n",
      "[28, 11, 16] #12 ->\n",
      "           [28, 2, 16]\n",
      "[1, 2, 21] #13 ->\n",
      "           [1, 16, 21]\n",
      "[13, 2, 21, 16] #14 ->\n",
      "           [13, 10, 21, 16]\n",
      "[9, 26, 2] #15 ->\n",
      "           [9, 9, 2]\n",
      "[1, 16, 2] #16 ->\n",
      "           [1, 16, 2]\n",
      "[2, 9, 10, 21] #17 ->\n",
      "           [2, 16, 2, 21]\n",
      "[16, 21, 26] #18 ->\n",
      "           [16, 2, 26]\n",
      "[17, 26, 27] #19 ->\n",
      "           [17, 17, 27]\n",
      "[15, 25, 26] #20 ->\n",
      "           [15, 10, 26]\n",
      "[1, 21, 2] #21 ->\n",
      "           [1, 16, 2]\n",
      "[26, 19, 2] #22 ->\n",
      "           [26, 10, 2]\n",
      "[13, 2, 20] #23 ->\n",
      "           [13, 2, 20]\n",
      "[21, 2, 11] #24 ->\n",
      "           [21, 12, 11]\n",
      "[1, 11, 26, 25, 29, 28, 12] #25 ->\n",
      "           [1, 16, 21, 16, 21, 16, 12]\n",
      "[25, 10, 12] #26 ->\n",
      "           [25, 19, 12]\n",
      "[11, 12, 26] #27 ->\n",
      "           [11, 16, 26]\n",
      "[28, 12, 20] #28 ->\n",
      "           [28, 2, 20]\n",
      "[20, 2, 12] #29 ->\n",
      "           [20, 2, 12]\n",
      "[2, 13, 11] #30 ->\n",
      "           [2, 21, 11]\n",
      "[2, 16, 1] #31 ->\n",
      "           [2, 16, 1]\n",
      "[2, 26, 20] #32 ->\n",
      "           [2, 16, 20]\n",
      "[21, 2, 16] #33 ->\n",
      "           [21, 2, 16]\n",
      "[21, 16, 2] #34 ->\n",
      "           [21, 16, 2]\n",
      "[2, 13, 22] #35 ->\n",
      "           [2, 16, 22]\n",
      "[1, 16, 2] #36 ->\n",
      "           [1, 20, 2]\n",
      "[1, 21, 16, 6] #37 ->\n",
      "           [1, 16, 21, 6]\n",
      "[13, 20, 28] #38 ->\n",
      "           [13, 2, 28]\n",
      "[1, 16, 28] #39 ->\n",
      "           [1, 16, 28]\n",
      "[1, 28, 20, 9] #40 ->\n",
      "           [1, 21, 16, 9]\n",
      "[29, 28, 12] #41 ->\n",
      "           [29, 8, 12]\n",
      "[10, 25, 26] #42 ->\n",
      "           [10, 15, 26]\n",
      "[15, 10, 26, 25] #43 ->\n",
      "           [15, 10, 9, 25]\n",
      "[10, 15, 25] #44 ->\n",
      "           [10, 10, 25]\n",
      "[2, 21, 16, 1] #45 ->\n",
      "           [2, 16, 21, 1]\n",
      "[25, 10, 9, 2] #46 ->\n",
      "           [25, 9, 10, 2]\n",
      "[15, 10, 19] #47 ->\n",
      "           [15, 25, 19]\n",
      "[21, 16, 1, 26, 9, 10, 15] #48 ->\n",
      "           [21, 25, 20, 29, 28, 20, 15]\n",
      "[29, 16, 2] #49 ->\n",
      "           [29, 16, 2]\n",
      "[10, 9, 26] #50 ->\n",
      "           [10, 10, 26]\n",
      "[26, 13, 20] #51 ->\n",
      "           [26, 10, 20]\n",
      "[11, 1, 16, 9] #52 ->\n",
      "           [11, 25, 20, 9]\n",
      "[16, 2, 21] #53 ->\n",
      "           [16, 21, 21]\n",
      "[1, 21, 2] #54 ->\n",
      "           [1, 21, 2]\n",
      "[26, 10, 9] #55 ->\n",
      "           [26, 13, 9]\n",
      "[13, 28, 2, 26] #56 ->\n",
      "           [13, 2, 21, 26]\n",
      "[1, 2, 20] #57 ->\n",
      "           [1, 16, 20]\n",
      "[2, 13, 9, 16, 21] #58 ->\n",
      "           [2, 9, 10, 1, 21]\n",
      "[2, 20, 26] #59 ->\n",
      "           [2, 16, 26]\n",
      "[11, 12, 28] #60 ->\n",
      "           [11, 20, 28]\n",
      "[13, 28, 12] #61 ->\n",
      "           [13, 2, 12]\n",
      "[11, 28, 20, 2] #62 ->\n",
      "           [11, 25, 20, 2]\n",
      "[2, 21, 16, 13, 10] #63 ->\n",
      "           [2, 10, 21, 16, 10]\n",
      "[16, 2, 21] #64 ->\n",
      "           [16, 21, 21]\n",
      "[12, 20, 2] #65 ->\n",
      "           [12, 20, 2]\n",
      "[20, 28, 12, 16, 1] #66 ->\n",
      "           [20, 11, 16, 2, 1]\n",
      "[9, 10, 15] #67 ->\n",
      "           [9, 10, 15]\n",
      "[9, 10, 1] #68 ->\n",
      "           [9, 28, 1]\n",
      "[1, 16, 2, 26, 6] #69 ->\n",
      "           [1, 26, 25, 29, 6]\n",
      "[10, 15, 11, 16, 21] #70 ->\n",
      "           [10, 10, 26, 25, 21]\n",
      "[2, 28, 12] #71 ->\n",
      "           [2, 21, 12]\n",
      "[21, 2, 19, 20] #72 ->\n",
      "           [21, 16, 2, 20]\n",
      "[2, 16, 21, 28] #73 ->\n",
      "           [2, 21, 1, 28]\n",
      "[13, 1, 11] #74 ->\n",
      "           [13, 2, 11]\n",
      "[19, 25, 26] #75 ->\n",
      "           [19, 16, 26]\n",
      "[8, 21, 16] #76 ->\n",
      "           [8, 20, 16]\n",
      "[15, 21, 12] #77 ->\n",
      "           [15, 10, 12]\n",
      "[20, 16, 2] #78 ->\n",
      "           [20, 2, 2]\n",
      "[20, 2, 16] #79 ->\n",
      "           [20, 21, 16]\n",
      "[21, 16, 20, 11] #80 ->\n",
      "           [21, 28, 2, 11]\n",
      "[11, 20, 10] #81 ->\n",
      "           [11, 28, 10]\n",
      "[1, 16, 13] #82 ->\n",
      "           [1, 16, 13]\n",
      "[1, 16, 20] #83 ->\n",
      "           [1, 16, 20]\n",
      "[20, 2, 21, 16] #84 ->\n",
      "           [20, 12, 21, 16]\n",
      "[21, 16, 12, 20, 2] #85 ->\n",
      "           [21, 27, 11, 16, 2]\n",
      "[1, 16, 11] #86 ->\n",
      "           [1, 16, 11]\n",
      "[1, 21, 2] #87 ->\n",
      "           [1, 16, 2]\n",
      "[26, 10, 20] #88 ->\n",
      "           [26, 10, 20]\n",
      "[13, 2, 28] #89 ->\n",
      "           [13, 2, 28]\n",
      "[2, 21, 13] #90 ->\n",
      "           [2, 21, 13]\n",
      "[21, 16, 2] #91 ->\n",
      "           [21, 5, 2]\n",
      "[16, 21, 20] #92 ->\n",
      "           [16, 21, 20]\n",
      "[16, 21, 20] #93 ->\n",
      "           [16, 2, 20]\n",
      "[2, 16, 1, 21] #94 ->\n",
      "           [2, 16, 1, 21]\n",
      "[10, 12, 2] #95 ->\n",
      "           [10, 10, 2]\n",
      "[10, 9, 26, 15] #96 ->\n",
      "           [10, 15, 11, 15]\n",
      "[2, 10, 13] #97 ->\n",
      "           [2, 21, 13]\n",
      "[2, 16, 21] #98 ->\n",
      "           [2, 16, 21]\n",
      "[26, 9, 10, 1, 2] #99 ->\n",
      "           [26, 13, 9, 26, 2]\n",
      "[16, 20, 29, 1] #100 ->\n",
      "           [16, 21, 21, 1]\n",
      "[15, 10, 9] #101 ->\n",
      "           [15, 10, 9]\n",
      "[16, 21, 2] #102 ->\n",
      "           [16, 21, 2]\n",
      "[2, 21, 1, 13] #103 ->\n",
      "           [2, 2, 21, 13]\n",
      "[16, 21, 1] #104 ->\n",
      "           [16, 21, 1]\n",
      "[26, 11, 28] #105 ->\n",
      "           [26, 10, 28]\n",
      "[13, 2, 21] #106 ->\n",
      "           [13, 2, 21]\n"
     ]
    }
   ],
   "source": [
    "if run_crf == True:\n",
    "    recdict_crf = dict()\n",
    "    cost = 1000\n",
    "    cnt = 1\n",
    "    for i in range(len(trajid_set_all)):\n",
    "        tid = trajid_set_all[i]\n",
    "        te = extract_traj(tid, traj_all)\n",
    "        \n",
    "        # trajectory too short\n",
    "        if len(te) < 3: continue\n",
    "            \n",
    "        # TSP, generally an ILP very hard for solvers (GUROBI/CBC)\n",
    "        if te[0] == te[-1]: continue\n",
    "\n",
    "        if uspecific == True:\n",
    "            user = traj_user.loc[tid, 'userID']\n",
    "            trajid_set_user = traj_user[traj_user['userID'] == user].index.tolist()\n",
    "            trajid_set_other = traj_user[traj_user['userID'] != user].index.tolist()\n",
    "            assert(tid in trajid_set_user)\n",
    "            trajid_set_add = [x for x in trajid_set_user if x != tid]\n",
    "            trajid_list_train = trajid_set_other.copy()\n",
    "            assert(KX > 0)\n",
    "            for k in range(KX):\n",
    "                trajid_list_train = trajid_list_train + trajid_set_add\n",
    "        else:\n",
    "            trajid_list_train = trajid_set_all[:i] + trajid_set_all[i+1:]\n",
    "        \n",
    "        poi_info = calc_poi_info(trajid_list_train, traj_all, poi_all)\n",
    "        \n",
    "        # trajectory too long\n",
    "        if len(te) > poi_info.shape[0]: continue\n",
    "            \n",
    "        # start/end is not in training set\n",
    "        if not (te[0] in poi_info.index and te[-1] in poi_info.index): continue\n",
    "            \n",
    "        # training set should contain all POIs\n",
    "        goodTrainset = True\n",
    "        for poi in te[1:-1]:\n",
    "            assert(poi in poi_cnt_dict)\n",
    "            if poi_cnt_dict[poi] == 1:  # POI exists in test set only\n",
    "                goodTrainset = False\n",
    "                break\n",
    "        if goodTrainset == False: continue\n",
    "        #poi_set_tr = set()\n",
    "        #for tid in trajid_list_train:\n",
    "        #    tr = extract_traj(tid, traj_all)\n",
    "        #    if len(tr) < 3: continue\n",
    "        #    #poi_set_tr = poi_set_tr | set(tr)\n",
    "        #    poi_set_tr = poi_set_tr | set(tr[1:-1])  # exclude the start and end POI\n",
    "        #if len(poi_set_tr & poi_set) != len(poi_set): \n",
    "        #    #print(sorted(poi_set_tr))\n",
    "        #    continue\n",
    "        \n",
    "        print(te, '#%d ->' % cnt); cnt += 1; sys.stdout.flush()\n",
    "        \n",
    "        # POI feature based ranking\n",
    "        train_df = gen_train_df(trajid_list_train, df_columns, traj_all, poi_info, poi_distmat, query_id_dict)\n",
    "        ranksvm = RankSVM(ranksvm_dir, useLinear=True)\n",
    "        ranksvm.train(train_df, cost=cost)\n",
    "            \n",
    "        # generate training data\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for tid in trajid_list_train:\n",
    "            tr = extract_traj(tid, traj_all)\n",
    "            if len(tr) < 3: continue\n",
    "            tr_df = gen_test_df(tr[0], tr[-1], len(tr), df_columns, poi_info, poi_distmat, query_id_dict)\n",
    "            tr_rank_df = ranksvm.predict(tr_df)\n",
    "            tr_rank_df.set_index('poiID', inplace=True)\n",
    "            # traj_length x (total number of POIs in trajectories with length >= 3)\n",
    "            featuremat = np.tile(softmax(tr_rank_df.loc[poi_ix, 'rank']), (len(tr)-2, 1)) # without start and end\n",
    "            #startfv = np.zeros(len(poi_ix), dtype=np.float); startfv[poi_id_dict[tr[0]]] = 1  # feature of start POI\n",
    "            #endfv = np.zeros(len(poi_ix), dtype=np.float); endfv[poi_id_dict[tr[-1]]] = 1  # feature of end POI\n",
    "            #X_train.append(np.vstack([startfv, featuremat, endfv]))\n",
    "            #y_train.append(np.array([poi_id_dict[x] for x in tr]))\n",
    "            X_train.append(featuremat)\n",
    "            y_train.append(np.array([poi_id_dict[x] for x in tr[1:-1]]))\n",
    "        \n",
    "        # train\n",
    "        t0 = time.time()\n",
    "        #crf = ChainCRF(directed=True)\n",
    "        crf = ChainCRF()\n",
    "        #C, maxIter = 0.01, 100\n",
    "        #ssvm = OneSlackSSVM(model=crf, C=C, max_iter=maxIter, n_jobs=4)\n",
    "        ssvm = OneSlackSSVM(model=crf, n_jobs=4)\n",
    "        ssvm.fit(X_train, y_train)\n",
    "        t1 = time.time()\n",
    "        \n",
    "        # generate test data\n",
    "        X_test = []\n",
    "        te_df = gen_test_df(te[0], te[-1], len(te), df_columns, poi_info, poi_distmat, query_id_dict)\n",
    "        te_rank_df = ranksvm.predict(te_df)\n",
    "        te_rank_df.set_index('poiID', inplace=True)\n",
    "        featuremat = np.tile(softmax(te_rank_df.loc[poi_ix, 'rank']), (len(te)-2, 1))\n",
    "        #startfv = np.zeros(len(poi_ix), dtype=np.float); startfv[poi_id_dict[te[0]]] = 1\n",
    "        #endfv = np.zeros(len(poi_ix), dtype=np.float); startfv[poi_id_dict[te[-1]]] = 1                 \n",
    "        #X_test.append(np.vstack([startfv, featuremat, endfv]))\n",
    "        X_test.append(featuremat)\n",
    "        \n",
    "        # test\n",
    "        y_pred = ssvm.predict(X_test)\n",
    "        \n",
    "        # map POIs back\n",
    "        rec = [te[0]] + [poi_id_rdict[x] for x in y_pred[0]] + [te[-1]]\n",
    "        \n",
    "        key = trajid_set_all[i]\n",
    "        recdict_crf[key] = {'REAL':te, 'REC_CRF':rec} \n",
    "        \n",
    "        print(' '*10, rec, '%.1f sec' % (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the F1 score for recommended trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_F1(traj_act, traj_rec, noloop=False):\n",
    "    '''Compute recall, precision and F1 for recommended trajectories'''\n",
    "    assert(isinstance(noloop, bool))\n",
    "    assert(len(traj_act) > 0)\n",
    "    assert(len(traj_rec) > 0)\n",
    "    \n",
    "    if noloop == True:\n",
    "        intersize = len(set(traj_act) & set(traj_rec))\n",
    "    else:\n",
    "        match_tags = np.zeros(len(traj_act), dtype=np.bool)\n",
    "        for poi in traj_rec:\n",
    "            for j in range(len(traj_act)):\n",
    "                if match_tags[j] == False and poi == traj_act[j]:\n",
    "                    match_tags[j] = True\n",
    "                    break\n",
    "        intersize = np.nonzero(match_tags)[0].shape[0]\n",
    "        \n",
    "    recall = intersize / len(traj_act)\n",
    "    precision = intersize / len(traj_rec)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1 = [calc_F1(recdict_crf[key]['REAL'], recdict_crf[key]['REC_CRF'], noloop=True) for key in sorted(recdict_crf.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.739824797844 0.180666219626\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(F1), np.std(F1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
