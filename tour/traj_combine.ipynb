{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import os, re, sys, time, pickle, tempfile\n",
    "import math, random, itertools, scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as sop\n",
    "from joblib import Parallel, delayed\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(987654321) # control random choice when splitting training/testing set\n",
    "np.random.seed(987654321)\n",
    "ranksvm_dir = '$HOME/work/ranksvm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_progress(cnt, total):\n",
    "    \"\"\"\n",
    "    Display a progress bar.\n",
    "    \"\"\"\n",
    "    assert(cnt > 0 and total > 0 and cnt <= total)\n",
    "    length = 80\n",
    "    ratio = cnt / total\n",
    "    n = int(length * ratio)\n",
    "    sys.stdout.write('\\r[%-80s] %d%%' % ('-'*n, int(ratio*100)))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/data-ijcai15'\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Osak.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Osak.csv')\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Glas.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Glas.csv')\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Edin.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Edin.csv')\n",
    "fvisit = os.path.join(data_dir, 'userVisits-Toro.csv')\n",
    "fcoord = os.path.join(data_dir, 'photoCoords-Toro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suffix = fvisit.split('-')[-1].split('.')[0]\n",
    "fseqpart = os.path.join(data_dir, 'seqPart-combine-' + suffix + '.pkl')\n",
    "#fF1data = os.path.join(data_dir, 'F1-combine-' + suffix + '.pkl')\n",
    "#fmlikeseq = os.path.join(data_dir, 'mlike-seq-' + suffix + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visits = pd.read_csv(fvisit, sep=';')\n",
    "coords = pd.read_csv(fcoord, sep=';')\n",
    "assert(visits.shape[0] == coords.shape[0])\n",
    "traj = pd.merge(visits, coords, on='photoID') # merge data frames according to column 'photoID'\n",
    "#traj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#photo</th>\n",
       "      <th>#photo/user</th>\n",
       "      <th>#poi</th>\n",
       "      <th>#seq</th>\n",
       "      <th>#seq/user</th>\n",
       "      <th>#user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Toro</th>\n",
       "      <td>39419</td>\n",
       "      <td>28.257348</td>\n",
       "      <td>29</td>\n",
       "      <td>6057</td>\n",
       "      <td>4.341935</td>\n",
       "      <td>1395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      #photo  #photo/user  #poi  #seq  #seq/user  #user\n",
       "Toro   39419    28.257348    29  6057   4.341935   1395"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_photo = traj['photoID'].unique().shape[0]\n",
    "num_user = traj['userID'].unique().shape[0]\n",
    "num_poi = traj['poiID'].unique().shape[0]\n",
    "num_seq = traj['seqID'].unique().shape[0]\n",
    "pd.DataFrame({'#photo': num_photo, '#user': num_user, '#poi': num_poi, '#seq': num_seq, \\\n",
    "              '#photo/user': num_photo/num_user, '#seq/user': num_seq/num_user}, index=[str(suffix)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Compute POI Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute POI (Longitude, Latitude) as the average coordinates of the assigned photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_coords = traj[['poiID', 'photoLon', 'photoLat']].groupby('poiID').mean()\n",
    "poi_coords.reset_index(inplace=True)\n",
    "poi_coords.rename(columns={'photoLon':'poiLon', 'photoLat':'poiLat'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract POI category and visiting frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_catfreq = traj[['poiID', 'poiTheme', 'poiFreq']].groupby('poiID').first()\n",
    "poi_catfreq.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poiTheme</th>\n",
       "      <th>poiFreq</th>\n",
       "      <th>poiLon</th>\n",
       "      <th>poiLat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poiID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sport</td>\n",
       "      <td>3506</td>\n",
       "      <td>-79.379243</td>\n",
       "      <td>43.643183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sport</td>\n",
       "      <td>609</td>\n",
       "      <td>-79.418634</td>\n",
       "      <td>43.632772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sport</td>\n",
       "      <td>688</td>\n",
       "      <td>-79.380045</td>\n",
       "      <td>43.662175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sport</td>\n",
       "      <td>3056</td>\n",
       "      <td>-79.389290</td>\n",
       "      <td>43.641297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cultural</td>\n",
       "      <td>986</td>\n",
       "      <td>-79.392396</td>\n",
       "      <td>43.653662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       poiTheme  poiFreq     poiLon     poiLat\n",
       "poiID                                         \n",
       "1         Sport     3506 -79.379243  43.643183\n",
       "2         Sport      609 -79.418634  43.632772\n",
       "3         Sport      688 -79.380045  43.662175\n",
       "4         Sport     3056 -79.389290  43.641297\n",
       "6      Cultural      986 -79.392396  43.653662"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_all = pd.merge(poi_catfreq, poi_coords, on='poiID')\n",
    "poi_all.set_index('poiID', inplace=True)\n",
    "poi_all.head()\n",
    "#poi_all.to_csv(fpoi, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Construct Travelling Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>seqID</th>\n",
       "      <th>poiID</th>\n",
       "      <th>arrivalTime</th>\n",
       "      <th>departureTime</th>\n",
       "      <th>#photo</th>\n",
       "      <th>poiDuration(sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10007579@N00</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1346844688</td>\n",
       "      <td>1346844688</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1142731848</td>\n",
       "      <td>1142732445</td>\n",
       "      <td>4</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1142916492</td>\n",
       "      <td>1142916492</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>1319327174</td>\n",
       "      <td>1319332848</td>\n",
       "      <td>9</td>\n",
       "      <td>5674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10014440@N06</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1196128621</td>\n",
       "      <td>1196128878</td>\n",
       "      <td>3</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         userID  seqID  poiID  arrivalTime  departureTime  #photo  \\\n",
       "0  10007579@N00      1     30   1346844688     1346844688       1   \n",
       "1  10012675@N05      2      6   1142731848     1142732445       4   \n",
       "2  10012675@N05      3      6   1142916492     1142916492       1   \n",
       "3  10012675@N05      4     13   1319327174     1319332848       9   \n",
       "4  10014440@N06      5     24   1196128621     1196128878       3   \n",
       "\n",
       "   poiDuration(sec)  \n",
       "0                 0  \n",
       "1               597  \n",
       "2                 0  \n",
       "3              5674  \n",
       "4               257  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_all = traj[['userID', 'seqID', 'poiID', 'dateTaken']].copy().groupby(['userID', 'seqID', 'poiID'])\\\n",
    "          .agg([np.min, np.max, np.size])\n",
    "seq_all.columns = seq_all.columns.droplevel()\n",
    "seq_all.reset_index(inplace=True)\n",
    "seq_all.rename(columns={'amin':'arrivalTime', 'amax':'departureTime', 'size':'#photo'}, inplace=True)\n",
    "seq_all['poiDuration(sec)'] = seq_all['departureTime'] - seq_all['arrivalTime']\n",
    "seq_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>seqLen</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seqID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10007579@N00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10014440@N06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             userID  seqLen\n",
       "seqID                      \n",
       "1      10007579@N00       1\n",
       "2      10012675@N05       1\n",
       "3      10012675@N05       1\n",
       "4      10012675@N05       1\n",
       "5      10014440@N06       1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_stats = seq_all[['userID', 'seqID', 'poiID']].copy().groupby(['userID', 'seqID']).agg(np.size)\n",
    "seq_stats.reset_index(inplace=True)\n",
    "seq_stats.rename(columns={'poiID':'seqLen'}, inplace=True)\n",
    "seq_stats.set_index('seqID', inplace=True)\n",
    "seq_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_seq(seqid, seq_all):\n",
    "    seqi = seq_all[seq_all['seqID'] == seqid].copy()\n",
    "    seqi.sort_values(by=['arrivalTime'], ascending=True, inplace=True)\n",
    "    return seqi['poiID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poi_info(seqid_set, seq_all, poi_all):\n",
    "    poi_info = seq_all[seq_all['seqID'].isin(seqid_set)][['poiID', 'poiDuration(sec)']].copy()\n",
    "    poi_info = poi_info.groupby('poiID').agg([np.mean, np.size])\n",
    "    poi_info.columns = poi_info.columns.droplevel()\n",
    "    poi_info.reset_index(inplace=True)\n",
    "    poi_info.rename(columns={'mean':'avgDuration(sec)', 'size':'popularity'}, inplace=True)\n",
    "    poi_info.set_index('poiID', inplace=True)\n",
    "    poi_info['poiTheme'] = poi_all.loc[poi_info.index, 'poiTheme']\n",
    "    poi_info['poiLon'] = poi_all.loc[poi_info.index, 'poiLon']\n",
    "    poi_info['poiLat'] = poi_all.loc[poi_info.index, 'poiLat']\n",
    "    return poi_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Train vs. Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate trajectories, i.e. same trajectory for different users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_stats = seq_stats[seq_stats['seqLen'] > 2]\n",
    "seq_stats = seq_stats[seq_stats['seqLen'] < 10]\n",
    "seqid_set_ = seq_stats.index.tolist()\n",
    "seq_dict = dict()\n",
    "for seqid in seqid_set_:\n",
    "    seq = extract_seq(seqid, seq_all)\n",
    "    key = str(seq)\n",
    "    if key in seq_dict: seq_dict[key].append(seqid)\n",
    "    else: seq_dict[key] = [seqid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seqid_set_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_exp = [seq_dict[x][0] for x in sorted(seq_dict.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seqid_set_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a *query* (in IR terminology) using tuple (start POI, end POI, #POI) ~~user ID.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_id_dict = dict()  # (start, end, length) --> qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqs_exp = [extract_seq(seqid, seq_all) for seqid in seqid_set_exp]\n",
    "keys = [(seq[0], seq[-1], len(seq)) for seq in seqs_exp]\n",
    "cnt = 0\n",
    "for key in keys:\n",
    "    if key not in query_id_dict:   # (start, end, length) --> qid\n",
    "        query_id_dict[key] = cnt\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n"
     ]
    }
   ],
   "source": [
    "print(len(query_id_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random split trajectories for training and testing.  \n",
    "Make sure all POIs in test set are covered in trajectories for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_percent = 0.8\n",
    "seqid_set_train0 = []\n",
    "seqid_set_test0 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(fseqpart):\n",
    "    (seqid_set_train0, seqid_set_test0) = pickle.load(open(fseqpart, 'rb'))\n",
    "else:\n",
    "    ntrain = int(train_percent * len(seqid_set_exp))\n",
    "    while True:\n",
    "        np.random.shuffle(seqid_set_exp)\n",
    "\n",
    "        seqid_set_train0 = sorted(list(seqid_set_exp[:ntrain]))\n",
    "        seqid_set_test0 = sorted(list(seqid_set_exp[ntrain:]))\n",
    "        poi_train = seq_all[seq_all['seqID'].isin(seqid_set_train0)]['poiID'].unique().tolist()\n",
    "        poi_test  = seq_all[seq_all['seqID'].isin(seqid_set_test0)]['poiID'].unique().tolist()\n",
    "        if len(set(poi_test)) == len(set(poi_train) & set(poi_test)):\n",
    "            pickle.dump((seqid_set_train0, seqid_set_test0), open(fseqpart, 'wb'))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#seq in total: 290\n",
      "#seq for training: 232\n",
      "#seq for testing: 58\n",
      "#query tuple: 230\n"
     ]
    }
   ],
   "source": [
    "print('#seq in total:', len(seqid_set_exp))\n",
    "print('#seq for training:', ntrain)\n",
    "print('#seq for testing:', len(seqid_set_exp)-ntrain)\n",
    "print('#query tuple:', len(query_id_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Agnostic POI Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI Features used for ranking:\n",
    "1. `popularity`: POI popularity\n",
    "2. `avgDuration`: average POI visit duration\n",
    "3. `sameCatStart`: 1 if POI category is the same as that of `startPOI`, -1 otherwise\n",
    "4. `sameCatEnd`: 1 if POI category is the same as that of `endPOI`, -1 otherwise\n",
    "5. `distStart`: distance (haversine formula) from `startPOI`\n",
    "6. `distEnd`: distance from `endPOI`\n",
    "7. `seqLen`: trajectory length (copy from query)\n",
    "8. `diffPopStart`: difference in POI popularity from `startPOI`\n",
    "9. `diffPopEnd`: difference in POI popularity from `endPOI`\n",
    "10. `diffDurationStart`: difference in average POI visit duration from the actual duration spent at `startPOI`\n",
    "11. `diffDurationEnd`: difference in average POI visit duration from the actual duration spent at `endPOI`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features aggregated from a number of trajectories:\n",
    "1. Compute POI `popularity` and average visit `duration` using all trajectories from training and querying set,\n",
    "1. Use the same features that computed above for the test set, except the distance based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 5)\n"
     ]
    }
   ],
   "source": [
    "poi_info_t = calc_poi_info(seqid_set_train0, seq_all, poi_all)\n",
    "print(poi_info_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute distance between two POIs using [Haversine formula](http://en.wikipedia.org/wiki/Great-circle_distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_dist(longitude1, latitude1, longitude2, latitude2):\n",
    "    \"\"\"Calculate the distance (unit: km) between two places on earth\"\"\"\n",
    "    # convert degrees to radians\n",
    "    lon1 = math.radians(longitude1)\n",
    "    lat1 = math.radians(latitude1)\n",
    "    lon2 = math.radians(longitude2)\n",
    "    lat2 = math.radians(latitude2)\n",
    "    radius = 6371.0088 # mean earth radius is 6371.009km, en.wikipedia.org/wiki/Earth_radius#Mean_radius\n",
    "    # The haversine formula, en.wikipedia.org/wiki/Great-circle_distance\n",
    "    dlon = math.fabs(lon1 - lon2)\n",
    "    dlat = math.fabs(lat1 - lat2)\n",
    "    return 2 * radius * math.asin(math.sqrt(\\\n",
    "               (math.sin(0.5*dlat))**2 + math.cos(lat1) * math.cos(lat2) * (math.sin(0.5*dlon))**2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data are generated as follows:\n",
    "1. each input tuple $(\\text{startPOI}, \\text{endPOI}, \\text{#POI})$ form a `query` (in IR terminology).\n",
    "1. the label of a specific POI is the number of presence of that POI in a specific `query`, excluding the presence as $\\text{startPOI}$ or $\\text{endPOI}$.\n",
    "1. for each `query`, the label of all absence POIs from trajectories of that `query` in training set got a label 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of training data matrix is `#(qid, poi)` by `#feature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_train_df(seqid_set, seq_all, poi_info, query_id_dict):\n",
    "    columns = ['poiID', 'label', 'queryID', 'popularity', 'avgDuration(sec)', 'sameCatStart', 'sameCatEnd', \\\n",
    "               'distStart', 'distEnd', 'seqLen', 'diffPopStart', 'diffPopEnd', 'diffDurationStart', 'diffDurationEnd']\n",
    "    seqid_set = sorted(set(seqid_set))\n",
    "    train_seqs = [extract_seq(seqid, seq_all) for seqid in seqid_set]\n",
    "    for seq in train_seqs:\n",
    "        assert(len(seq) > 2)\n",
    "    \n",
    "    qid_set = sorted(set([query_id_dict[(seq[0], seq[-1], len(seq))] for seq in train_seqs]))\n",
    "    poi_set = sorted(set(poi_info.index.tolist()))\n",
    "    qid_poi_pair = list(itertools.product(qid_set, poi_set)) # Cartesian product of qid_set and poi_set\n",
    "    \n",
    "    df_ = pd.DataFrame(data=np.zeros((len(qid_poi_pair), len(columns)), dtype= np.float), columns=columns)\n",
    "    \n",
    "    query_id_rdict = dict()\n",
    "    for k, v in query_id_dict.items(): \n",
    "        query_id_rdict[v] = k  # qid --> (start, end, length)\n",
    "    \n",
    "    for i in range(df_.index.shape[0]):\n",
    "        qid = qid_poi_pair[i][0]\n",
    "        poi = qid_poi_pair[i][1]\n",
    "        (p0, pN, seqLen) = query_id_rdict[qid]\n",
    "        lon0 = poi_info.loc[p0, 'poiLon']; lat0 = poi_info.loc[p0, 'poiLat']\n",
    "        lonN = poi_info.loc[pN, 'poiLon']; latN = poi_info.loc[pN, 'poiLat']\n",
    "        lon = poi_info.loc[poi, 'poiLon']; lat = poi_info.loc[poi, 'poiLat']\n",
    "        pop = poi_info.loc[poi, 'popularity']; cat = poi_info.loc[poi, 'poiTheme']\n",
    "        duration = poi_info.loc[poi, 'avgDuration(sec)']\n",
    "        idx = df_.index[i]\n",
    "        df_.loc[idx, 'poiID'] = poi\n",
    "        df_.loc[idx, 'queryID'] = qid\n",
    "        df_.loc[idx, 'popularity'] = pop\n",
    "        df_.loc[idx, 'avgDuration(sec)'] = duration\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_info.loc[p0, 'poiTheme'] else -1\n",
    "        df_.loc[idx, 'sameCatEnd']   = 1 if cat == poi_info.loc[pN, 'poiTheme'] else -1\n",
    "        df_.loc[idx, 'distStart'] = 0 if poi == p0 else calc_dist(lon, lat, lon0, lat0)\n",
    "        df_.loc[idx, 'distEnd']   = 0 if poi == pN else calc_dist(lon, lat, lonN, latN)\n",
    "        df_.loc[idx, 'seqLen'] = seqLen\n",
    "        df_.loc[idx, 'diffPopStart'] = 0 if poi == p0 else pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffPopEnd']   = 0 if poi == pN else pop - poi_info.loc[pN, 'popularity']\n",
    "        df_.loc[idx, 'diffDurationStart'] = 0 if poi == p0 else duration - poi_info.loc[p0, 'avgDuration(sec)']\n",
    "        df_.loc[idx, 'diffDurationEnd']   = 0 if poi == pN else duration - poi_info.loc[pN, 'avgDuration(sec)']\n",
    "    \n",
    "    # set label\n",
    "    df_.set_index(['queryID', 'poiID'], inplace=True)\n",
    "    for seq in train_seqs:\n",
    "        qid = query_id_dict[(seq[0], seq[-1], len(seq))]\n",
    "        for poi in seq[1:-1]:  # do NOT count if the POI is startPOI/endPOI\n",
    "            df_.loc[(qid, poi), 'label'] += 1\n",
    "    \n",
    "    df_.reset_index(inplace=True)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: \n",
    "- different POIs have different features for the same query trajectory\n",
    "- the same POI get different features for different query-id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data are generated the same way as training data, except that the labels of testing data (unknown) could be arbitrary values as suggested in [libsvm FAQ](http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#f431).\n",
    "The reported accuracy (by `svm-predict` command) is meaningless as it is calculated based on these labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of training data matrix is `#poi` by `#feature` with one specific `query`, i.e. tuple $(\\text{startPOI}, \\text{endPOI}, \\text{#POI})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_test_df(startPOI, endPOI, nPOI, poi_info, query_id_dict):\n",
    "    columns = ['poiID', 'label', 'queryID', 'popularity', 'avgDuration(sec)', 'sameCatStart', 'sameCatEnd', \\\n",
    "               'distStart', 'distEnd', 'seqLen', 'diffPopStart', 'diffPopEnd', 'diffDurationStart', 'diffDurationEnd']\n",
    "    key = (p0, pN, seqLen) = (startPOI, endPOI, nPOI)\n",
    "    assert(key in query_id_dict)\n",
    "    \n",
    "    poi_set = sorted(set(poi_info.index.tolist()))\n",
    "    df_ = pd.DataFrame(data=np.zeros((len(poi_set), len(columns)), dtype= np.float), columns=columns)\n",
    "    \n",
    "    qid = query_id_dict[key]\n",
    "    df_['queryID'] = qid\n",
    "    df_['label'] = np.random.rand(df_.shape[0]) # label for test data is arbitrary according to libsvm FAQ\n",
    "\n",
    "    lon0 = poi_info.loc[p0, 'poiLon']; lat0 = poi_info.loc[p0, 'poiLat']\n",
    "    lonN = poi_info.loc[pN, 'poiLon']; latN = poi_info.loc[pN, 'poiLat']\n",
    "    for i in range(df_.index.shape[0]):\n",
    "        poi = poi_set[i]\n",
    "        lon = poi_info.loc[poi, 'poiLon']; lat = poi_info.loc[poi, 'poiLat']\n",
    "        pop = poi_info.loc[poi, 'popularity']; cat = poi_info.loc[poi, 'poiTheme']\n",
    "        duration = poi_info.loc[poi, 'avgDuration(sec)']\n",
    "        idx = df_.index[i]\n",
    "        df_.loc[idx, 'poiID'] = poi \n",
    "        df_.loc[idx, 'popularity'] = pop\n",
    "        df_.loc[idx, 'avgDuration(sec)'] = duration\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_info.loc[p0, 'poiTheme'] else -1\n",
    "        df_.loc[idx, 'sameCatEnd']   = 1 if cat == poi_info.loc[pN, 'poiTheme'] else -1\n",
    "        df_.loc[idx, 'distStart'] = 0 if poi == p0 else calc_dist(lon, lat, lon0, lat0)\n",
    "        df_.loc[idx, 'distEnd']   = 0 if poi == pN else calc_dist(lon, lat, lonN, latN)\n",
    "        df_.loc[idx, 'seqLen'] = seqLen\n",
    "        df_.loc[idx, 'diffPopStart'] = 0 if poi == p0 else pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffPopEnd']   = 0 if poi == pN else pop - poi_info.loc[pN, 'popularity']\n",
    "        df_.loc[idx, 'diffDurationStart'] = 0 if poi == p0 else duration - poi_info.loc[p0, 'avgDuration(sec)']\n",
    "        df_.loc[idx, 'diffDurationEnd']   = 0 if poi == pN else duration - poi_info.loc[pN, 'avgDuration(sec)']\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: \n",
    "- different POIs have different features for the same query trajectory\n",
    "- the same POI get different features for different query-id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a string for a training/test data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data_str(df_):\n",
    "    columns = ['label', 'queryID', 'popularity', 'avgDuration(sec)', 'sameCatStart', 'sameCatEnd', 'distStart', \\\n",
    "               'distEnd', 'seqLen', 'diffPopStart', 'diffPopEnd', 'diffDurationStart', 'diffDurationEnd']\n",
    "    for col in columns:\n",
    "        assert(col in df_.columns)\n",
    "        \n",
    "    lines = []\n",
    "    for idx in df_.index:\n",
    "        slist = [str(df_.loc[idx, 'label'])]\n",
    "        slist.append(' qid:')\n",
    "        slist.append(str(int(df_.loc[idx, 'queryID'])))\n",
    "        for j in range(2, len(columns)):\n",
    "            slist.append(' ')\n",
    "            slist.append(str(j-1))\n",
    "            slist.append(':')\n",
    "            slist.append(str(df_.loc[idx, columns[j]]))\n",
    "        slist.append('\\n')\n",
    "        lines.append(''.join(slist))\n",
    "    return ''.join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platt Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform ranking scores (output of rankSVM) to a probability distribution using [Platt scaling](https://en.wikipedia.org/wiki/Platt_scaling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from paper `Lin, Hsuan-Tien; Lin, Chih-Jen; Weng, Ruby C. (2007).` \n",
    "[`A note on Platt's probabilistic outputs for support vector machines`](http://stocktrendresearch.googlecode.com/svn-history/r77/trunk/Paper/SVM_ANN/plattprob.pdf).\n",
    "`Machine Learning 68 (3): 267–276.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def platt_scaling_params(deci, label, prior1, prior0):\n",
    "    \"\"\"\n",
    "    Input parameters:\n",
    "    - deci = array of SVM decision values (of examples in training set)\n",
    "    - label = array of booleans: is the example labeled +1? (in training set)\n",
    "    - prior1 = number of positive examples (in training set)\n",
    "    - prior0 = number of negative examples (in training set)\n",
    "    Outputs:\n",
    "    - A, B = parameters of sigmoid\n",
    "    \"\"\"\n",
    "\n",
    "    # Parameter setting\n",
    "    maxiter = 100   # Maximum number of iterations\n",
    "    minstep = 1e-10 # Minimum step taken in line search\n",
    "    sigma = 1e-12   # Set to any value > 0\n",
    "\n",
    "    # Construct initial values: target support in array t, initial function value in fval\n",
    "    hiTarget = (prior1 + 1.0) / (prior1 + 2.0)\n",
    "    loTarget = 1 / (prior0 + 2.0)\n",
    "    len_ = prior1 + prior0  # Total number of data\n",
    "    t = np.zeros(len_, dtype=np.float)\n",
    "    for i in range(len_):\n",
    "        if label[i] == True: \n",
    "            t[i] = hiTarget\n",
    "        else:\n",
    "            t[i] = loTarget\n",
    "\n",
    "    A = 0.0\n",
    "    B = math.log((prior0 + 1.0) / (prior1 + 1.0))\n",
    "    fval = 0.0\n",
    "    for i in range(len_):\n",
    "        fApB = deci[i] * A + B\n",
    "        if fApB >= 0:\n",
    "            fval += t[i] * fApB + math.log(1 + math.exp(-fApB))\n",
    "        else:\n",
    "            fval += (t[i] - 1) * fApB + math.log(1 + math.exp(fApB))\n",
    "\n",
    "    for it in range(maxiter):\n",
    "        # Update Gradient and Hessian (use H' = H + sigma I)\n",
    "        h11 = h22 = sigma\n",
    "        h21 = g1 = g2 = 0.0\n",
    "        for i in range(len_):\n",
    "            fApB = deci[i] * A + B\n",
    "            if fApB >= 0:\n",
    "                p = math.exp(-fApB) / (1.0 + math.exp(-fApB))\n",
    "                q = 1.0 / (1.0 + math.exp(-fApB))\n",
    "            else:\n",
    "                p = 1.0 / (1.0 + math.exp(fApB))\n",
    "                q = math.exp(fApB) / (1.0 + math.exp(fApB))\n",
    "            d2 = p * q\n",
    "            h11 += deci[i] * deci[i] * d2\n",
    "            h22 += d2\n",
    "            h21 += deci[i] * d2\n",
    "            d1 = t[i] - p\n",
    "            g1 += deci[i] * d1\n",
    "            g2 += d1\n",
    "\n",
    "        # Stopping criteria\n",
    "        if math.fabs(g1) < 1e-5 and math.fabs(g2) < 1e-5:  \n",
    "            break\n",
    "\n",
    "        # Compute modified Newton directions\n",
    "        det = h11 * h22 - h21 * h21\n",
    "        dA = -(h22 * g1 - h21 * g2) / det\n",
    "        dB = -(-h21 * g1 + h11 * g2) / det\n",
    "        gd = g1 * dA + g2 * dB\n",
    "\n",
    "        # Line search\n",
    "        stepsize = 1\n",
    "        while stepsize >= minstep:\n",
    "            newA = A + stepsize * dA\n",
    "            newB = B + stepsize * dB\n",
    "            newf = 0.0\n",
    "            for i in range(len_):\n",
    "                fApB = deci[i] * newA + newB\n",
    "                if fApB >= 0:\n",
    "                    newf += t[i] * fApB + math.log(1 + math.exp(-fApB))\n",
    "                else:\n",
    "                    newf += (t[i] - 1) * fApB + math.log(1 + math.exp(fApB))\n",
    "\n",
    "            # Sufficient decrease satisfied\n",
    "            if newf < fval + 0.0001 * stepsize * gd:\n",
    "                A = newA\n",
    "                B = newB\n",
    "                fval = newf\n",
    "                break\n",
    "            else:\n",
    "                stepsize /= 2.0\n",
    "\n",
    "        if stepsize < minstep:\n",
    "            print('Line search fails')\n",
    "            break\n",
    "\n",
    "    if it >= maxiter:\n",
    "        print('Reaching maximum iterations')\n",
    "\n",
    "    return (A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking POIs using rankSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RankSVM implementation in [libsvm.zip](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ranksvm/libsvm-ranksvm-3.20.zip) or [liblinear.zip](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ranksvm/liblinear-ranksvm-1.95.zip), please read `README.ranksvm` in the zip file for installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a python wrapper of the `svm-train` or `train` and `svm-predict` or `predict` commands of rankSVM with ranking probabilities calibrated using [Platt scaling](https://en.wikipedia.org/wiki/Platt_scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python wrapper of rankSVM\n",
    "class RankSVM:\n",
    "    def __init__(self, bin_dir, useLinear=True, debug=False):\n",
    "        dir_ = !echo $bin_dir  # deal with environmental variables in path\n",
    "        assert(os.path.exists(dir_[0]))\n",
    "        self.bin_dir = dir_[0]\n",
    "        \n",
    "        self.bin_train = 'svm-train'\n",
    "        self.bin_predict = 'svm-predict'\n",
    "        if useLinear:\n",
    "            self.bin_train = 'train'\n",
    "            self.bin_predict = 'predict'\n",
    "        \n",
    "        # Platt scaling parameters\n",
    "        self.A = None\n",
    "        self.B = None\n",
    "        \n",
    "        assert(isinstance(debug, bool))\n",
    "        self.debug = debug\n",
    "        \n",
    "        # create named tmp files for model and feature scaling parameters\n",
    "        self.fmodel = None\n",
    "        self.fscale = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fmodel = fd.name\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fscale = fd.name\n",
    "        \n",
    "        if self.debug:\n",
    "            print('model file:', self.fmodel)\n",
    "            print('feature scaling parameter file:', self.fscale)\n",
    "    \n",
    "    \n",
    "    def __del__(self):\n",
    "        # remove tmp files\n",
    "        if self.fmodel is not None and os.path.exists(self.fmodel):\n",
    "            os.unlink(self.fmodel)\n",
    "        if self.fscale is not None and os.path.exists(self.fscale):\n",
    "            os.unlink(self.fscale)\n",
    "    \n",
    "    \n",
    "    def train(self, train_df, cost=1):\n",
    "        # cost is parameter C in SVM\n",
    "        # write train data to file\n",
    "        ftrain = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain = fd.name\n",
    "            datastr = gen_data_str(train_df)\n",
    "            fd.write(datastr)\n",
    "        \n",
    "        # feature scaling\n",
    "        ftrain_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -s $self.fscale $ftrain > $ftrain_scaled\n",
    "        \n",
    "        if self.debug:\n",
    "            print('cost:', cost)\n",
    "            print('train data file:', ftrain)\n",
    "            print('feature scaled train data file:', ftrain_scaled)\n",
    "        \n",
    "        # train rank svm and generate model file, if the model file exists, rewrite it\n",
    "        #n_cv = 10  # parameter k for k-fold cross-validation, NO model file will be generated in CV mode\n",
    "        #result = !$self.bin_dir/svm-train -c $cost -v $n_cv $ftrain $self.fmodel\n",
    "        result = !$self.bin_dir/$self.bin_train -c $cost $ftrain_scaled $self.fmodel\n",
    "        if self.debug:\n",
    "            print('Training finished.')\n",
    "            for i in range(len(result)): print(result[i])\n",
    "\n",
    "        # remove train data file\n",
    "        os.unlink(ftrain)\n",
    "        os.unlink(ftrain_scaled)\n",
    "        \n",
    "        # learn Platt scaling parameters\n",
    "        rank_df = self.predict(train_df)\n",
    "        deci = rank_df['rank']                  # array of SVM decision values (of examples in training set)\n",
    "        label = (train_df['label'] > 0).values  # array of booleans: is the example labeled +1? (in training set)\n",
    "        prior1 = train_df[train_df['label'] > 0].shape[0] # number of positive examples (in training set)\n",
    "        prior0 = train_df.shape[0] - prior1               # number of negative examples (in training set)\n",
    "        (self.A, self.B) = platt_scaling_params(deci, label, prior1, prior0)\n",
    "    \n",
    "    \n",
    "    def predict(self, test_df):\n",
    "        # predict ranking scores for the given feature matrix\n",
    "        if self.fmodel is None or not os.path.exists(self.fmodel):\n",
    "            print('Model should be trained before predicting')\n",
    "            return\n",
    "        \n",
    "        # write test data to file\n",
    "        ftest = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftest = fd.name\n",
    "            datastr = gen_data_str(test_df)\n",
    "            fd.write(datastr)\n",
    "                \n",
    "        # feature scaling\n",
    "        ftest_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            ftest_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -r $self.fscale $ftest > $ftest_scaled\n",
    "            \n",
    "        # generate prediction file\n",
    "        fpredict = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            fpredict = fd.name\n",
    "            \n",
    "        if self.debug:\n",
    "            print('test data file:', ftest)\n",
    "            print('feature scaled test data file:', ftest_scaled)\n",
    "            print('predict result file:', fpredict)\n",
    "            \n",
    "        # predict using trained model and write prediction to file\n",
    "        result = !$self.bin_dir/$self.bin_predict $ftest_scaled $self.fmodel $fpredict\n",
    "        if self.debug:\n",
    "            print('Predict result: %-30s  %s' % (result[0], result[1]))\n",
    "        \n",
    "        # generate prediction DataFrame from prediction file\n",
    "        poi_rank_df = pd.read_csv(fpredict, header=None)\n",
    "        poi_rank_df.rename(columns={0:'rank'}, inplace=True)\n",
    "        poi_rank_df['poiID'] = test_df['poiID']\n",
    "        #poi_rank_df.set_index('poiID', inplace=True) # duplicated 'poiID' when evaluating training data\n",
    "        if self.A is not None and self.B is not None: \n",
    "            # compute probability by Platt scaling: en.wikipedia.org/wiki/Platt_scaling\n",
    "            poi_rank_df['probability'] = 1 / (1 + np.exp(self.A * poi_rank_df['rank'] + self.B))\n",
    "        \n",
    "        # remove test file and prediction file\n",
    "        os.unlink(ftest)\n",
    "        os.unlink(ftest_scaled)\n",
    "        os.unlink(fpredict)\n",
    "        \n",
    "        return poi_rank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_ = gen_train_df(seqid_set_train0, seq_all, poi_info_t, query_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranksvm_ = RankSVM(ranksvm_dir, useLinear=True)\n",
    "ranksvm_.train(train_df_, cost=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: -2.735, B: 7.289\n"
     ]
    }
   ],
   "source": [
    "print('A: %.3f, B: %.3f' % (ranksvm_.A, ranksvm_.B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_idx_ = 12\n",
    "seq_ = extract_seq(seqid_set_test0[seqid_idx_], seq_all)\n",
    "test_df_ = gen_test_df(seq_[0], seq_[-1], len(seq_), poi_info_t, query_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.48296648298\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poiID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.421740</td>\n",
       "      <td>0.339459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.313210</td>\n",
       "      <td>0.276371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.197020</td>\n",
       "      <td>0.217498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.189870</td>\n",
       "      <td>0.214188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.148980</td>\n",
       "      <td>0.195966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.148440</td>\n",
       "      <td>0.195734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.073290</td>\n",
       "      <td>0.165383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.063380</td>\n",
       "      <td>0.161676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.034770</td>\n",
       "      <td>0.151349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.027300</td>\n",
       "      <td>0.148744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.997130</td>\n",
       "      <td>0.138596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.629260</td>\n",
       "      <td>0.055561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.586710</td>\n",
       "      <td>0.049761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.473080</td>\n",
       "      <td>0.036960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.407510</td>\n",
       "      <td>0.031081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.310600</td>\n",
       "      <td>0.024018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.171010</td>\n",
       "      <td>0.016522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.149560</td>\n",
       "      <td>0.015595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.100010</td>\n",
       "      <td>0.013646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.015430</td>\n",
       "      <td>0.010858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.896800</td>\n",
       "      <td>0.007873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.889259</td>\n",
       "      <td>0.007714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.708854</td>\n",
       "      <td>0.004724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.503632</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.131086</td>\n",
       "      <td>0.000976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.405150</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rank  probability\n",
       "poiID                       \n",
       "30     2.421740     0.339459\n",
       "22     2.313210     0.276371\n",
       "16     2.197020     0.217498\n",
       "23     2.189870     0.214188\n",
       "28     2.148980     0.195966\n",
       "4      2.148440     0.195734\n",
       "21     2.073290     0.165383\n",
       "8      2.063380     0.161676\n",
       "24     2.034770     0.151349\n",
       "7      2.027300     0.148744\n",
       "1      1.997130     0.138596\n",
       "6      1.629260     0.055561\n",
       "25     1.586710     0.049761\n",
       "29     1.473080     0.036960\n",
       "3      1.407510     0.031081\n",
       "19     1.310600     0.024018\n",
       "27     1.171010     0.016522\n",
       "2      1.149560     0.015595\n",
       "11     1.100010     0.013646\n",
       "17     1.015430     0.010858\n",
       "14     0.896800     0.007873\n",
       "15     0.889259     0.007714\n",
       "10     0.708854     0.004724\n",
       "13     0.503632     0.002700\n",
       "20     0.131086     0.000976\n",
       "26    -1.405150     0.000015"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_df_ = ranksvm_.predict(test_df_)\n",
    "rank_df_.set_index('poiID', inplace=True)\n",
    "rank_df_.sort_values(by='rank', ascending=False, inplace=True)\n",
    "print(rank_df_['probability'].sum())  # it is ranking probabilities, not probability distribution\n",
    "rank_df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorise Transition Probabilities between POI Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI features used to factorise transition matrix of Markov Chain with POI features (vector) as states:\n",
    "- POI category\n",
    "- POI popularity (discritize with uniform log-scale bins, #bins <=5 )\n",
    "- POI average visit duration (discritise with uniform log-scale bins, #bins <= 5)\n",
    "- POI neighborhood (clustering POI(lat, lon) using k-means, #clusters <= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POIs in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_train = seq_all[seq_all['seqID'].isin(seqid_set_train0)]['poiID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition matrix between POI cateogries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amusement', 'Beach', 'Cultural', 'Shopping', 'Sport', 'Structure']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_cats = poi_all.loc[poi_train, 'poiTheme'].unique().tolist()\n",
    "poi_cats.sort()\n",
    "poi_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transmat_cat = pd.DataFrame(data=np.zeros((len(poi_cats), len(poi_cats)), dtype=np.float), \\\n",
    "                            columns=poi_cats, index=poi_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of transitions between POI categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for seqid in seqid_set_train0:\n",
    "    seq = extract_seq(seqid, seq_all)\n",
    "    assert(len(seq) > 2)\n",
    "    for pi in range(len(seq)-1):\n",
    "        p1 = seq[pi]\n",
    "        p2 = seq[pi+1]\n",
    "        cat1 = poi_all.loc[p1, 'poiTheme']\n",
    "        cat2 = poi_all.loc[p2, 'poiTheme']\n",
    "        transmat_cat.loc[cat1, cat2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise each row to obtain transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ix in transmat_cat.index:\n",
    "    rowsum = transmat_cat.loc[ix].sum()\n",
    "    assert(rowsum > 0)\n",
    "    transmat_cat.loc[ix] /= rowsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amusement</th>\n",
       "      <th>Beach</th>\n",
       "      <th>Cultural</th>\n",
       "      <th>Shopping</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Structure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amusement</th>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.120690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beach</th>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.110294</td>\n",
       "      <td>0.198529</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.316176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cultural</th>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.184397</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.226950</td>\n",
       "      <td>0.070922</td>\n",
       "      <td>0.283688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shopping</th>\n",
       "      <td>0.060345</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.215517</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.060345</td>\n",
       "      <td>0.284483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sport</th>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.191489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Structure</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.246154</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>0.084615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Amusement     Beach  Cultural  Shopping     Sport  Structure\n",
       "Amusement   0.051724  0.120690  0.293103  0.120690  0.293103   0.120690\n",
       "Beach       0.044118  0.110294  0.198529  0.272059  0.058824   0.316176\n",
       "Cultural    0.148936  0.184397  0.085106  0.226950  0.070922   0.283688\n",
       "Shopping    0.060345  0.310345  0.215517  0.068966  0.060345   0.284483\n",
       "Sport       0.191489  0.191489  0.234043  0.106383  0.085106   0.191489\n",
       "Structure   0.100000  0.307692  0.246154  0.192308  0.069231   0.084615"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transmat_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition matrix between POI popularities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 10, 15, 18, 21, 22, 28, 30, 36, 38, 70, 74, 75, 83, 85, 92, 95]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_pops = poi_info_t.loc[poi_train, 'popularity'].unique().tolist()\n",
    "poi_pops.sort()\n",
    "poi_pops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretize POI popularity with uniform log-scale bins (#bins $\\le 5$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = pd.Series(poi_pops).hist(figsize=(5, 3), bins=20)\n",
    "ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate a transition matrix for each feature of POI, transition matrix over all POI features (vector) is obtrained by the [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product) of the individual transition matrix corresponding to each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Ranking with Factorised Markov Chain to Recommend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "- Ranking is over POIs (in POI space)\n",
    "- Transition probabilities of Markov Chain of POI features (in POI feature space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two approaches:\n",
    "- recommend trajectory in POI space: \n",
    "  - transform transition matrix of feature vector into transition matrix of POIs,\n",
    "  - recommend a sequence of POIs.\n",
    "- recommend trajectory in POI feature space: \n",
    "  - transform ranking probability of POIs into ranking probability of POI features,\n",
    "  - recommend a sequence of features, \n",
    "  - transform the squence of features into a sequence of POIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two methods for recommendation:\n",
    "- find a simple path with maximum objective in a digraph\n",
    "- make greedy choice (of POI or POI feature vector) step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics without considering visiting order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_F1score(seq_act, seq_rec)#, includeStartEnd=True):\n",
    "    assert(len(seq_act) > 2)\n",
    "    assert(len(seq_rec) > 2)\n",
    "\n",
    "    act_set = set(seq_act)\n",
    "    rec_set = set(seq_rec)\n",
    "    intersect = act_set & rec_set\n",
    "    \n",
    "    recall    = len(intersect) / len(act_set) #if includeStartEnd else (len(intersect)-2) / (len(act_set)-2)\n",
    "    precision = len(intersect) / len(rec_set) #if includeStartEnd else (len(intersect)-2) / (len(rec_set)-2)\n",
    "    #F1score   = 0 if abs(precision + recall) < 1e-6 else 2. * precision * recall / (precision + recall)\n",
    "    F1score   = 2. * precision * recall / (precision + recall)\n",
    "  \n",
    "    return F1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import comb\n",
    "from math import factorial\n",
    "def rand_guess(npoi, length):\n",
    "    assert(length <= npoi)\n",
    "    if length == npoi: return 1\n",
    "    N = npoi - 2\n",
    "    m = length - 2 # number of correct POIs\n",
    "    k = m\n",
    "    expected_F1 = 0\n",
    "    while k >= 0:\n",
    "        F1 = (k + 2) / length\n",
    "        prob = comb(m, k) * comb(N-m, m-k) / comb(N, m)\n",
    "        expected_F1 += prob * F1\n",
    "        k -= 1\n",
    "    return expected_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_guess(20, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
