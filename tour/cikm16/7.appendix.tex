\newpage

\appendix

\section{POI Categories}
We collected the POI categories in all of the five trajectory datasets,
their names with the corresponding icons are shown in figure \ref{fig:poicats}.

%\cheng{Report the proportion of recommended trajectories and actual trajectories with sub-tours, in the Appendix.}

\section{POIs sharing the same feature vector}
When a group of POIs sharing identical (discretised) features,
we distribute the probability uniformly among the POIs in the group as follows.

Firstly, the incoming (unnormalised) transition probability of the group computed by taking 
the Kronecker product is divided uniformly among POIs in the group,
which is equivalent to choose a POI in the group uniformly at random.

On the other hand, the outgoing (unnormalised) transition probability of each POI 
should be the same as that of the POI group, 
since in this case, the transition from any POI in the group to one outside the group 
represents an outgoing transition from the POI group.

Furthermore, the self-loop transition of the POI group represents transitions from any POI in the group
to any other POI in the same group, we distribute the self-loop transition probability uniformly 
among all these pairs.
In particular, suppose the (unnormalised) self-loop transition probability is $P_o$,
and the number of POIs in the group is $N_o$,
the transition probability from $p_i$ to $p_j$ in the same group is
\begin{displaymath}
    P(p_j | p_i) = \delta(i \ne j) \frac{P_o}{N_o - 1}, \forall 1 \le i, j \le N_o
\end{displaymath}
Where $\delta(i \ne j) = 1$ if $i \ne j$ and $0$ otherwise, which is the same constraint that no POI
self-loops are allowed, as described in Section~\ref{sec:transition}.

Lastly, we normalise each row of the (unnormalised) POI-POI transition matrix 
to form a valid (outgoing) probability distribution for each POI.


\section{Implementation details}
We use the rankSVM implementation in libsvmtools \url{https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/},
and the Structured SVM implementation in PyStruct \url{https://pystruct.github.io/}.
Integer linear programming are solved using both Gurobi Optimizer \url{http://www.gurobi.com/} 
and lp\_solve \url{http://lpsolve.sourceforge.net/}.


\section{Avoid Peeking}
When working with machine learning algorithms, to make sure the reported performance is a good approximation
of the generalisation performance, it is critical to prevent information in test set from leaking into
training set.
Many algorithms shown in Table~\ref{tab:algsummary} leveraging both learning to rank and 
factorised POI-POI transition matrix,
e.g., \textsc{Rank+Markov}, \textsc{Rank+MarkovPath} and \textsc{StructuredSVM},
both of them need to be trained or parameters be estimated before being utilised in other algorithms.
POI features such as popularity, the number of visits and average visit duration are
determined by not only the POI itself but also trajectories in training set, 
let's call them aggregated features, as they are computed by aggregating a set of trajectories.
To make sure the prediction performance is reliable, 
it is very important to exclude trajectories in test set when computing aggregated features.
Unfortunately, it is quite easy, especially when utilising multiple levels of machine learning models,
to use all data, including those in test set, to compute aggregated features, 
and many researchers and practitioners did not realise the fact that 
some bits of information in test set were leaked into training set via these aggregated features.

%One may argue that many of these features will not change much when computed with or without data in test set,
%but in certain areas, such as aerodynamics, some decisions are very sensitive to the quantity of certain features.
%Nevertheless, the exact impact still needs further investigation.


\begin{figure}[t]
	\centering
	\includegraphics[width=0.6\columnwidth]{fig/poi_cats.pdf}
	\caption{POI Categories}
	\label{fig:poicats}
\end{figure}


\begin{table}[t]
\caption{POI features used to factorise POI-POI transition probabilities}
\label{tab:featuretran}
\centering
\begin{tabular}{l|l} \hline
\textbf{Feature}     & \textbf{Description} \\ \hline
\texttt{category}    & category of POI \\
\texttt{position}    & the cluster that a POI resides in \\
\texttt{popularity}  & (discritised) popularity of POI \\
\texttt{nVisit}      & (discritised) total number of visit at POI \\
\texttt{avgDuration} & (discritised) average duration at POI \\ \hline
\end{tabular}
\end{table}


\begin{table*}[t]
\caption{Features of POI $p$ used in rankSVM given query $(p_s, p_e, L)$}
\label{tab:featurerank}
\centering
%\small
%\setlength{\tabcolsep}{2pt} % tweak the space between columns
%\begin{tabular}{l|p{0.7\columnwidth}} \hline
\begin{tabular}{l|l} \hline
\textbf{Feature}  & \textbf{Description} \\ \hline
\texttt{category}          & one-hot encoding of the category of $p$ \\
\texttt{position}          & one-hot encoding of the POI cluster that $p$ resides in \\
\texttt{popularity}        & logarithm of POI popularity of $p$ \\
\texttt{nVisit}            & logarithm of the total number of visit by all users at $p$ \\
\texttt{avgDuration}       & logarithm of the average duration at $p$ \\ \hline
\texttt{trajLen}           & trajectory length $L$, i.e., the number of POIs required \\
\texttt{sameCatStart}      & $1$ if the category of $p$ is the same as that of $p_s$, $-1$ otherwise \\
\texttt{sameCatEnd}        & $1$ if the category of $p$ is the same as that of $p_e$, $-1$ otherwise \\
\texttt{sameClusterStart}  & $1$ if $p$ resides in the same POI cluster as $p_s$, $-1$ otherwise \\
\texttt{sameClusterEnd}    & $1$ if $p$ resides in the same POI cluster as $p_e$, $-1$ otherwise \\
\texttt{distStart}         & distance between $p$ and $p_s$, calculated using the Haversine formula \\
\texttt{distEnd}           & distance between $p$ and $p_e$, calculated using the Haversine formula \\
\texttt{diffPopStart}      & real-valued difference in POI popularity of $p$ from that of $p_s$ \\
\texttt{diffPopEnd}        & real-valued difference in POI popularity of $p$ from that of $p_e$ \\
\texttt{diffNVisitStart}   & real-valued difference in the total number of visit at $p$ from that at $p_s$ \\
\texttt{diffNVisitEnd}     & real-valued difference in the total number of visit at $p$ from that at $p_e$ \\
\texttt{diffDurationStart} & real-valued difference in average duration at $p$ from that at $p_s$ \\
\texttt{diffDurationEnd}   & real-valued difference in average duration at $p$ from that at $p_e$ \\ \hline
\end{tabular}
\end{table*}


\section{RankSVM}
\label{appendix:ranksvm}

\subsection{Prediction}
The ranking score for POI $p_i$ is $S_{p_i} = \mathbf{w_r} \mathbf{f}_{p_i}$,
where $\mathbf{w_r}$ is a vector of parameters,
and $\mathbf{f}_{p_i}$ is the (scaled) feature vector for POI $p_i$ as described in Table~\ref{tab:featurerank}.


\subsection{Training}
To estimate the parameters $\mathbf{w_r}$, we train a rankSVM with linear kernel and L$2$ loss:
\begin{displaymath}
\min_{\mathbf{w_r}} \frac{1}{2} \mathbf{w_r}^T \mathbf{w_r} +
                    C_r \sum_{(p_i, p_j) \in \mathcal{P}}
                    \max \left( 0, 1 - \mathbf{w_r}^T (\mathbf{f}_{p_i} - \mathbf{f}_{p_j}) \right)^2
\end{displaymath}
where $\mathcal{P}$ is the set of POIs to rank,
$C_r > 0$ is the regularisation parameter.

We generate a training example for each pair $(p, q)$ in the Cartesian product $\mathcal{P} \times \mathcal{Q}$,
where $\mathcal{P}$ is the set of POIs and $\mathcal{Q}$ is the set of queries appeared in training set.
The label of a training example is the number of occurrences of POI $p$ in trajectories grouped by query $q = (p_s, p_e, L)$,
without counting the occurrence of $p$ when it is the origin or destination of a trajectory.


\section{POI-POI transition matrix}

\label{appendix:transition}
We factorised the transition probability from POI $p_i$ to POI $p_j$
as a product of transition probabilities between pairs of 
individual POI features (and appropriately normalised), as described in Table~\ref{tab:featuretran}.


\section{Structured SVM}
\label{appendix:ssvm}

To recommend a trajectory that starts from POI $p_s$, ends at POI $p_e$, and with $L$ POIs in total.
We build a chain with $L$ random variables (nodes) and $L-1$ directed edges between neighbouring variables,
each discrete variable has $|\mathcal{P}|$ states.
The first variable corresponds to $p_s$, the last variable corresponds to $p_e$ and
all intermediate variables corresponds to POIs to be recommended.


\subsection{Node Features}
\label{appendix:node}
A naive approach to build node features:
\begin{itemize}
\item The feature vector for a node (variable) is computed by concatenating the feature vectors of all POIs in $\mathcal{P}$, 
      the feature vector for a POI is the same as that used in rankSVM as described in Table~\ref{tab:featurerank}.
\item All nodes (variables) share the same feature vector.
\end{itemize}


\subsection{Edge Features}
\label{appendix:edge}
A naive approach to build edge features:
\begin{itemize}
\item The feature vector for a directed edge is computed by unrolling the POI-POI transition matrix 
      described in Section~\ref{appendix:transition}.
\item All edges share the same feature vector.
\end{itemize}


\subsection{Prediction}
The best trajectory to recommend with respect to query $q = (p_s, p_e, L)$ is
\begin{displaymath}
    \mathcal{T}^* = \argmax_{\mathcal{T} \in \mathcal{P}^L} 
                    \sum_{j=1}^L \mathbf{w_u}^T \mathbf{\phi}_j \left( q, \mathcal{T}_j \right) +
                    \sum_{j=1}^{L-1} \mathbf{w_p}^T \mathbf{\phi}_{j, j+1} \left( q, \mathcal{T}_j, \mathcal{T}_{j+1} \right)
\end{displaymath}
where $\mathbf{\phi}_j$ is the feature vector of the $j$-th node (variable), as described in Section~\ref{appendix:node},
and $\mathbf{\phi}_{j, j+1}$ is the feature vector of the $j$-th edge, as described in Section~\ref{appendix:edge}.
$\mathbf{w_u}$ and $\mathbf{w_p}$ are the parameters of node and edge features respectively.


\subsection{Training}
To estimate the parameters $\mathbf{w_u}$ and $\mathbf{w_p}$, we train a Structured Support Vector Machine
using the 1-slack formulation,
\begin{align*}
    \min_{\mathbf{w}, \xi \ge 0} ~~& \frac{1}{2} \mathbf{w}^T \mathbf{w} + C \xi \\
    s.t. ~~& \forall \left( \hat{\mathcal{T}}^{(1)}, \cdots, \hat{\mathcal{T}}^{(N)} \right) \in \mathscr{T}^N: \\
         ~~& \frac{1}{N} \mathbf{w}^T \sum_{i=1}^N \delta \left( \hat{\mathcal{T}}^{(i)} \right) \ge
             \frac{1}{N} \sum_{i=1}^N \Delta \left( \mathcal{T}^{(i)}, \hat{\mathcal{T}}^{(i)} \right) - \xi
\end{align*}
where $\mathbf{w} = [\mathbf{w_u}^T, \mathbf{w_p}^T]^T$ is the parameter vector,
$\mathcal{T}^{(i)}$ is the $i$-th trajectory in training set, 
and its corresponding recommendation is $\hat{\mathcal{T}}^{(i)}$.
$N$ is the training set size, i.e., the number of trajectories in training set, 
$C$ is the regularisation parameter, $\xi$ is the slack variable, and
\begin{displaymath}
    \delta \left( \hat{\mathcal{T}}^{(i)} \right) = \Psi \left( q^{(i)}, \mathcal{T}^{(i)} \right) - 
                                                    \Psi \left( q^{(i)}, \hat{\mathcal{T}}^{(i)} \right)
\end{displaymath}
where $q^{(i)}$ and $\Psi \left( q^{(i)}, \mathcal{T}^{(i)} \right)$ are the query and the joint feature vector 
correspond to the $i$-th trajectory in training set. 
A joint feature vector is computed from the node and edge feature vectors.
$\Delta \left( \mathcal{T}^{(i)}, \hat{\mathcal{T}}^{(i)} \right)$ is the loss associated with the $i$-th trajectory 
and its corresponding recommendation, and Hamming loss is used in our experiment.
