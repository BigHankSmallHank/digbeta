\newpage

\appendix

\urlstyle{tt}

\section{POI Categories}
We collected the POI categories in all of the five trajectory datasets,
their names with the corresponding icons are shown in figure \ref{fig:poicats}.

%\cheng{Report the proportion of recommended trajectories and actual trajectories with sub-tours, in the Appendix.}

\section{POIs sharing the same feature vector}
When a group of POIs sharing identical (discretised) features,
we distribute the probability uniformly among the POIs in the group as follows.

Firstly, the incoming (unnormalised) transition probability of the group computed by taking 
the Kronecker product is divided uniformly among POIs in the group,
which is equivalent to choose a POI in the group uniformly at random.

On the other hand, the outgoing (unnormalised) transition probability of each POI 
should be the same as that of the POI group, 
since in this case, the transition from any POI in the group to one outside the group 
represents an outgoing transition from the POI group.

Furthermore, the self-loop transition of the POI group represents transitions from any POI in the group
to any other POI in the same group, we distribute the self-loop transition probability uniformly 
among all these pairs.
In particular, suppose the (unnormalised) self-loop transition probability is $P_o$,
and the number of POIs in the group is $N_o$,
the transition probability from $p_i$ to $p_j$ in the same group is
\begin{displaymath}
    P(p_j | p_i) = \delta(i \ne j) \frac{P_o}{N_o - 1}, \forall 1 \le i, j \le N_o
\end{displaymath}
Where $\delta(i \ne j) = 1$ if $i \ne j$ and $0$ otherwise, which is the same constraint that no POI
self-loops are allowed, as described in Section~\ref{sec:transition}.

Lastly, we normalise each row of the (unnormalised) POI-POI transition matrix 
to form a valid (outgoing) probability distribution for each POI.


\section{Implementation details}
We use the rankSVM implementation in libsvmtools \url{https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/},
and the Structured SVM implementation in PyStruct \url{https://pystruct.github.io/}.
Integer linear programming are solved using both Gurobi Optimizer \url{http://www.gurobi.com/} 
and lp\_solve \url{http://lpsolve.sourceforge.net/}.


\section{Avoid Peeking}
When working with machine learning algorithms, to make sure the reported performance is a good approximation
of the generalisation performance, it is critical to prevent information in test set from leaking into
training set.
Many algorithms shown in Table~\ref{tab:algsummary} leveraging both learning to rank and 
factorised POI-POI transition matrix,
e.g., \textsc{Rank+Markov}, \textsc{Rank+MarkovPath} and \textsc{StructuredSVM},
both of them need to be trained or parameters be estimated before being utilised in other algorithms.
POI features such as popularity, the number of visits and average visit duration are
determined by not only the POI itself but also trajectories in training set, 
let's call them aggregated features, as they are computed by aggregating a set of trajectories.
To make sure the prediction performance is reliable, 
it is very important to exclude trajectories in test set when computing aggregated features.
Unfortunately, it is quite easy, especially when utilising multiple levels of machine learning models,
to use all data, including those in test set, to compute aggregated features, 
and many researchers and practitioners did not realise the fact that 
some bits of information in test set were leaked into training set via these aggregated features.

%One may argue that many of these features will not change much when computed with or without data in test set,
%but in certain areas, such as aerodynamics, some decisions are very sensitive to the quantity of certain features.
%Nevertheless, the exact impact still needs further investigation.


\begin{figure}[t]
	\centering
	\includegraphics[width=0.6\columnwidth]{fig/poi_cats.pdf}
	\caption{POI Categories}
	\label{fig:poicats}
\end{figure}


\begin{table}[t]
\caption{POI features used to factorise POI-POI transition probabilities}
\label{tab:featuretran}
\centering
\setlength{\tabcolsep}{2pt} % tweak the space between columns
\begin{tabular}{l|l} \hline
\textbf{Feature}       & \textbf{Description} \\ \hline
\texttt{category}      & category of POI \\
\texttt{neighbourhood} & the cluster that a POI resides in \\
\texttt{popularity}    & (discretised) popularity of POI \\
\texttt{nVisit}        & (discretised) total number of visit at POI \\
\texttt{avgDuration}   & (discretised) average duration at POI \\ \hline
\end{tabular}
\end{table}


\begin{table*}[t]
\caption{Features of POI $p$ used in rankSVM given query $(p_s, p_e, L)$}
\label{tab:featurerank}
\centering
%\small
%\setlength{\tabcolsep}{2pt} % tweak the space between columns
%\begin{tabular}{l|p{0.7\columnwidth}} \hline
\begin{tabular}{l|l} \hline
\textbf{Feature}  & \textbf{Description} \\ \hline
\texttt{category}               & one-hot encoding of the category of $p$ \\
\texttt{neighbourhood}          & one-hot encoding of the POI cluster that $p$ resides in \\
\texttt{popularity}             & logarithm of POI popularity of $p$ \\
\texttt{nVisit}                 & logarithm of the total number of visit by all users at $p$ \\
\texttt{avgDuration}            & logarithm of the average duration at $p$ \\ \hline
\texttt{trajLen}                & trajectory length $L$, i.e., the number of POIs required \\
\texttt{sameCatStart}           & $1$ if the category of $p$ is the same as that of $p_s$, $-1$ otherwise \\
\texttt{sameCatEnd}             & $1$ if the category of $p$ is the same as that of $p_e$, $-1$ otherwise \\
\texttt{sameNeighbourhoodStart} & $1$ if $p$ resides in the same POI cluster as $p_s$, $-1$ otherwise \\
\texttt{sameNeighbourhoodEnd}   & $1$ if $p$ resides in the same POI cluster as $p_e$, $-1$ otherwise \\
\texttt{distStart}              & distance between $p$ and $p_s$, calculated using the Haversine formula \\
\texttt{distEnd}                & distance between $p$ and $p_e$, calculated using the Haversine formula \\
\texttt{diffPopStart}           & real-valued difference in POI popularity of $p$ from that of $p_s$ \\
\texttt{diffPopEnd}             & real-valued difference in POI popularity of $p$ from that of $p_e$ \\
\texttt{diffNVisitStart}        & real-valued difference in the total number of visit at $p$ from that at $p_s$ \\
\texttt{diffNVisitEnd}          & real-valued difference in the total number of visit at $p$ from that at $p_e$ \\
\texttt{diffDurationStart}      & real-valued difference in average duration at $p$ from that at $p_s$ \\
\texttt{diffDurationEnd}        & real-valued difference in average duration at $p$ from that at $p_e$ \\ \hline
\end{tabular}
\end{table*}


\section{RankSVM}
\label{appendix:ranksvm}

\subsection{Prediction}
Given a set of $M$ POIs $\mathcal{P} = \{p_1, \cdots, p_M\}$, 
and $N$ queries $\mathcal{Q} = \{q_1, \cdots, q_N\}$.
The ranking score for POI $p_m$ with respect to query $q_n$ is 
$S_{m,n} = \langle \mathbf{w_r}, \mathbf{\phi}(p_m, q_n) \rangle$,
where $\mathbf{w_r}$ is a vector of parameters,
and $\mathbf{\phi}(p_m, q_n)$ is the scaled feature vector for $p_m$ with respect to query $q_n$,
as described in Table~\ref{tab:featurerank}.

\subsection{Feature scaling}
We scale the features described in Table~\ref{tab:featurerank} to range $[-1, 1]$ using the same approach as that 
utilised by libsvm \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm/},
i.e., we fit a linear function $f(x) = a x + b$ for feature $x$ such that the maximum value of $x$ maps to $1$ 
and the minimum value of $x$ map to $-1$.

\subsection{Training}
To estimate the parameters $\mathbf{w_r}$, we train a rankSVM with linear kernel and L$2$ loss:
%\begin{displaymath}
\begin{multline}
\min_{\mathbf{w_r}} \frac{1}{2} \mathbf{w_r}^T \mathbf{w_r} + \\ \hfill
                    C_r \sum_{(p_i, q_n), (p_j, q_n) \in \mathcal{P} \times \mathcal{Q}}
                    \max \left( 0, 1 - \mathbf{w_r}^T (\mathbf{\phi}(p_i, q_n) - \mathbf{\phi}(p_j, q_n)) \right)^2
\end{multline}
%\end{displaymath}
where $C_r > 0$ is the regularisation parameter.

The label of a training example is the number of occurrences of POI $p_m$ in trajectories grouped by query $q = (p_s, p_e, L)$,
without counting the occurrence of $p_m$ when it is the origin or destination of a trajectory.


\section{Factorised POI-POI transitions}

\label{appendix:transition}
We factorise the transition probability from POI $p_i$ to POI $p_j$
according to $5$ POI features described in Table~\ref{tab:featuretran},
which result in a transition matrix for each individual POI feature.


\section{Structured SVM}
\label{appendix:ssvm}

To recommend a trajectory that starts from POI $p_s$, ends at POI $p_e$, and with $L$ POIs in total.
We build a chain with $L$ random variables (nodes) and $L-1$ directed edges between neighbouring variables,
each discrete variable has $|\mathcal{P}|$ states.
Both the first variable and the last variable are observed, i.e., they are $p_s$ and $p_e$ respectively,
all intermediate variables corresponds to POIs that should be recommended.


\subsection{Node Features}
\label{appendix:node}
Given a POI $p_m$ and a query $q_n$, the node features are the same as that described in Table~\ref{tab:featurerank}.
Features are scaled the same way as that described in Section~\ref{appendix:ranksvm}.


\subsection{Edge Features}
\label{appendix:edge}
Given an ordered pair of POIs $(p_i, p_j)$, 
the edge/pairwise features are built from the factorised transition probabilities 
according to features described in Table~\ref{tab:featuretran}:
\begin{enumerate}
\item The transition probability from the category of $p_i$ to the category of $p_j$.
\item The transition probability from the cluster with $p_i$ to the cluster with $p_j$.
\item The transition probability from the interval with the popularity of $p_i$ to the interval 
      with the popularity of $p_j$.
\item The transition probability from the interval with the number of visit at $p_i$ to the interval 
      with the number of visit at $p_j$.
\item The transition probability from the interval with the average duration at $p_i$ to the interval 
      with the average duration at $p_j$.
\end{enumerate}


\subsection{Prediction}
Given parameters for node and edge features $\mathbf{w_u}$ and $\mathbf{w_p}$, 
we can compute the score of $\mathcal{T}$ as follows:
\begin{displaymath}
S'(\mathcal{T}) = \sum_{j=1}^L     \langle \mathbf{w_u}, \mathbf{\phi}_j (\mathcal{T}_j, q) \rangle +
                  \sum_{j=1}^{L-1} \langle \mathbf{w_p}, \mathbf{\phi}_{j, j+1} (\mathcal{T}_j, \mathcal{T}_{j+1}) \rangle
\end{displaymath}
where $L = |\mathcal{T}|$ is the length of $\mathcal{T}$, 
$\mathbf{\phi}_j$ is the feature vector of the $j$-th node (variable) described in Section~\ref{appendix:node},
$q$ is the query corresponding to $\mathcal{T}$, i.e., $q = (\mathcal{T}_0, \mathcal{T}_L, L)$,
and $\mathbf{\phi}_{j, j+1}$ is the feature vector of the $j$-th edge, as described in Section~\ref{appendix:edge}.

One approach to recommend a trajectory with respect to query $q = (p_s, p_e, L)$ and parameters 
$\mathbf{w_u}$ and $\mathbf{w_p}$ is recommending the one with the maximum score, i.e.,
\begin{displaymath}
    \mathcal{T}^* = \argmax_{\mathcal{T} \in \mathcal{P}^L} S'(\mathcal{T})
\end{displaymath}


\subsection{Training}
To estimate the parameters $\mathbf{w_u}$ and $\mathbf{w_p}$, we train a Structured Support Vector Machine
using the 1-slack formulation,
\begin{align*}
    \min_{\mathbf{w}, \xi \ge 0} ~~& \frac{1}{2} \mathbf{w}^T \mathbf{w} + C \xi \\
    s.t. ~~& \forall \left( \bar{\mathcal{T}}^{(1)}, \cdots, \bar{\mathcal{T}}^{(N)} \right) \in \mathscr{T}^N: \\
         ~~& \frac{1}{N} \mathbf{w}^T \sum_{i=1}^N \delta \left( \bar{\mathcal{T}}^{(i)} \right) \ge
             \frac{1}{N} \sum_{i=1}^N \Delta \left( \mathcal{T}^{(i)}, \bar{\mathcal{T}}^{(i)} \right) - \xi
\end{align*}
where $\mathbf{w} = [\mathbf{w_u}^T, \mathbf{w_p}^T]^T$ is the parameter vector,
$C > 0$ is the regularisation parameter, $\xi$ is the slack variable, 
$N$ is the number of trajectories in training set, 
$\mathcal{T}^{(i)}$ is the $i$-th trajectory in training set, 
and its corresponding recommendation is $\bar{\mathcal{T}}^{(i)}$, and
\begin{displaymath}
    \delta \left( \bar{\mathcal{T}}^{(i)} \right) = \Psi \left( \mathcal{T}^{(i)}, q^{(i)} \right) - 
                                                    \Psi \left( \bar{\mathcal{T}}^{(i)}, q^{(i)} \right)
\end{displaymath}
where $q^{(i)}$ is the query corresponding to $\mathcal{T}^{(i)}$ and the joint feature vector of $\mathcal{T}^{(i)}$ is
\begin{displaymath}
\Psi \left( \mathcal{T}^{(i)}, q^{(i)} \right) = \left[
\left( \sum_{j=1}^{|\mathcal{T}^{(i)}|}   \mathbf{\phi}_{j}      \left( \mathcal{T}_{j}^{(i)}, q^{(i)} \right) \right)^T,
\left( \sum_{j=1}^{|\mathcal{T}^{(i)}|-1} \mathbf{\phi}_{j, j+1} \left( \mathcal{T}_{j}^{(i)}, \mathcal{T}_{j+1}^{(i)} \right) \right)^T 
\right]^T
\end{displaymath}
$\Delta \left( \mathcal{T}^{(i)}, \bar{\mathcal{T}}^{(i)} \right)$ is the loss associated with the $i$-th trajectory 
and its corresponding recommendation, and Hamming loss is used in our experiment.
